
\input{headers}

\begin{document}
%\layout

\dominitoc

\begin{titlepage}

\input{cover}

%\chapter*{Remerciements}

%\chapter*{Résumé}

\tableofcontents

\postheader

\chapter{Introduction}
\minitoc

\chapter{Reconnaissance automatique de la parole}
\minitoc
\newpage

La reconnaissance automatique de la parole (RAP) a pour but de transcrire sous forme textuelle un signal audio. La RAP est un problème complexe, seuls des sous-problèmes ont été résolus à ce jour. Un système idéal serait capable de transcrire n'importe quel locuteur (qu'il le connaisse ou non), dans n'importe quel environnement sonore (en studio, au téléphone, dans la rue, etc.) en utilisant un registre de langage et une locution spontanés. Malheureusement, un tel système n'existe pas et des contraintes doivent lui être ajoutées pour fonctionner. Ces contraintes peuvent être la connaissance du locuteur pour apprendre sa voix afin de mieux la reconnaître, ou encore la réduction du nombre de mots que le système va être capable de reconnaître.

Les systèmes de RAP peuvent êtres classés selon plusieurs critères \cite{Danon91}~:

\begin{itemize}
  \item \textbf{le mode d'élocution} : cela peut être des syllabes ou des mots isolés, des mots connectés entre eux mais avec des pauses artificielles ou alors une parole quasi naturelle, appelée parole continue. 
    
    À noter que pour la parole continue, la distinction est faite entre la parole dite \og préparée \fg{} et la parole dite \og spontanée \fg{} \cite{Bazillon08}. La parole \og préparée \fg{} est le type de parole qu'aura un journaliste lorsqu'il présente les informations~: les répétitions, les faux départs et autres hésitations du langage parlé y sont peu présentes. La parole \og spontanée \fg{} concerne quant à elle le type de parole utilisée lors des conversations entre plusieurs personnes~: les répétitions, hésitations et autres irrégularités du langage parlé y sont très présentes.

  \item \textbf{la taille du vocabulaire} : tous les systèmes ont besoin d'un vocabulaire plus ou moins important. La taille d'un vocabulaire peut être inférieure à 1~000 mots pour les systèmes à petit vocabulaire comme les serveurs vocaux mais peut dépasser les 100~000 mots pour les systèmes de dictée vocale ou de transcription de journaux d'information. En fonction de la taille de ce vocabulaire, différentes couvertures grammaticales sont utilisées. Les systèmes à petit vocabulaire utilisent des grammaires contraintes. En revanche, les systèmes à grands vocabulaires utilisent des modèles de langage (cf. section \ref{ssection:modele_langage}) qui permettent une plus grande couverture grammaticale. 

  \item \textbf{la dépendance (ou non) vis-à-vis des locuteurs} : les serveurs vocaux doivent par exemple être indépendants de la personne qui va l'utiliser, tandis que les systèmes de dictée vocale peuvent se permettre d'être dépendant du locuteur.
  \item \textbf{l'environnement} protégé ou non : les serveurs vocaux vont devoir être capable de fonctionner dans un environnement bruité (on ne peut connaître à l'avance quel sera l'environnement sonore du locuteur qui interrogera le serveur), en revanche les systèmes de dictée vocale fonctionneront en environnement plus protégé.
\end{itemize}

Les applications de la RAP sont multiples et peuvent aller de la simple commande vocale (reconnaissance de mots isolés) à la transcription complète d'une émission de radio en passant par la transcription de réunions. Nous donnons par la suite un aperçu des quatre grands types de systèmes qui existent en reconnaissance de la parole.

\section{Différents types de systèmes}

Les progrès réalisés par les techniques de RAP ont permis aux systèmes d'évoluer et d'être de plus en performants. Des systèmes de commandes vocales ne reconnaissant que de simples mots, aux systèmes de transcriptions de journaux d'informations, la complexité des SRAP (Système de Reconnaissance Automatique de la Parole) ne cesse d'augmenter. Nous décrivons ici les principaux systèmes mis en place au fil des années \cite{Danon91}.

\subsection{Commandes vocales}

Les systèmes à commandes vocales sont des systèmes que l'on trouve abondamment dans les systèmes embarqués. Ils vont permettre une interaction entre l'utilisateur et la machine grâce à des commandes vocales. Ces commandes sont de simples mots isolés que l'utilisateur doit prononcer pour interagir avec le système. Ils se caractérisent par une taille de vocabulaire réduite et une indépendance vis-à-vis du locuteur. Les cas d'utilisation sont nombreux et il serait impossible de tous les citer ; en voici un bref aperçu :

\begin{itemize}
  \item jeux vidéo : la console portable DS de Nintendo embarque un micro pour permettre l'interaction homme/machine grâce à des mots simples.
  \item téléphones portables : beaucoup de téléphones récents permettent de naviguer dans l'interface grâce à la voix.
  \item aide aux handicapés : de tels systèmes peuvent pallier à un organe défaillant ou peuvent être utilisés dans des systèmes destinés à la ré\-éducation d'enfants sourds.
\end{itemize}

\label{lexique}

Comme tous les systèmes de RAP, ces systèmes se basent sur un vocabulaire (aussi appelé lexique) préalablement constitué. Ce lexique va définir la liste des mots que le système va être capable de reconnaître. Aucun autre mot que ceux contenus dans le lexique ne pourra être reconnu par le système.
Bien qu'il soit techniquement possible d'avoir un vocabulaire assez important, il est généralement limité à une centaine de mots. En effet, les personnes se servant de ces systèmes ne sont de toute façon pas capable de mémoriser plus d'une centaine de commandes. À noter que ces mots doivent être contrastés au niveau phonétique de manière à réduire les risques d'ambiguïté lors de la reconnaissance.

%Bien que d'autres méthodes aient été employées, comme la comparaison de références par programmation dynamique \cite{Tubach}, les systèmes utilisant des méthodes statistiques sont majoritairement utilisés depuis les années 1980 \cite{Rabiner89}. Certains de ces système utilisent des réseaux neuronaux, mais la plus part ont comme base les Modèles de Markov Cachés \cite{Gagnoulet89} (cf. \ref{ssec:trans_auto}). 

\subsection{Systèmes de compréhension}

Les systèmes de compréhension vont permettre un dialogue contraint avec une machine. L'utilisateur va devoir prononcer une suite de mots-clefs que le système est capable de reconnaître. En plus du système de reconnaissance vocale, un dispositif de compréhension des phrases doit être utilisé afin d'interpréter les phrases et de réagir en conséquence. Comme dans les systèmes à commandes vocales, le vocabulaire est restreint à quelques centaines de mots et le système est indépendant du locuteur. Afin de faciliter la gestion du dialogue par le système, les phrases acceptables se limitent à des schémas grammaticaux simplifiés. La mode d'élocution utilisé est généralement une parole continue et spontanée.

Bien que peu naturelles, ces restrictions syntaxiques et sémantiques sont viables si l'application est bien ciblée sur un domaine particulier. Les applications choisies pour ce type de systèmes le sont d'ailleurs en conséquence. Elles se limitent généralement à l'interrogation d'une base de données, de standards téléphoniques automatisés pour des renseignements météo ou des réservations de places.

Ces systèmes ont connu un important essor avec le lancement de projets financés par la DARPA (\emph{Defense Advanced Research Projects Agency}) aux États-Unis dans les années 1970 \cite{Klatt1977}. Plusieurs laboratoires participent aux projets DARPA comment CMU, MIT, BBN, SRI ou encore Bell Labs. Par exemple, le système SPHINX développé au CMU utilise un vocabulaire d'environ mille mots (issus du corpus \og Resource Management Database \fg{} \cite{Pri88}); ce système a notamment le mérite de permettre aux locuteurs une parole continue et ne nécessite pas d'apprentissage préalable du locuteur à reconnaître. Le taux de reconnaissance sur les mots est excellent, il est de l'ordre de 96 \% \cite{Lee1990}.

En France, la recherche s'est orientée vers des systèmes capables de gommer les erreurs de reconnaissance. En effet, même avec des erreurs de décodage du signal sonore, il est souvent possible de comprendre le sens d'une phrase. Ces erreurs de \og bas niveau \fg{} pourraient être récupérées grâce à des systèmes de plus haut niveau. C'est dans cette optique qu'ont été conçus les systèmes KEAL au CNET \cite{MercierKeal}, Esole au LIMSI \cite{Tubach}, Myrtille au CRIN \cite{Pierrel82} et Arial au CERFIA \cite{Perennou82}.

Les différents systèmes développés ainsi que les approches proposées depuis sont passées en revue dans \cite{SpokenDemori}.

\subsection{Systèmes de dictée automatique}

Les systèmes de dictée automatique ont pour but de transcrire un texte dicté par un locuteur de la façon la plus fidèle possible. Comme pourrait le faire une secrétaire, le texte transcrit doit respecter les règles orthographiques et grammaticales propres à la langue concernée. La compréhension du texte à transcrire n'est pas requise, et c'est d'ailleurs ce qui amène les erreurs les plus perturbantes pour l'utilisateur. En effet, les systèmes de dictée automatique ne sont pas capable de distinguer les différents sens d'un même mot.


Ce type de système se trouve à la charnière entre l'oral et l'écrit. En effet, même si l'interaction avec le système se fait grâce à la voix, le registre de langue utilisé sera plus proche du registre écrit. Ces systèmes sont souvent utilisés pour transcrire des rapports, des comptes-rendus ou rédiger des lettres. Quoiqu'il en soit, la personne est consciente qu'elle s'adresse à un ordinateur et adaptera donc sa locution en conséquence. De plus, les tournures agrammaticales ou les hésitations seront beaucoup moins nombreuses que dans un dialogue spontané entre deux personnes. La complexité est donc moindre que s'il fallait retranscrire un dialogue tel quel. 

En revanche, l'exercice même de la dictée sous-entend l'utilisation de plusieurs dizaines voire plusieurs centaines de milliers de formes fléchies. Les mots sont pris en contexte, et régis par une syntaxe aussi libre que la grammaire de la langue naturelle le permet. Le vocabulaire doit lui aussi être conséquent et est constitué de plusieurs milliers de mots (de 5~000 à plus de 60~000 mots). Avec ce type de système, une séance de dictée ressemblera plus à un médecin qui dicte un compte-rendu d'opération à sa secrétaire qu'à une maîtresse d'école qui fait faire un exercice de français à ses élèves. L'utilisateur n'est donc pas comme l'enseignant, même si il a bien en tête ce qu'il doit dire à la machine, il improvise quand même un minimum. Il doit donc pouvoir se tromper, corriger des mots ou remanier une tournure.

Pour obtenir de bonnes performances en temps réel, ces systèmes à grand vocabulaire sont fortement dépendants du locuteur. Une phase d'apprentissage est requise afin de permettre au logiciel d'apprendre des modèles spécifiques de la voix de la personne qui l'utilise. Cette phase d'apprentissage peut durer jusqu'à une heure.

Historiquement, c'est l'équipe de recherche IBM dirigée par F. Jelinek qui est la première à avoir développé un système grand vocabulaire (\emph{Tangora} 5~000 mots en 1985, 20~000 en 1987) pour la dictée vocale. Par la suite, l'ensemble des grands systèmes développés se sont inspirés du système \emph{Tangora}. Avec un taux de réussite supérieur à 95~\% pour un vocabulaire de 20 000 mots, \emph{Tangora} est un système multilingue existant notamment pour l'anglais \cite{Averbuch87}, l'italien \cite{Alto87}, le français \cite{CerfDanon90} ou encore l'allemand \cite{Wothke89}.


Au milieu des années 90 sont apparues des campagnes d'évaluation ayant pour but de tester les performances des systèmes sur des corpus lus. Par exemple, les données disponibles pour la campagne américaine Hub-3 \cite{Pallet96} sont des extraits de journaux comme le \emph{Wall Street Journal} ou le \emph{New York Times}. Ces extraits sont lus et enregistrés dans différentes conditions de manière à constituer des corpus audio. Le langage utilisé est donc celui utilisé pour de l'écrit. De ce fait, ce type de campagne évalue les performances de systèmes s'apparentant à la dictée vocale. Les performances deviennent rapidement excellentes, le système d'IBM affiche des taux d'erreur de l'ordre de 7~\% \cite{Bahl95}. En France, la première campagne lancée par l'AUPELF-UREF consistait à transcrire des textes lus du journal \emph{Le Monde} en utilisant le corpus BREF \cite{Lamel91}. Comme pour la campagne Hub-3, les résultats sont très bons puisque le système du LIMSI affiche des taux d'erreurs de l'ordre de 11~\% \cite{Gauvain94speaker-independentcontinuous}. Suite à ces succès, de nouveaux systèmes permettant de transcrire des enregistrements non préparés ont vu le jour.

\subsection{Systèmes de transcription grand vocabulaire}

\label{ssec:trans_broadcast}

Les systèmes de transcription grand vocabulaire ont pour but de transcrire des documents audio non préparés en essayant d'extraire le maximum d'informations de l'enregistrement. Le signal audio est très riche en informations (méta-données) susceptibles de venir compléter la transcription en mots. Ces méta-données peuvent être de différentes natures comme les frontières des phrases, les informations sur les locuteurs, les zones de musique, de publicité ou encore les hésitations des locuteurs.

Un système de transcription grand vocabulaire est composé d'un module qui va permettre d'obtenir la transcription en mots du signal ainsi que d'autres modules qui vont extraire les méta-données disponibles dans le signal audio. Les modules permettant d'extraire les méta-données sont diverses, comme les modules issus de la reconnaissance automatique du locuteur (modules de segmentation et de classification en locuteurs).

Les documents que les systèmes de transcription grand vocabulaire sont capables de traiter sont multiples. Ce sont notamment les enregistrements de journaux radiophoniques, de comptes-rendus de réunions ou d'émissions de télévision. Le mode d'élocution n'est pas contraint et la parole peut être préparée (journaliste qui présente les informations) ou spontanée (interview d'un invité). Le vocabulaire utilisé est très grand, il dépasse généralement les 65~000 mots. Ce type de système est complètement indépendant du locuteur et doit pouvoir s'adapter à l'environnement utilisé lors de l'enregistrement (environnement bruité, studio, téléphone, etc.). 



%Les campagnes d'évaluation se sont adaptées et de nouveaux corpus constitués d'enregistrements de journaux d'informations radiophoniques ont été constitués. Même si ces corpus contiennent beaucoup de parole qui a été préparée et écrite à l'avance par les journalistes, elle est néanmoins convertie à l'oral. À la différence des corpus précédents qui n'étaient constitués que de corpus écrits ensuite lus tel quel. De plus, ces corpus contiennent aussi de la parole plus spontanée, puisque beaucoup d'émissions font intervenir des invités ou interrogent des personnes dans la rue. À noter que ces corpus sont aussi fortement multi-locuteurs. Avec ces nouvelles campagnes d'évaluations, les systèmes sont confrontés à des corpus qui se rapprochent de plus en plus de dialogues complètement spontanés entre plusieurs personnes (même si comme nous l'avons fait remarquer, une partie de la parole est encore relativement préparée).

Aux États-Unis, le DARPA puis le NIST ont organisés des campagnes d'évaluations des systèmes de transcription enrichie (\emph{Rich Transcription, RT}) \cite{Graff96the1996} à partir de 1996. En France, c'est la campagne ESTER \cite{Gravier04,Galliano06} organisée par l'AFCP, la DGA et ELRA qui a permis de réaliser les premières évaluations des systèmes de reconnaissance Français sur des journaux d'informations radiophoniques. Une deuxième campagne d'évaluation Française nommée ESTER II \cite{Ester2} a ensuite été menée de 2008 à 2009 de manière à mesurer les progrès des différents systèmes.

\begin{figure}[h]

\begin{center}
\includegraphics[width=\columnwidth]{img/standford2003.png}

\caption{Évolution des taux d'erreur en reconnaissance de la parole}
\label{fig:standford2003}
\end{center}

\end{figure}


La figure \ref{fig:standford2003} \cite{Pallett2003} présente l'évolution des performances des systèmes de reconnaissance de la parole de la fin des années 90 à 2003. Les performances des systèmes de transcription sur des émissions radiophoniques anglaises sont maintenant très bonnes (10~\% de taux d'erreur). On observe aussi des taux similaires sur les campagnes d'évaluation ESTER I et ESTER II. Les défis futurs de tels systèmes portent sur l'adaptation à d'autres langues et à d'autre domaines, mais aussi sur leur capacité à \emph{passer à l'échelle}. En effet, le type de données à traiter évolue de plus en plus (données provenant de plusieurs capteurs, de la vidéo et du son, etc.) et la taille de ces données ne cesse de croître. Un bon exemple est la \emph{Smart Room} NIST \cite{Standford2003} qui génère environ 1Go de données par minute, issues de multiples capteurs. La nécessité de prendre en compte cette problématique devient alors évidente.
%Cette campagne s'est aussi caractérisée par l'évaluation des systèmes de détection des Entités Nommées.

\section{Transcription automatique de la parole}
\label{ssec:trans_auto}

Les techniques de transcription automatique de la parole sont au c\oe{}ur du processus de production d'une transcription~: ce sont elles qui vont permettre de fournir le texte correspondant aux paroles prononcées. Les travaux présentés dans cette thèse s'appuient sur ces techniques, il convient donc de les décrire en détail ici. 


Les systèmes de transcription automatique de la parole utilisés actuellement sont des systèmes fondés sur des méthodes statistiques. Ces fondements ont été élaborés par Jelinek et ses collègues chez IBM \cite{Jelinek1976}, au milieu des années 1970. La figure \ref{fig:principes_srap} donne un aperçu du fonctionnement actuel d'un système de transcription automatique de la parole. 
 
\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/sys_rap}

\caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
\label{fig:principes_srap}
\end{center}

\end{figure}

\subsection{Principes généraux} % (fold)
\label{ssub:principes_generaux}


L'objectif d'un système de RAP probabiliste est d'associer une séquence de mots $\hat{W} = w_1 w_2 ...wk$ (avec $w_i$ qui est un mot de cette séquence) à une séquence d'observations acoustiques $X$. Le système recherche la séquence de mots qui maximise la probabilité \emph{a posteriori} $P(W|X)$, où $P(W|X)$ est la probabilité d'émission de $W$ sachant $X$ . On obtient, après application de la règle de Bayes :

\begin{equation}
  \label{eq:bayes}
  \hat{W} = \underset{W}{arg\ max}\ P(W|X)\ =\ \underset{W}{arg\ max}\ \frac{P(W)P(X|W)}{P(X)}
\end{equation}

Comme la séquence d'observations acoustiques $X$ est fixée, $P(X)$ peut être considérée comme une valeur constante inutile dans l'équation \ref{eq:bayes}. On a donc :

\begin{equation}
  \label{eq:bayes_simplifie}
  \hat{W} =\ \underset{W}{arg\ max}\ P(W)P(X|W)
\end{equation}

Deux types de modèles probabilistes sont utilisés pour la recherche de la séquence de mots la plus probable : des modèles acoustiques qui fournissent la valeur de $P(X |W)$, et un modèle de langage qui fournit la valeur de $P(W)$. $P(X|W)$ peut se concevoir comme la probabilité d'observer $X$ lorsque $W$ est prononcée, alors que $P(W)$ se réfère à la probabilité que $W$ soit prononcée dans un langage donné. La difficulté pour obtenir un système de RAP performant est de définir les modèles les plus pertinents possibles pour le calcul de $P(W)$ et $P(X|W)$ (voir figure \ref{fig:principes_srap}).

\subsection{Modèles acoustiques}

Il est impossible pour un système de transcription d'exploiter le signal de la parole directement. En effet, il contient bon nombre d'informations qui ne sont pas utiles au décodage de la parole (mais qui le seront pour la reconnaissance du locuteur, cf \ref{ssub:decoupage_regroupement}) et qui peuvent le parasiter. Le signal contient en effet, en plus des informations permettant de décoder les mots, des informations sur les locuteurs, sur les conditions d'enregistrement, etc. Le signal de la parole étant naturellement très variable et redondant, il nécessite de toute façon des traitements spécifiques avant de pouvoir être exploité. Ces traitements (appelés paramétrisation) vont consister à extraire du signal les paramètres qui sont dépendants de la parole prononcée.

Plusieurs techniques de paramétrisation existent, les deux plus utilisées sont :
\begin{itemize}
  \item PLP (\emph{Perceptual Linear Prediction}) : domaine spectral \cite{Hermansky91}
  \item MFCC (\emph{Mel Frequency Cepstral Coefficients}) : domaine cepstral \cite{Bridle74,Mermelstein76}
\end{itemize}

Le signal de la parole est modélisable par un ensemble d'unités acoustiques, qui peuvent être considérées comme des sons élémentaires de la langue. Classiquement, l'unité choisie est le phonème. Le phonème est la plus petite unité discrète ou distinctive d'un mot, auquel elle peut faire perdre son sens par substitution. Par exemple, \emph{"chou"} et \emph{"pou"} se distinguent par leurs phonèmes initiaux. Dans le cadre des systèmes de transcription de la parole actuels, ces unités acoustiques sont modélisées par des Modèles de Markov Cachés (MMC) \cite{Rabiner89}. 

L'apprentissage de ces modèles acoustiques s'effectue grâce a des données \emph{a priori} annotées. Cet apprentissage permet d'obtenir une modélisation du message de la parole. Différentes techniques d'apprentissage et d'adaptation sont utilisées, parmi les plus connues figurent notamment :

\begin{itemize}
  \item l'apprentissage par maximum de vraisemblance (\emph{Maximum Likelihood : ML}) avec l'algorithme EM \cite{Dempster77maximumlikelihood} (\emph{Expectation/Maximisation}), 
  \item l'estimation par information mutuelle maximale \cite{Valtech97} (\emph{Maximum Mutual Information Estimation : MMIE}), 
  \item l'adaptation par maximum \emph{a posteriori} \cite{Gauvain94maximuma} (\emph{Maximum \emph{a posteriori} probability : MAP}), 
  \item et l'adaptation par régression linéaire \cite{Gales98maximumlikelihood} (\emph{Maximum Likelihood Linear Regression : MLLR}).
\end{itemize}

\subsection{Modèles de langage}
\label{ssection:modele_langage}

Les systèmes de transcription ont besoin de contraintes linguistiques propres à la langue à décoder. Ces contraintes vont êtres utilisées par le système pour guider le décodage dans la sélection d'hypothèses acoustiques concurrentielles. Ces contraintes sont introduites par les modèles de langage dans les systèmes de transcription. Ces modèles sont des modèles probabilistes, et la probabilité d'apparition de la séquence de $k$ mots $W^k_1$ s'exprime de la façon suivante :

\begin{equation}
P(W^k_1) = P(w_1) \prod_{i=2}^{k} P(w_i|h_i)
\label{eq4}
\end{equation}

Où $h_i$ correspond aux mots précédents $w_i$. $h_i$ est aussi appelé l'historique du mot $w_i$. On a donc $h_i = w_1, ..., w_{i-1}$.

Les modèles utilisés en RAP sont des modèles de langage de type \textit{n-gramme} \cite{Jelinek1976}. Bien qu'ancien, ce type de modèle constitue toujours l'état de l'art.

Ils correspondent à une modélisation stochastique du langage, où l'historique d'un mot est représenté par les $n-1$ mots qui le précèdent. La formule \ref{eq4} peut donc s'écrire~:

\begin{equation}
P(W^k_1) = P(w_1) \prod_{i=2}^{n-1} P(w_i|w_1, ..., w_{i-1}) \prod_{i=n}^{k} P(w_i|w_{i-n+1}, ..., w_{i-1})
\label{eq5}
\end{equation}

Les modèles couramment utilisés dans les SRAP sont généralement des modèles d'ordre 3 ou 4 ($n=3$ ou $n=4$). Nous parlons de modèle \textit{trigramme} ou \textit{quadrigramme} (pour $n=1$ \textit{unigramme}, pour $n=2$ \textit{bigramme}, ...).
Dans le cas d'un modèle quadrigramme, l'équation \ref{eq5} s'écrit :

\begin{equation}
P(W^k_1) = P(w_1) P(w_2|w_1) P(w_3|w_1,w_2) \prod_{i=4}^{k} P(w_i|w_{i-3}w_{i-2}w_{i-1})
\label{eq6}
\end{equation}

Les modèles de langage $n-grammes$ ont la particularité d'être assez souples, puisqu'ils permettent de modéliser des phrases grammaticalement incorrectes. En revanche, ils ne s'interdisent pas non plus de produire des phrases dénuées de tout sens.
En effet, ces modèles de langage sont capables d'attribuer une probabilité à des mots inconnus. Le système est donc capable de probabiliser des phrases qu'il n'a jamais observées dans son corpus d'apprentissage, tout en privilégiant les séquences de mots les plus fréquemment observées. 

\subsection{Évaluation}

La métrique utilisée dans les différentes campagnes (cf. section \ref{ssec:trans_broadcast}) pour évaluer les transcription est le taux d'erreur mot (WER -- Word Error Rate). Pour calculer ce taux d'erreur, la meilleure hypothèse de transcription du système est alignée avec les transcriptions réalisées manuellement (transcriptions dites de référence). Trois types d'erreurs sont alors possibles. Lorsqu'un mot apparaît dans l'hypothèse de reconnaissance alors qu'il n'y a aucun mot correspondant dans la référence, il s'agit d'une insertion. Si, au contraire, il y avait un mot dans la référence et que ce mot n'apparaît pas dans l'hypothèse, nous parlons de suppression. Quand un mot de la référence est remplacé par un autre mot lors du décodage, c'est une substitution. Le taux d'erreur est ensuite déterminé par la formule \ref{eq:wer}. 

\begin{equation}\mathbf{}
  WER=\frac{\mathrm{nb.~insertions + nb.~suppressions + nb.~substitutions}}{\mathrm{nombre~de~mots~de~la~r\acute ef\acute erence}}
\label{eq:wer}
\end{equation}  

À titre d'exemple, lors de la première campagne ESTER \cite{Galliano05}, le meilleur système (celui du LIMSI) atteignait un taux d'erreur mot de 11,9~\%. L'écart entre les différents systèmes était cependant relativement important, puisque le deuxième système, celui du LIUM, affichait un taux d'erreur mot de 23,6~\%. Lors de la campagne ESTER II \cite{Ester2}, le premier système était toujours le LIMSI avec un taux d'erreur de 12,1~\% . En revanche l'écart avec le système du LIUM s'est réduit puisque son taux d'erreur n'était plus que de 17,8~\%.


\section{Reconnaissance automatique du locuteur}
\label{sec:ral}

La Reconnaissance Automatique du Locuteur (RAL) regroupe un ensemble de méthodes capables d'extraire les caractéristiques vocales propres à chaque individu à partir d'un document audio. La RAL joue un rôle important dans l'enrichissement d'une transcription, puisqu'elle va permettre d'apporter des informations sur les différents locuteurs du document.
  

\subsection{Caractéristiques et variabilité}
\label{ssec:carac_variabilite}

Toutes les techniques de RAL se basent sur l'extraction des caractéristiques (\emph{features}) du signal audio \cite{Atal76,Doddington85,hollien_acoustics_of_crime90,oshaughnessy86,Sambur75}. Ces caractéristiques doivent être robustes par rapport aux conditions d'enregistrement et fournir le plus de renseignements possibles concernant l'identité du locuteur.

Une question importante est celle de la variabilité des caractéristiques mesurées. On parle de variabilité intra-locuteur lorsque l'on s'intéresse à la variation des paramètres mesurés pour un même locuteur, et de variabilité inter-locuteur si on considère des locuteurs différents \cite{Doddington85,hollien_acoustics_of_crime90,Rosenberg91}. Pour la reconnaissance du locuteur, on cherche à extraire des caractéristiques du signal de parole qui présentent une forte variabilité inter-locuteur (pour pouvoir différencier les locuteurs entre eux) et une faible variabilité intra-locuteur (pour garantir la robustesse du système).

\subsection{Applications}

Des serveurs vocaux aux terminaux mobiles en passant par les dispositifs de sécurité, la RAL est utilisée dans de nombreux domaines. Certains serveurs vocaux l'utilisent notamment pour détecter le genre du locuteur qui s'exprime. Par exemple, grâce à la plateforme MISTRAL, la société Calistel \cite{MeignierMistral} propose de router les appels provenant de locuteurs masculins vers des opératrices féminines et inversement, afin d'obtenir un taux de satisfaction des appels plus important. Mais les applications de la RAL les plus connues sont certainement celles qui concernent la biométrie. 

Dans les applications biométriques, un système de RAL va essayer de reconnaître, grâce à sa voix, un locuteur qui cherche à s'identifier auprès d'un terminal. Même si ces systèmes donnent de bons résultats, ils ne sont pas parfaits. Les systèmes à l'état de l'art comme MISTRAL ont un taux d'erreur aux alentours de 5~\% \cite{MeignierMistral}. Ils ne peuvent donc pas être considérés comme aussi sûr que les empreintes digitales et doivent être utilisés avec prudence à des fins judiciaires par exemple \cite{Boe99, Bonastre2003}. En revanche, avec l'arrivée des terminaux mobiles dans la vie de tous les jours, la RAL s'invite avec succès dans les téléphones portables de dernière génération \cite{Larcher_JEP10}.


L'utilisation de la RAL dans les applications que nous venons de citer peut se découper en quatre grandes catégories décrites ci-dessous. Ces catégories sont d'ailleurs sujet à évaluation dans les différentes campagnes comme NIST \cite{Alvin04nistspeaker} et ESTER \cite{Gravier04}.

%En plus de la difficulté à différencier deux locuteurs différents, la RAL est aussi confrontée à des problèmes intra-locuteurs. En effet, la voix d'un locuteur peut fortement varier d'un enregistrement à l'autre. Le niveau de fatigue, l'émotion, le contexte d'enregistrement ou encore l'âge d'un locuteur sont autant de variations qui peuvent venir perturber la RAL. Mais la biométrie n'est pas la seule application possible, de par ses caractéristiques, la RAL est utilisée dans bien d'autres domaines.

\subsection{Identification automatique du locuteur}

L'identification automatique du locuteur (IAL) a été une des premières utilisations de la RAL \cite{Atal76}. En IAL, la liste des locuteurs à identifier est connue du système. Le système doit pouvoir décider, à partir d'un échantillon de voix, à quelle identité connue du système correspond l'échantillon. La figure \ref{fig:principe_ial} décrit le principe général de l'IAL.

L'identification automatique du locuteur se découpe en deux étapes : une étape d'apprentissage et une étape de test. À partir d'un ensemble d'enregistrements de voix de chaque locuteur, le système apprend un modèle pour chaque locuteur lors de l'étape d'apprentissage. Lors de l'étape de test, le système confrontera l'échantillon de voix qu'il recevra aux différents modèles qu'il aura déjà appris afin de déterminer si l'identité du locuteur est déjà connue.

En fonction de l'application, deux types de décisions sont possibles. Prenons le cas d'un centre d'appel téléphonique qui enregistre les conversations de ses employés. Un système de IAL peut être utilisé pour classer chacun des enregistrements en fonction de l'employé concerné. Dans ce cas, la liste des locuteurs possibles est connue du système (tous les employés), et aucun autre locuteur non employé ne peut être concerné. Dans ce cas, l'application présuppose que l'ensemble des locuteurs possible est fermé, et connu du système. Le système de IAL choisira alors, parmi la liste, le modèle de locuteur le plus ressemblant à l'enregistrement de test.  

En revanche, dans une application utilisant un ensemble ouvert de locuteurs possibles, le système de IAL ne connait pas tous les locuteurs possibles. Dans ce cas, en plus de déterminer le locuteur le plus vraisemblable, le système a la possibilité de rejeter l'échantillon de test en ne renvoyant aucune identité connue pour cet échantillon. 

\begin{figure}[h]

\begin{center}
\includegraphics[width=1.0\columnwidth]{img/ial}

\caption{Principe de l'identification automatique du locuteur}
\label{fig:principe_ial}
\end{center}

\end{figure}

\subsection{Vérification automatique du locuteur}

La vérification automatique du locuteur (VAL) permet de décider si l'identité revendiquée par un locuteur est compatible avec sa voix. Il s'agit donc de trancher entre deux hypothèses : soit le locuteur est bien le locuteur autorisé (on l'appelle aussi locuteur client), c'est à dire que son identité correspond à celle qu'il revendique, soit le locuteur est un imposteur qui cherche à se faire passer pour la personne qu'il n'est pas. À partir d'un échantillon de voix de référence, et d'un échantillon de voix de test, le système va donc devoir dire si oui ou non les deux locuteurs correspondent \cite{Atal76}.

Les systèmes de VAL sont très dépendants des différences entre les échantillons de voix de référence et les échantillons de tests (cf. \ref{ssec:carac_variabilite}). Accepter un locuteur qui devrait être rejeté peut avoir de lourdes conséquences, en particulier dans les applications où un haut niveau de sécurité est demandé \cite{Bonastre2003} (contrôle aux frontières, système bancaire, identification judiciaire, etc.). 

\subsection{Suivi de locuteur}

\label{ssec:sl}

Le suivi de locuteur (SL) se base sur la tâche de vérification du locuteur. À partir d'un enregistrement de référence d'un locuteur, le système va devoir déterminer si le locuteur intervient dans le document. Si c'est le cas, il devra être capable de préciser où et quand ce locuteur intervient dans l'enregistrement. À noter qu'à part le locuteur à suivre, aucun autre locuteur du document n'est connu du système. Plusieurs méthodes différentes décrites dans \cite{EURECOM269} peuvent être employées pour réaliser cette tâche, la figure \ref{fig:suivi_locu} propose un aperçu général du principe du SL.

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/suivi_locu}

\caption{Principe du suivi de locuteur}
\label{fig:suivi_locu}
\end{center}

\end{figure}

\subsection{Segmentation et classification en locuteur}

\label{ssub:decoupage_regroupement}

La tâche de segmentation et de classification en locuteur consiste à délimiter l'intervention de chaque locuteur dans le document audio. À la différence du suivi de locuteur, aucune information \emph{a priori} sur les locuteurs du document n'est disponible pour cette tâche. Plus précisément, aucune information sur le nombre de locuteurs n'est disponible, ni leur identité, ni des modèles de voix qui auraient pu être appris au préalable. 

Les travaux fondamentaux de la segmentation et de la classification en locuteur ont été réalisés au début des années 1990 par la société BBN sous la direction de H. Gish \cite{Siu1991,Siu1992}. Ces travaux consistent à indexer les différents échanges radio entre pilotes et contrôleurs aériens. Ces échanges sont enregistrés puis segmentés automatiquement. Ces enregistrements peuvent contenir plusieurs dialogues contrôleur/pilote. La segmentation est ensuite utilisée pour reconstruire ces dialogues.

Depuis, les techniques ont évoluées \cite{TranterReynolds2006} et le champ d'application de la segmentation en locuteurs s'est étendu, elle se retrouve intégrée dans le cadre plus vaste de l'indexation en locuteurs de bases de données de documents multimédia \cite{PhdMeignier}. Les documents traités sont divers et variés, on peut notamment citer les conversations téléphoniques, les enregistrements de journaux télévisés ou radiophoniques, les films ou encore les enregistrements de réunion.

La figure \ref{fig:seg_classif} décrit les principes généraux du processus de segmentation et de classification qui s'effectue en plusieurs étapes :

\begin{itemize}
  \item La \textbf{paramétrisation} va extraire les caractéristiques acoustiques du document audio. Ces caractéristiques seront représentées via des vecteurs acoustiques qui pourront ensuite être exploités par les autres phases du processus.
  \item La \textbf{segmentation} va s'attacher à trouver des points de ruptures, des frontières, au sein de l'enregistrement. Ces frontières peuvent être de différentes natures : changement de locuteur, changement de canal de transmission (studio, téléphone), présence d'un long silence, musique, etc. Les portions de signal entre ces différentes frontières sont appelées segments. Ces segments contiennent des données homogènes : même locuteur, même canal de transmission.
  \item La \textbf{classification} groupe les segments locuteur par locuteur jusqu'à découvrir le nombre de classes correspondant au nombre de locuteurs intervenant dans le document. Chaque classe contient à la fin de la classification l'ensemble des segments prononcés par un locuteur. La classe définit alors l'intervention du locuteur dans le document.
  \item La \textbf{resegmentation} affine les frontières des segments. Dans cette étape, le document est de nouveau découpé en segments en fonction des données contenues dans les classes.
\end{itemize}

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.8\columnwidth]{img/segmentation-classification}

\caption{Principe de la segmentation et classification en locuteur}
\label{fig:seg_classif}
\end{center}

\end{figure}

À noter que les classes de locuteur produites sont anonymes. En effet, ce type de système étiquette chacune des classes avec un nom unique, mais dénué de tout sens. Le processus qui consiste à étiqueter ces classes avec le vrai couple prénom/nom du locuteur auquel elles se rapportent est appelé \textbf{identification nommée du locuteur}. C'est à cette tâche que s'intéressent les travaux de cette thèse. 


\section{Système de transcription enrichie}

\label{sec:trans_enrichie}

Avec l'évolution des systèmes au fil de années (de la simple reconnaissance de mots isolés à la transcription de journaux d'informations) est apparue la notion de transcription enrichie. En effet, comme décrit dans \ref{ssec:trans_broadcast}, nombre d'informations annexes à la transcription en mots peuvent être extraites du signal.

Par rapport aux systèmes de dictée vocale, les documents utilisés pour la transcription de journaux d'informations sont multi-locuteurs. Au sein d'un même enregistrement, des dizaines de locuteurs différents peuvent parler tout au long du fichier. De plus, ces locuteurs peuvent s'exprimer depuis un téléphone ou en studio. Toutes ces informations permettent d'étayer, d'enrichir la transcription. La reconnaissance du locuteur est utilisée pour segmenter le document en locuteurs ainsi que pour détecter leur genre ou l'environnement à partir duquel ils s'expriment (studio ou téléphone). La détection des entités nommées est quant à elle utilisée pour étiqueter des groupes de mots particuliers comme les noms de personnes ou encore les lieux.


La figure \ref{fig:etapes_transcription_enrichie} décrit les différentes composantes des transcriptions produites pour les campagnes de type ESTER. Ces composantes constituent la base de tous les systèmes de transcription actuels. Il est possible de distinguer deux grandes parties que nous détaillons ci-dessous. 

\subsection{Segmentation et classification}
Cette première partie consiste à découper le fichier de manière à obtenir des portions homogènes, c'est à dire concernant le même locuteur et les mêmes conditions d'enregistrement. Ce premier niveau de découpage est communément appelé découpage en segments. Contrairement à ce que l'on pourrait penser, les segments ne représentent généralement pas des phrases, mais plutôt des groupes de souffle~: ils commencent et s'arrêtent lorsqu'un silence significatif est détecté. Ce premier découpage est effectué pour des raisons techniques, et notamment pour faciliter la tâche des systèmes de transcription automatique (cf. \ref{ssec:trans_auto}), qui doivent décoder segment par segment pour éviter des problèmes d'explosion combinatoire. Cette étape se base sur la méthode décrite dans la section \ref{ssub:decoupage_regroupement} à laquelle un module de détection de zones de parole ou de non-parole (silence, musique, bruit, etc.) a été ajouté.

Tous les segments contigus appartenant au même locuteur définissent un tour de parole de ce locuteur. Un même locuteur qui parle à plusieurs endroits d'un enregistrement aura donc plusieurs tours de parole au sein de cet enregistrement. En reconnaissance du locuteur (cf. \ref{ssub:decoupage_regroupement}), on dit que tous les segments (et donc les tours de parole) d'un même locuteur sont regroupés au sein d'une même classe. À chaque locuteur du document correspond donc une classe de locuteur.

Au niveau purement acoustique, il est aussi possible de détecter le genre (masculin ou féminin) du locuteur qui s'exprime \cite{liumspkdiarization}. Cette information peut être ajoutée à la transcription enrichie, comme c'est le cas dans la figure \ref{fig:etapes_transcription_enrichie}.

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/systeme_transcription_description}

\caption{Les étapes de réalisation d'une transcription enrichie}
\label{fig:etapes_transcription_enrichie}
\end{center}

\end{figure}


\subsection{Transcription et entités nommées}

La deuxième partie consiste tout simplement à écrire les paroles prononcées par les différents locuteurs afin d'obtenir la transcription en mots du signal. Lorsque cette transcription est réalisée automatiquement, la ponctuation ainsi que les majuscules ne sont pas requises. En revanche, des techniques existent pour rajouter la ponctuation et les majuscules aux transcriptions afin de se rapprocher, tant que faire se peut, d'une transcription réalisée manuellement \cite{Beeferman98cyberpunc,Huang2002}.

Les transcriptions, qui plus est les transcriptions de journaux radiophoniques, contiennent un grand nombre d'entités nommées (noms de personnes, lieux, radios, etc). Des systèmes de détection des entités nommées permettent de les repérer dans les transcriptions (cf. section suivante). Ce sont des informations supplémentaires qui vont pouvoir être utilisées en recherche d'informations, ou plus précisément dans le cadre des présents travaux, pour identifier les locuteurs d'un document.


%\section{Architecture détaillée}

%Les différentes campagnes d'évaluations des systèmes de RAP actuelles met\-tent l'accent sur la réalisation d'une transcription enrichie (cf. \ref{ssec:trans_broadcast}) via l'extraction d'informations sur les locuteurs ou encore la détection des entités nommées. Lors de la réalisation d'une transcription enrichie, plusieurs systèmes entrent en jeu. Les systèmes de transcription automatique de la parole évidemment, mais aussi des systèmes de détection d'entités nommées et de reconnaissance automatique du locuteur. Afin de situer le contexte des travaux présentés dans cette thèse, ces différents domaines de recherche sont présentés ci-dessous.



\section{Détection des entités nommées}
\label{ssec:EN}

%Avec l'avènement d'Internet, la quantité de données textuelles à disposition augmente de jour en jour de façon exponentielle. Comme c'est le cas pour les données audio, ces données sont, à cause de leur quantité, impossible à indexer efficacement par un humain. L'extraction d'informations et le traitement de ces données est pourtant capitale pour bon nombre de scientifiques et d'industriels. 

Dans le but d'enrichir une transcription, une des étapes est généralement de détecter les entités nommées. Ces entités nommées vont permettre d'extraire un certain nombre d'information des transcriptions, comme les noms de personne, les lieux, les noms d'organisations, etc.

\subsection{Catégorisation}

Par la richesse des informations qu'elles contiennent, les entités nommées (EN) sont des éléments très importants pour les systèmes d'extraction d'information. Dans les années 1980, les campagnes d'évaluation MUC ont permis de définir ce qu'était une tâche de reconnaissance des entités nommées. Pour MUC-6 \cite{MUC6,MUC6results}, les EN sont les noms propres, les acronymes et éventuellement d'autres mots qui rentrent dans les catégories suivantes :

\begin{itemize}
  \item \emph{Organisation} : regroupe les entreprises, les institutions gouvernementales et les autres organisations ;
  \item \emph{Person} : regroupe les noms de personnes ou de familles ;
  \item \emph{Location} : regroupe les noms de lieux politiquement ou géographiquement définis (villes, pays, régions, etc.) ;
  \item \emph{Time} : regroupe les dates et données temporelles ;
  \item \emph{Number} : regroupe les données numériques comme les sommes d'argent ou les pourcentages.
\end{itemize}

Il existe des typologies des EN beaucoup plus complètes que celle des campagnes MUC comme celle de Paik et al. \cite{Paik96} ou de Coates-Stephens \cite{Coates92}. Par exemple, celle de Coates-Stephens définit 8 catégories d'EN :
\begin{itemize}
  \item les noms de personnes ;
  \item les noms de lieux ;
  \item les noms d'organisations ;
  \item les noms d'origines (noms d'habitants de pays, de villes, de régions, etc.) ;
  \item les noms de législations (\emph{loi Évin}), d'indices boursiers (\emph{Nikkei, CAC 40, Dow Jones}) ;
  \item les noms d'événements (guerres, révolutions, catastrophes, salons, JO, etc.) ;
  \item les noms d'objets.
  
\end{itemize}

Elles couvrent donc les noms propres et des formes linguistiques spécifiques.

\subsection{Les différents types de systèmes}

Selon Poibeau \cite{Poibeau2002} et Sekine et Eriguchi \cite{Sekine2000}, les systèmes de détection des EN peuvent êtres classés en trois grands types :\\

\textbf{Les systèmes fondés sur des règles écrites \og à la main \fg{}}\\
Dans ces systèmes, le concepteur doit élaborer un ensemble de règles qui seront ensuite utilisées pour détecter les entités nommées. Historiquement, cette technique fût la première utilisée dans le domaine. Bien que depuis la campagne d'évaluation MUC-6, l'apprentissage automatique ait fait son apparition dans ce domaine, les systèmes à base de règles écrites \og à la main \fg{} restent encore très utilisés aujourd'hui.\\

\textbf{Les systèmes à base d'apprentissage automatique}\\
Ces systèmes utilisent des techniques d'apprentissage automatique pour apprendre un modèle capable d'étiqueter des entités nommées à partir d'un corpus d'apprentissage. Ces travaux sont largement inspirés des techniques utilisées en reconnaissance automatique de la parole, et utilisent diverses techniques d'apprentissage : modèles de Markov cachés, modèle d'entropie maximum, arbres de décision, etc. Le système (LIA\_NE) développé par Béchet \cite{bechetc-icassp10} au Laboratoire d'Informatique d'Avignon (LIA) pour ESTER2 \cite{Ester2} fait partie de ce type de systèmes.\\ 

\textbf{Les systèmes mixtes}\\
Ces systèmes utilisent généralement des lexiques initiaux. Parmi ces systèmes, Poibeau \cite{Poibeau2002} distingue deux approches. La première consiste à apprendre automatiquement des règles, puis à utiliser un expert pour les réviser. Dans la seconde, un ensemble de règles de base est constitué par le concepteur, puis étendu (semi-)automatiquement par inférence, afin d'obtenir une meilleure couverture. Par exemple, Nemesis le système développé par Fourour \cite{Fourour04} au Laboratoire d'Informatique de Nantes Atlantique (LINA), fait partie de ces systèmes.\\


\subsection{Reconnaissance et découverte des Entités Nommées}

Lors de la reconnaissance des entités nommées, plusieurs types de problèmes doivent être résolus \cite{Fourour04}. Pour exprimer ces problèmes, la terminologie introduite par \cite{Daille00} fondée sur des critères graphiques est utilisée~:

\begin{itemize}
    \item \textbf{Entités nommées pures}~: les entités nommées d'une seule forme commençant par une majuscule comme \emph{France}, \emph{FFF}, \emph{États-Unis}.
    \item \textbf{Entités nommées composées}~: les entités nommées constituées de plusieurs formes pleines comportant toutes une majuscule comme \emph{Quai Branly}, \emph{Grand Palais}.
    \item \textbf{Entités nommées mixtes}~: les entités nommées constituées de plusieurs formes qui peuvent ou non commencer par une majuscule, mais dont au moins une est entièrement en minuscules comme \emph{Parti socialiste}, \emph{château de Versailles}.
\end{itemize}


Tout d'abord, la reconnaissance des entités nommées connues est réalisée grâce à la projection de bases lexicales sur le texte à étiqueter. Les systèmes sont alors confrontés au problème de l'ambiguïté des entités nommées pures~: \emph{Paris} est associé à un nom de ville mais peut apparaître au sein d'entités nommées composées ou mixtes. Il peut alors désigner une personne comme le \emph{Comte de Paris}, un nom de rue \emph{rue de Paris} ou encore une société \emph{Paris International}. Même pour ces entités nommées pures, une analyse du contexte est nécessaire pour obtenir une catégorisation correcte.

À ce problème d'ambiguïté viennent s'ajouter les variations que peuvent subir les entités nommées~:les variations graphiques (\emph{Parti Socialiste}, \emph{Parti socialiste}), les sigles, acronymes ou abréviations, (\emph{Société Nationale des Chemins de fer Français} $\rightarrow$ \emph{SNCF}), les métaphores (\emph{l'Everest} $\rightarrow$ \emph{le toit du monde}), etc.

Outre la gestion de ces différents problèmes, les systèmes doivent être capable de découvrir de nouvelles entités nommées qui ne sont pas présentes dans leurs bases lexicales.

Par exemple, les entités nommées mixtes sont par définition constituées d'une ou plusieurs formes contenant une majuscule et de noms communs. Pour ce type d'entités nommées, la difficulté réside dans la délimitation de l'entité nommée~: où se trouvent les bornes gauche et droite de l'entité nommée ?

Pour la délimitation à gauche, le problème est d'inclure ou non les noms communs situés avant la forme comportant une majuscule au sein de l'entité nommée mixte. Ces noms communs peuvent préciser un rôle social (\emph{le premier ministre François Fillon}), des statuts particuliers (\emph{l'exilé Musil}) ou encore une catégorisation géographique.

La délimitation à droite se heurte aux problèmes modification, de la résolution de l'attachement prépositionnel et de la portée de la coordination \cite{Wacholder97}. Par exemple, l'adjectif modifiant une entité nommée comme \emph{fédéral} dans \emph{Allemagne fédérale} constitue une entité nommée mixte, ce qui n'est pas le cas avec \emph{lointain} dans \emph{Chine lointaine}. En plus de cette ambiguïté sur la limite des droites des entités se pose le problème de la sur-composition~: une entité nommée mixte peut contenir une entité nommée d'une autre catégorie (par exemple \emph{Université du Mans}, \emph{Guerre du Vietnam}).

Les systèmes adoptent différentes techniques pour résoudre ces problèmes. Ces techniques font appel à des solutions linguistiques, à des solutions à base d'apprentissage ou à un mixte des deux. Quoiqu'il en soit, la délimitation des entités nommées peut varier en fonction du type d'application auxquelles elles sont destinées. Par exemple, une application cherchant à extraire les prénoms et patronymes d'un document aura besoin d'entités nommées très courtes, ne contenant que les informations recherchées. Dans l'entité nommée mixte \emph{le président Jacques Chirac}, seuls le prénom et le nom lui seront utiles. La recherche de la borne gauche ne devra donc pas englober le nom commun \emph{président}.



%Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.

%Dans un enregistrement audio, chaque prise de parole d'un locuteur est appelée \og tour de parole \fg. Ces tours de paroles commencent quand un locuteur prend la parole et s'arrêtent lorsqu'un autre locuteur prend la parole (ou lorsqu'un jingle ou une publicité coupe l'intervention). Ces locuteurs peuvent n'intervenir qu'une fois dans le fichier, auquel cas ils n'auront qu'un tour de parole. Mais, comme les journalistes ou les présentateurs, beaucoup de locuteurs sont présents tout au long du fichier et prennent la parole à plusieurs endroits dans l'enregistrement. Ces différents tours de paroles appartiennent au même locuteur, ils sont alors regroupés au sein de la même \og classe de locuteur \fg{}. La figure \ref{fig:decoupage_tours_classes} donne un exemple de découpage en tours de parole et de regroupement en classes de locuteurs.



%On peut noter sur la figure \ref{fig:decoupage_tours_classes}, qu'en plus de regrouper les tours de parole par classe de locuteur (classe du Locuteur 1, classe du Locuteur 2), le genre du locuteur est aussi mentionné. Cette information vient enrichir la transcription et pourra être utile par la suite. Les classes de locuteur sont pour l'instant anonymes. En effet, la transcription ne nous permet pas de dire que c'est \og Monsieur untel \fg{} qui parle, elle permet juste de dire que c'est le même locuteur qui parle à deux endroits différents. La solution idéale serait de pouvoir nommer les classes de locuteurs par le prénom et le patronyme du locuteur concerné (couple que nous appellerons par la suite nom complet), c'est ce que les techniques d'identification nommée du locuteur cherchent à réaliser.


%\chapter{Systèmes automatiques}
%\minitoc
%
%\label{sec:sys_auto}
%
%Depuis les débuts de l'informatique, l'Homme a toujours essayé de l'utiliser pour automatiser des tâches humaines. Que ce soit de la simple calculatrice à la reconnaissance de la parole, l'informatique est de nos jours utilisée dans tous les domaines. Mais pour pouvoir réaliser ces tâches, les programmes informatiques doivent avoir une certaine connaissance des actions à effectuer pour pouvoir les réaliser.
%
%Il existe deux principales façons d'insuffler ces connaissances à un programme. La première est de tout simplement lui donner les règles qui vont lui permettre de réaliser les tâches qu'il a à accomplir. Dans l'exemple de la calculatrice, il suffit de lui fournir les différentes formules arithmétiques pour qu'elle puisse fonctionner. Ce type de procédé marche bien lorsqu'il existe des règles formelles pour réaliser une tâche. Mais ces règles ne sont pas toujours disponibles ou peuvent varier fortement en fonction du contexte, c'est par exemple le cas en reconnaissance de la parole.
%
%La reconnaissance de la parole fait intervenir plusieurs domaines complexes à gérer pour un système automatique. Il faut tout d'abord transformer le signal audio en paramètres exploitables par un programme informatique. Il faut ensuite transformer ce signal audio en phonèmes, puis les phonèmes en mots et les mots en phrases. Ces étapes dépendent de beaucoup de paramètres comme les conditions d'enregistrement, la langue utilisée dans le document, l'accent des personnes qui parlent ou leur façon d'utiliser la langue (néologismes, mauvaises constructions grammaticales). En plus de variabilité de ces paramètres, il est impossible de définir des règles formelles pour encadrer chacun d'eux.
%
%Prenons l'exemple d'un enregistrement dans lequel les locuteurs parlent français. Même s'il existe une grammaire et un orthographe bien définis pour la langue française écrite, ce n'est pas le cas à l'oral. En effet les contractions (cheval sera prononcé ch'val), les faux départs et les répétitions sont légions. En plus de ces différences entre le français écrit et le français parlé, les locuteurs peuvent très bien faire des fautes de grammaire ou employer des mots qui ne font pas parti du dictionnaire courant. Toutes ces variations dans l'utilisation de la langue française rendent difficile l'utilisation de règles formelles pour reconnaître les mots prononcés. C'est pour pallier à ces problèmes que les systèmes de reconnaissance automatique de la parole utilisent des techniques d'apprentissage automatique qui vont permettre au système d'apprendre automatiquement de nouvelles connaissances en fonction des données qui lui seront fournies.
%
%\section{Apprentissage automatique}
%\subsection{Généralités}
%
%L'apprentissage automatique (qui est un sous-domaine de l'Intelligence Artificielle) a pour but de permettre à une machine, un programme, de s'adapter et d'évoluer grâce aux données qui lui sont fournies, de manière à remplir des tâches complexes qu'il serait difficile, voir impossible, de remplir avec des méthodes plus traditionnelles. Un des défis de l'apprentissage automatique est d'extraire les données pertinentes présentes dans les corpus d'apprentissage afin d'en tirer des généralités applicables à d'autres données. L'apprentissage automatique repose sur la notion de classification. La classification va viser à étiqueter chaque donnée en l'associant à une classe. Ces classes peuvent être connues à l'avance ou déterminées automatiquement par le système. C'est ce qui va différencier l'apprentissage supervisé de l'apprentissage non supervisé. 
%
%\subsection{Apprentissage supervisé}
%\label{sssec:apprentissage_supervise}
%L'apprentissage supervisé va permettre d'apprendre des règles à partir de données d'apprentissage servant d'exemple. Ces données sont généralement validées et étiquetées par des êtres humains. C'est à partir de ces exemples déjà classés que le système va apprendre des règles essayant de reproduire la classification à partir des données. Par exemple, les arbres de décision décrits dans \ref{sec:sct} sont des outils utilisant un apprentissage supervisé, puisqu'ils vont permettent de ranger des données de test dans des classes prédéfinies. Ils auront été appris grâce à des données d'apprentissage étiquetées au préalable avec les bonnes classes.
%
%\subsection{Apprentissage non supervisé}
%
%L'apprentissage non supervisé (aussi appelé classification automatique) consiste à trier un groupe hétérogène de données en regroupant les données homogènes au sein d'une même classe, et les données hétérogènes au sein d'autres classes. À la différence de l'apprentissage supervisé, il n'y a pas de \og sortie attendue \fg{}. C'est à dire que c'est au système d'établir ses propres classes, au lieu de devoir classer les données dans des classes prédéfinies comme pour l'apprentissage supervisé. En reconnaissance automatique de la parole, et plus précisément dans le domaine de la reconnaissance du locuteur, les modèles à mélange de gaussiennes (aussi appelés GMM) sont un bon exemple de système d'apprentissage non supervisé. Ils permettent de représenter une distribution multidimensionnelle quelconque par une somme pondérée de distributions gaussiennes. Chaque distribution gaussienne étant caractérisée par un vecteur moyen et une matrice de covariance.

%\subsection{Transcription enrichie}
%\label{sub:transcription_enrichie}

%Transcrire un signal audio consiste d'une part à retranscrire les mots qui ont été prononcés mais aussi à enrichir cette transcription avec différentes informations éventuellement disponibles comme le début et la fin de chaque intervention, le nom des différents locuteurs ou encore leur genre. Ces transcriptions peuvent être réalisées par un humain (elles seront appelées transcriptions manuelles) ou de manière entièrement automatique (elles seront appelés transcriptions automatiques) : en fonction de cela, les informations disponibles dans la transcription peuvent varier, c'est notamment le cas du nom des locuteurs. En effet, un humain pourra essayer, en fonction du contexte, de nommer les locuteurs de l'enregistrement à l'inverse d'un système automatique qui utilisera des étiquettes anonymes en guise de nom de locuteur (comme locu1, locu2, ...). C'est précisément à cet aspect de la transcription enrichie (à savoir le nommage des locuteurs) que les travaux sur l'identification nommée du locuteur s'intéressent. Il convient tout d'abord d'expliquer plus précisément les différents aspects d'une transcription enrichie.



% \subsubsection{Transcription et étiquetage des entités nommées}
% 
%   La transcription du signal audio consiste tout simplement à écrire les mots qui ont été prononcés par les locuteurs. Que cette transcription soit réalisée par un humain ou par un système automatique de reconnaissance de la parole (RAP), elle consiste à remplir les différents tours de parole avec les mots correspondants. QUand les transcriptions sont réalisées par des systèmes de RAP, elles sont beaucoup moins riches que celles produites par un humain. En effet, en plus des erreurs de transcription, elles ne contiennent généralement pas de ponctuation. Ce manque peut être comblé par des post-traitements qui essaient de remettre la ponctuation dans la transcription. 
% 
% Un autre type de post-traitement utile pour l'identification nommée du locuteur est la détection des entités nommées. En effet, pour pouvoir attribuer un nom complet à un locuteur à partir de la transcription, il va d'abord falloir être capable de détecter ce nom complet dans la transcription. Pour ce faire, il existe des systèmes de détection des entités nommées qui vont être capable de détecter, en plus des noms complets, différentes entités nommées comme les lieux, les organisations ou encore les radios. La figure \ref{fig:etapes_transcription_enrichie} montre toutes les étapes pour obtenir une transcription enrichie en entités nommées.


% \subsection{Les systèmes automatiques : reconnaissance de la parole et détection des entités nommées}
% 
% À l'heure où l'informatique et Internet sont partout, les quantités de données numériques ne cessent de croître. Ces grandes collections de données sont difficilement indexables manuellement, il faut donc, pour faciliter la recherche et l'accès à l'information, qu'elles soient traitées de manière automatique. Les enregistrements audio sont traités à l'aide de systèmes de reconnaissance automatique de la parole (SRAP), ces systèmes sont constitués de plusieurs composantes comment le montre la figure \ref{fig:principes_srap}.
% 
% \begin{figure}
% 
% \begin{center}
% \includegraphics[width=0.9\columnwidth]{img/sys_rap}
% 
% \caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
% \label{fig:principes_srap}
% \end{center}
% 
% \end{figure}
% 
% \subsubsection{Principes généraux} % (fold)
% \label{ssub:principes_generaux}



% subsubsection principes_généraux (end)

% \subsubsection{Segmentation et classification} % (fold)
% \label{ssub:les_systemes_de_segmentation_et_de_classification_en_locuteurs}
% 
% Afin de traiter de manière efficace un signal complexe comme le signal audio, les SRAP ont besoin de segmenter le signal en parties homogènes appelées segments. Ces segments se doivent d'être cohérents, un même segment doit avoir les mêmes conditions d'enregistrement, doit être prononcé par le même locuteur, ... Ces segments sont caractérisés par des conditions acoustiques spécifiques comme la présence de parole, la nature de la parole (téléphonique ou enregistrement studio), la présence de musique, le genre du locuteur ou encore son identité ou une étiquette anonyme. Comme décrit dans \ref{ssub:decoupage_regroupement}, l'identification nommée utilise les tours de parole. Un tour de parole n'est en fait qu'une suite de segments dont l'identité ou l'étiquette sont identiques.
% 
% 
% Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.
% 
% 
% Les méthodes qui donnent les meilleurs résultats sur les journaux radiophoniques utilisent un regroupement BIC suivi d'un regroupement de type CLR [1, 2]. C'est ce type de système \ref{article sylvain} qui sera utilisé pour les travaux du présent papier.
% 
% Ces méthodes sont évaluées selon une métrique nommée DER (Diarization Error Rate). TODO : expliquer et donner la formule

% subsubsection les_systèmes_de_segmentation_et_de_classification_en_locuteurs (end)

% \subsubsection{Transcription automatique de la parole} % (fold)
% \label{ssub:transcription_automatique_de_la_parole}
% 
% Les systèmes de transcription automatique de la parole sont évalués selon une métrique nommée WER (Word Error Rate). TODO : expliquer et donner la formule

% subsubsection transcription_automatique_de_la_parole (end)


% \subsubsection{Détection des entités nommées} % (fold)
% \label{ssub:detection_des_entites_nommees}

% subsubsection détection_des_entités_nommées (end)


\chapter{L'identification nommée du locuteur}
\minitoc
\newpage

L'identification nommée du locuteur (INL) consiste à nommer les locuteurs d'un document audio en leur attribuant un prénom et un patronyme. Ces informations viennent enrichir la transcription et peuvent être utilisées dans plusieurs domaines d'applications. 

\section{Applications}

Disposer des noms complets des intervenants d'un document audio peut être utilisé à des fins de recherche d'informations. En effet, les bases de données audio et vidéo ne cessent de croître avec la numérisation massive des documents audiovisuels. Ainsi, les besoins en indexation et recherche automatiques croissent de la même manière. Par exemple en France, l'Institut National de l'Audiovisuel (INA) a lancé un plan de sauvegarde et de numérisation de ses différents documents en 1999. Ce plan a pour but de numériser plusieurs millions d'heures d'enregistrements de radio et de télévision. Il doit permettre de ne pas perdre les données qu'elles contiennent à cause de l'obsolescence des supports. Suite au lancement de ce plan de sauvegarde, de nombreux travaux ont été menés à l'INA sur l'indexation automatique des documents \cite{Veneau2001,Allauzen2003,Joly2004,Poli2007}. L'utilisation de l'INL pour ce type de bases de données pourrait apporter un réel plus pour l'indexation. Elle permettrait par exemple de faciliter la recherche des différents intervenants dans les documents indexés ou encore de remplir automatiquement les grilles de programmes avec le nom des intervenants.

Les récentes campagnes d'évaluation des systèmes de reconnaissance automatique de la parole (cf. \ref{ssec:trans_broadcast}) s'intéressent à l'enrichissement de la transcription (cf. \ref{sec:trans_enrichie}) par diverses informations comme les entités nommées ou encore les informations sur les locuteurs du document. Cette transcription contient notamment une segmentation et une classification en locuteurs (cf. \ref{ssub:decoupage_regroupement}) qui permettent de délimiter les différentes interventions dans l'enregistrement. En revanche, les informations fournies par le processus de segmentation et de classification en locuteur ne permettent pas d'identifier les locuteurs du document par leur prénom et patronyme. Les locuteurs sont uniquement identifiés par des labels anonymes du type \emph{Locuteur1}, \emph{Locuteur2}. L'INL s'inscrit dans l'enrichissement de la transcription en permettant de remplacer ces labels anonymes par le prénom et le nom des locuteurs. Ce couple prénom/patronyme sera par la suite désigné comme étant le \textbf{nom complet} du locuteur.

Afin d'évaluer les différents systèmes d'INL, une même métrique est utilisée. Nous commençons par décrire cette métrique commune avant de présenter les différents systèmes par la suite.

%Contrairement à la reconnaissance de la parole et aux tâches de reconnaissance du locuteur présentées dans le chapitre précédent, l'identification nommée de locuteurs est un domaine de recherche relativement récent. En effet, ce sont les travaux du LIMSI et plus particulièrement de Leonardo Canseco-Rodriguez \cite{CansecoRodriguez2005}, qui ont été les premiers à s'intéresser à l'identification nommée du locuteur utilisant la transcription enrichie (réalisée manuellement) d'enregistrements radiophoniques.

\section{Métrique d'évaluation}

L'évaluation va consister à comparer les noms complets des locuteurs des transcriptions de référence aux locuteurs proposés par le système d'INL. Ces comparaisons se basent sur les opérations élémentaires permettant de comparer deux chaînes de caractères. Plusieurs cas possibles sont étudiés lors de la comparaison entre la référence et les hypothèses fournies par le système d'INL:

\begin{itemize}
  \item Correct ($C$)~: le système propose une identité correspondant à celle indiquée dans la référence ;
  \item Substitution ($S$)~: le système propose une identité différente de l'identité présente dans la référence ;
  \item Insertion ($I$)~: le système propose une identité alors que le locuteur n'est pas identifié dans la référence ;
  \item Suppression ($D$) : le système ne propose pas d'identité alors que le locuteur est identifié dans la référence ;
  \item Inconnu ($U$)~: le système ne propose pas d'identité et la référence ne contient pas d'identité.
\end{itemize}



Une mesure de Précision et de Rappel peut être définie à partir des 5 cas d'erreur~:
\begin{equation}
	\label{eq:PR}
	P = \frac{C}{C+S+I} \ \ ; \ \ R = \frac{C}{C+S+D}
\end{equation}

%Dans tous les précédents articles \cite{Tranter06,Esteve07,Chengyuan07} traitant de l'identification nommée du locuteur, les résultats sont présentés sous la forme de mesures de précision et de rappel.

Ces valeurs peuvent être complétées par un taux d'erreur $E$ global. Ce taux s'inspire du calcul du $WER$ utilisé pour l'évaluation de la transcription. Il a l'avantage de mesurer la qualité des résultats du système d'identification nommée en une seule valeur, facilitant les comparaisons entre les systèmes par rapport aux mesures de précision et de rappel.
\begin{equation}
	\label{eq:PR}
	E = \frac{S+I+D}{S+I+D+C+U} \ \ ;
\end{equation}

À noter que tous les résultats sont exprimés en terme du durée : plus les locuteurs s'exprimant de manière conséquente sont détectés, plus les résultats sont élevés~: le nombre de locuteurs correctement nommés n'est pas pris en compte. En section \ref{sec:metrique} les différentes métriques d'évaluation sont approfondies.

Afin de pouvoir attribuer aux locuteurs d'un document leurs noms complets, il faut obtenir leurs prénoms et patronymes. Deux principales approches sont possibles : disposer d'informations a priori sur les locuteurs comme dans le cas du suivi de locuteur (cf. \ref{ssec:sl}) ou utiliser les informations fournies par le document lui même pour déterminer l'identité des locuteurs. Nous commençons par présenter la première approche avant de nous intéresser plus longuement à la deuxième approche qui constitue le c\oe{}ur de cette thèse.

\section{Utilisation de connaissances a priori}
\label{sub:suivi_locuteur}

Obtenir l'identité d'un locuteur a d'abord été réalisé avec des méthodes purement acoustiques \cite{Bimbot2004}. Le suivi de locuteur (décrit dans le chapitre \ref{sec:ral}) peut être étendu à $n$ locuteurs afin d'identifier tous les locuteurs d'un document. Bien qu'utilisées depuis des années, ces méthodes présentent plusieurs inconvénients.

Tout d'abord, il est nécessaire de connaître les personnes que l'on cherche à identifier. Les systèmes doivent commencer par apprendre des modèles acoustiques correspondant à chaque locuteur du document pour ensuite les associer aux différents intervenants de l'enregistrement traité. Ces systèmes ne peuvent identifier que des personnes dont ils possèdent déjà les modèles. De plus, pour apprendre ce modèle acoustique, une quantité de données suffisante est nécessaire~: plusieurs minutes sont un minimum.

Enfin, les conditions acoustiques des données d'apprentissage doivent être similaires à celles sur lesquelles le système va chercher à détecter les locuteurs : des données trop éloignées dans le temps (et donc avec une voix qui peut avoir changé) ou des conditions d'enregistrement différentes (studio ou téléphone par exemple) dégraderont les performances d'identification (cf \ref{ssec:carac_variabilite}).

Dans l'hypothèse d'une utilisation sur des enregistrements de journaux radiophoniques par exemple, il faudrait disposer des enregistrements de tous les locuteurs (présentateurs, invités, intervenants, etc.) en quantité suffisante. S'il semble plausible de pouvoir les obtenir pour les journalistes qui sont souvent présents dans les émissions de radio, il semble beaucoup plus compliquer de les obtenir pour certains invités peu connus ou pour les inconnus interrogés par téléphone. 

Ces différentes limitations ont motivé les récents travaux ne nécessitant pas de connaissances a priori sur les locuteurs pour réaliser l'identification. Ces travaux se basent sur la transcription du signal audio pour identifier les locuteurs d'un enregistrement. 

\section{Utilisation des informations de la transcription}

Sans connaissance a priori, ces méthodes doivent extraire les noms complets à partir des informations disponibles dans le signal audio. La transcription en mots du signal est une source d'information de choix pour trouver les prénoms et les patronymes des locuteurs : dans beaucoup d'enregistrements (et notamment ceux provenant des journaux radiophoniques) les noms complets des intervenants font partie de la transcription. L'identification nommée consiste donc à détecter les noms complets présents dans la transcription, pour ensuite les attribuer aux différents locuteurs du document. 

La figure \ref{fig:principe_inl} donne un aperçu de ce que l'identification nommée à partir de la transcription cherche à réaliser. 

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/discussion_nommee}

\caption{Aperçu global de l'identification nommée du locuteur utilisant la transcription}
\label{fig:principe_inl}
\end{center}

\end{figure}

\subsection{Hypothèses}
\label{ssec:hypotheses}
Identifier les locuteurs en se basant sur la transcription d'un document audio nécessite que plusieurs hypothèses soient vérifiées~:

\begin{itemize}

    \item Les locuteurs sont \textbf{annoncés par leurs noms complets} dans le document.


À partir du moment où les noms complets des locuteurs sont présents dans la transcription d'un document audio, il semble possible de les exploiter pour identifier les différents intervenants. 

C'est une hypothèse forte, qui permet d'exploiter un grand nombre de documents. Les journaux d'informations (qu'ils soient radiophoniques ou télévisuels) sont par exemple tout à fait adaptés à ce style de techniques. Dans ce type de document, la majorité des locuteurs s'annoncent ou sont annoncés. C'est systématiquement le cas des présentateurs et des invités par exemple. Dans les enregistrements de réunions, les locuteurs sont présentés et se passent la parole~: il est donc possible de récupérer leurs noms complets. De manière plus générale, tout type de document où les locuteurs s'annoncent ou sont annoncés vont pouvoir être exploités avec les techniques d'identification nommée basées sur la transcription du signal audio.

    \item Les noms complets sont exploitables dans la transcription : ils ont été \textbf{correctement transcrits}.

        Les systèmes d'INL doivent être capable de transcrire correctement les noms complets présents dans l'enregistrement si ils veulent pouvoir les exploiter. Lorsqu'ils sont bien transcrits, c'est ensuite le travail des systèmes de détection des entités nommées (présentés dans la section \ref{ssec:EN}) d'identifier les différents noms complets.
        
        Dans le cadre de la mise en place de systèmes automatiques d'INL, cette hypothèse est importante~: la transcription des noms propres est un réel problème pour les systèmes automatique de reconnaissance de la parole. \tocite{ref reco noms propres}.

    \item Le \textbf{contexte lexical} au voisinage d'un nom complet permet de déterminer si ce nom complet est un \textbf{locuteur du document}.

        La dernière hypothèse est d'être capable d'attribuer un nom complet présent dans la transcription à un des locuteurs du document. Pour ce faire, il faut que le contexte lexical de chaque nom complet permette de déterminer si ce nom complet se rapporte à un locuteur du document, et à quel locuteur. C'est ce que la phase d'attribution locale des systèmes d'INL cherche à effectuer.
\end{itemize}

%\subsection{Présence des noms complets}
%
%\label{ssec:presence_noms_complets}
%
%À partir du moment où les noms complets des locuteurs sont présents dans la transcription d'un document audio, il semble possible de les exploiter pour identifier les différents intervenants. C'est l'hypothèse de base de toutes les méthodes utilisant la transcription pour réaliser l'identification nommée~: les noms complets des locuteurs du document sont présents dans le document lui même.
%
%C'est une hypothèse forte, qui permet d'exploiter un grand nombre de documents. Les journaux d'informations (qu'ils soient radiophoniques ou télévisuels) sont par exemple tout à fait adaptés à ce style de techniques. Dans ce type de document, la majorité des locuteurs s'annoncent ou sont annoncés. C'est systématiquement le cas des présentateurs et des invités par exemple. Dans les enregistrements de réunions, les locuteurs sont présentés et se passent la parole : il est donc possible de récupérer leurs noms complets. De manière plus générale, tout type de document où les locuteurs s'annoncent ou sont annoncés vont pouvoir être exploités avec les techniques d'identification nommée basées sur la transcription du signal audio.
%
%Les systèmes d'INL devront donc être capable de transcrire et de détecter ces noms complets à partir du signal pour pouvoir ensuite les exploiter. C'est le travail des systèmes de détection des entités nommées présentés dans la partie \ref{ssec:EN}.
%
%Mais disposer de documents dans lesquels les noms complets des locuteurs sont cités n'est pas suffisant, il faut ensuite être capable d'attribuer ces noms complets aux différents intervenants du document. La partie suivante décrit le principe d'attribution commun à toutes les méthodes d'INL basées sur la transcription.
%

\subsection{Attribution locale}
\label{ssec:attributions_locales}

La technique utilisée pour attribuer un nom complet détecté à un des locuteurs du document est commune à tous les systèmes d'INL à partir de la transcription. Cette technique consiste à analyser le contexte lexical d'un nom complet de manière à déterminer si ce nom complet se rapporte à un locuteur qui est en train de parler (tour de parole courant), au locuteur qui parle ensuite (tour de parole suivant) ou au locuteur qui vient de parler (tour de parole précédent). La figure \ref{fig:segment} résume le principe de cette attribution, que l'on doit aux travaux de Leonardo-Canseco \cite{CansecoRodriguez2005}.


\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/segment.pdf}
\caption{Principe de base des systèmes d'identification nommée basés sur la transcription}
\label{fig:segment}
\end{center}
\end{figure}

Typiquement, un nom complet peut être attribué au locuteur courant lorsque celui-ci s'annonce ou se présente. Par exemple les journalistes ont pour habitude de finir leurs interventions en rappelant leur identité, comme par exemple \emph{\og c'était Paul Dupond en direct de \ldots \fg}. Ici, le nom complet \emph{Paul Dupond} devrait être attribué au locuteur qui a prononcé cette phrase (le locuteur courant). À l'inverse, un locuteur prononçant cette phrase \emph{\og Nous écoutons maintenant, Jean Durand.\fg{}} annonce une personne qui va parler après lui. Le nom complet \emph{Jean Durand} devra donc être attribué au locuteur suivant. Pour finir, une phrase du type \emph{\og Merci Maude Bayeu, nous passons maintenant à la météo \fg{}} rappelle le locuteur qui vient de s'exprimer juste avant. Le nom complet \emph{Maude Bayeu} sera donc attribué au locuteur précédent.

Un des problèmes que les systèmes d'INL ont a résoudre est d'attribuer un nom complet détecté au tour de parole correspondant (précédent, courant ou suivant) ou, à défaut, de ne pas réaliser d'attribution si le nom complet ne correspond pas au locuteur d'un des tours de parole courant ou contigu. C'est par exemple le cas des noms complets qui ne font pas partie du document (\emph{Jean-Jacques Rousseau} aura par exemple peu de chance d'être un des locuteurs du document) ou qui font référence à des locuteurs présents plus loin dans le document. En effet, même si un nom complet détecté correspond bien à un des locuteurs du document, il ne sera pas exploité par ces techniques s'il ne correspond pas à un des locuteurs des tours de parole contigus ou courant. Ce cas est appelé \og autre \fg{} sur la figure \ref{fig:segment}.

Ce type d'attribution sera par la suite appelée \og attribution locale \fg. On attribue ici un nom complet à un des tours de parole contigu au tour de parole où le nom complet a été détecté. Aucune attribution au niveau du document entier n'est pour l'instant effectué, seuls les tours de paroles contigus sont concernés. La propagation de ces attributions locales au sein de l'ensemble des tours de parole est une étape appelée \og attribution globale \fg.

\subsection{Attribution globale}

\label{attributions_globales}

Afin de bien comprendre la problématique de l'attribution globale des noms complets, il faut placer les attributions locales dans le contexte d'une transcription enrichie (cf. \ref{sec:trans_enrichie}).

En effet, chaque nom complet détecté dans un tour de parole est attribué au tour de parole courant, précédent ou suivant. Chacun de ces tours de parole appartient à un locuteur, que l'INL cherche à nommer en lui attribuant un et un seul nom complet. Or, plusieurs noms complets peuvent être candidats pour le même locuteur. Comme le montre la figure \ref{fig:decisions_locales_conflit}, il est tout à fait possible que plusieurs noms différents soient proposés pour un même locuteur. Dans la figure \ref{fig:decisions_locales_conflit}, deux noms complets sont possibles pour le LOCUTEUR 2~: Maude Bayeu et Pierre Moscovici.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/decision_multiple.pdf}
\caption{Exemple de décisions locales conflictuelles}
\label{fig:decisions_locales_conflit}
\end{center}
\end{figure}

L'attribution du nom complet au locuteur consiste à choisir un nom complet parmi les noms complet obtenus à partir des décisions locales. Cette phase sera nommée \og décision globale \fg.

%Chaque tour de parole appartenant à une même classe de locuteur, les conflits des tours de parole vont se répercuter au sein de la classe de locuteur. En effet, plusieurs noms complets peuvent être candidats pour la même classe de locuteur, il faudra alors choisir quel nom complet attribuer à cette classe. La figure \ref{fig:processus_entier} montre un exemple du processus global, avec tous les conflits que cela engendre.


\subsection{Processus d'attribution}

Le processus d'attribution global se résume en quatre principales étapes décrites dans la figure \ref{fig:processus_entier}~:

\begin{itemize}
    \item Préparation d'une transcription enrichie
    \item Étape d'attributions locales
    \item Étape d'attributions globales
    \item Production d'une transcription avec les locuteurs nommées
\end{itemize}

Dans l'étape 1, la transcription en mots est enrichie d'une segmentation et d'une classification en locuteurs ainsi que d'un étiquetage des entités nommées. Chacune des entités nommées de type \emph{PERSONNE} est ensuite attribuée localement au tour de parole correspondant dans l'étape 2. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/entire_process_fr.pdf}
\caption{Vue globale du processus attribution d'un nom complet à une classe de locuteur anonyme}
\label{fig:processus_entier}
\end{center}
\end{figure}

Par exemple, le nom complet \emph{C} est attribué aux tours de parole 5 et 11, qui correspondent respectivement à deux classes de locuteur différentes (classe du locuteur 3 et classe du locuteur 1). Un nom complet ne pouvant être attribué qu'à une classe de locuteur, \emph{C} se retrouve donc en conflit entre la classe du locuteur 3 et la classe du locuteur 1. Le nom \emph{B} quant à lui se retrouve aussi en conflit, puisqu'il est attribué aux tours de parole 1 et 8 qui appartiennent respectivement à la classe du locuteur 1 et à la classe du locuteur 2.

À l'issue de cette seconde étape, plusieurs noms complets sont donc possibles pour une même classe de locuteur : la classe du locuteur 1 peut être nommée avec \emph{A},\emph{B} ou \emph{C}, la classe du locuteur 2 avec \emph{A} ou \emph{B} et la classe du locuteur 3 avec \emph{C}. En revanche les classes de locuteur 4 et 5 n'ont pas de candidats possibles. Les méthodes d'INL utilisent un processus de décision (lors de l'étape 3) permettant de régler les conflits entre ces attributions globales afin de nommer chaque classe avec un et un seul nom complet.

Une fois les conflits de la phase d'attribution globale résolus, la dernière étape consiste à répartir au sein du fichier les différentes décisions. 


Même si les façons de réaliser les différentes étapes différent entre les méthodes d'identification nommée, les principes qui viennent d'être décrits sont communs à toutes. Le traitement d'un fichier audio avec un système d'identification nommée se résume donc aux étapes qui sont décrites dans la figure \ref{fig:fonctionnement_general}.

Deux principales approches ont été étudiées pour l'attribution locale des noms complets, à savoir une approche de type symbolique avec l'utilisation de règles écrites manuellement, et plusieurs approches à base de méthodes statistiques. Nous commençons par décrire la première approche utilisée dans la littérature, à savoir l'approche symbolique. Les méthodes à base de systèmes statistiques sont décrites dans la suite de cette thèse.


\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/fonctionnement_general.pdf}
\caption{Aperçu global d'un processus d'identification nommée du locuteur}
\label{fig:fonctionnement_general}
\end{center}
\end{figure}


\section{Approche symbolique}
\label{sub:regles_manuelles}

 Les travaux réalisés par le Canseco-Rodriguez \cite{CansecoRodriguez2005,CansecoRodriguez2006} ont été les premiers à montrer que le couple prénom/patronyme d'un locuteur apparaissant dans un contexte lexical donné permettait d'identifier de manière précise l'identité des locuteurs s'exprimant dans les tours de parole contigus. Ces travaux sont à la base de toutes les techniques d'INL, puisque ce sont eux qui ont défini le principe de l'attribution locale décrit en section \ref{ssec:attributions_locales}.
 
\subsection{Règles linguistiques}

Ces travaux s'appuient sur l'utilisation de règles linguistiques pour réaliser l'attribution locale des noms complets. L'idée est d'utiliser le contexte linguistique de chaque nom complet pour lui attribuer une des trois étiquettes (\og locuteur précédent \fg, \og locuteur courant \fg ,\og locuteur suivant \fg). Ces règles ont été définies manuellement après analyse d'un corpus de langue anglaise (\emph{Hub-4e}). Ce corpus est composé de transcriptions manuelles d'émissions de radio et de télévisions américaines (ABC, CNN, CSPAN et NPR) enregistrées entre 1993 et 1998. Comme décrit en section \ref{ssec:hypotheses}, ce type de corpus se prête particulièrement à l'utilisation de méthodes d'INL basées sur la transcription.

Dans le but de réaliser des règles les plus génériques possibles, certains mots ou groupes de mots ont été regroupés au sein de classes sémantiques. Ces groupes de mots ont ensuite été substitués par leur classe sémantique dans les textes. Par exemple, tous les noms lieux sont remplacés par la classe sémantique [location]. De ce fait, les deux règles \og reporting from Bagdad \fg{} et \og reporting from Paris \fg{} peuvent être scindées en une seule règle \og reporting from [location] \fg. Plusieurs dictionnaires de classes sont utilisés pour remplacer les mots. Les dictionnaires utilisés, majoritairement récupérés à partir d'internet, sont au nombre de 6 :


\begin{itemize}
	\item \textbf{noms complets} (prénom et patronyme) de locuteurs ([name]) : ce dictionnaire contient les noms anglais les plus communs, y compris les personnes célèbres et les personnalités internationales. Ce dictionnaire a été complété par des informations trouvées dans les textes à traiter, et notamment par l'identité des différents locuteurs (tous corpus confondus).
	\item \textbf{toponymes} [(location]) : ce dictionnaire est constitué des lieux géographiques les plus courants comme les villes, les régions, les états, les monuments... Ils ont été récupérés à partir de journaux en ligne. La majorité des lieux se situent aux États-Unis.
	\item \textbf{noms d'émissions} ([show]) : ce dictionnaire contient le nom des émissions et des radios. Les informations ont été extraites des transcriptions de référence (tous corpus confondus).
	\item \textbf{titres} ([title]) : ce dictionnaire est composé des titres (Monsieur, Madame, ...), des rangs militaires et des professions.
	\item \textbf{vocabulaire de la communication} : c'est un dictionnaire global incluant une variété de mots et d'expressions utilisés pour gérer la communication. Il contient une sélection des mots les plus fréquemment utilisés pour les remerciements ([thanks]), les salutations ([greet]), les consentements ([agree]), les questions ([quest]), les réflexions personnelles ([reflect]) et les pronoms démonstratifs ([dem]).
  \item \textbf{flexions} : ce dictionnaire contient la liste complète des verbes anglais. Ils sont conjugués et classés comme verbe à l'infinitif ([infverb]), au passé ([pasverb]), au présent ([preverb]) ou au participe ([parverb]).
\end{itemize}

À partir du corpus de développement de 150 heures d'enregistrements de radio et de télévision en langue anglaise (corpus Hub-4e), les règles les plus fréquentes pour trouver l'identité des locuteurs ont été extraites. Elles sont décrites dans le tableau \ref{table:canseco_regles}. Au final, douze règles sont utilisées pour désigner le locuteur courant, 34 pour le suivant et 6 pour le précédent.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      3162 & [title] [name] \\
      848 & I am [name] \\
      673 & [show]'s [name] \\
      382 & [agree] [name] \\
      293 & [name] [show] [location] \\
      186 & [show]'s [name] reports \\
      176 & [thanks] [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Règles les plus fréquentes pour déterminer l'identité d'un locuteur sur le corpus de développement}
  \label{table:canseco_regles}    
\end{table}

À ces règles viennent s'ajouter d'autres mécanismes comme l'utilisation d'un caractère générique (*) permettant de remplacer n'importe quel mot ou suite de mots, de manière à généraliser encore un peu plus les règles. Des exemples de règles avec caractère générique sont données dans le tableau \ref{table:canseco_joker}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      458 & with [comm] us * [name] \\
      109 & joining * [name] \\
      108 & [name] * joins \\
      45 & with * [comm] me \\
      24 & [comm-agreement] * [name] reporting \\
      24 & we are joined * by [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Exemple de règles utilisant des joker (*) sur le corpus de développement}
  \label{table:canseco_joker}    
\end{table}

\subsection{Corpus et métriques d'évaluation}

L'évaluation a été réalisée sur un corpus de test de 10 heures de données en langue anglaise (corpus Hub-4e, années 1997, 1998 et 1999). Deux types de transcriptions enrichies ont été utilisées : transcriptions manuelles et automatiques. Le taux d'erreur mot pour les transcriptions automatiques est de 18~\%, le système utilisé est celui présenté dans \cite{Lamel02lightlysupervised}. 

Les erreurs réalisées par le système de transcription automatique, et notamment les erreurs de découpage en tours de paroles (produits par le système de segmentation et de classification en locuteur), posent problème pour l'évaluation des règles. Lorsqu'un tour de parole produit par le système automatique correspond à plusieurs tours de parole de la référence, il est difficile de décider si la règle doit être considérée comme bonne ou mauvaise. Pour résoudre ce problème, deux types de tours de parole ont été pris en compte dans les transcriptions réalisées automatiquement~: les tours de paroles \og corrects \fg, c'est à dire les tours de paroles qui correspondent aux tours de parole de la référence et les tours de paroles \og bruités \fg, c'est à dire les tours de paroles qui regroupent plusieurs tours de parole de la référence (à cause d'une erreur du système de segmentation et de classification). Les erreurs commises par le système sont ensuite calculées selon différents cas de figure~:

\begin{itemize}
  \item \textbf{Identité correcte, tour correct} (C1) : l'identité extraite de la transcription est associée avec un tour de parole correct et correspond exactement à l'identité indiquée dans la référence (prénom et patronyme).
  \item \textbf{Identité correcte, tour bruité} (C2) : l'identité extraite de la transcription est associée avec un tour de parole bruité et correspond exactement à une des identités indiquées dans la référence (prénom et patronyme).
  \item \textbf{Identité partielle, tour correct} (C3) : l'identité extraite de la transcription est associée avec un tour de parole correct et correspond partiellement à l'identité indiquée dans la référence.
  \item \textbf{Identité partielle, tour bruité} (C4) : l'identité extraite de la transcription est associée avec un tour de parole bruité et correspond partiellement à une des identités indiquées dans la référence.
  \item \textbf{Indéfini} : la règle correspond à un locuteur inconnu dans la référence (ces cas sont exclus des résultats de test)
  \item \textbf{Mauvaise identification} : Aucun des cas ci-dessus ne s'applique, c'est donc une erreur d'association avec un tour de parole.
\end{itemize}

\subsection{Résultats}

Le tableau \ref{tab:results_canseco} résume les résultats obtenus sur les transcriptions manuelles et automatiques. Sur le corpus de test avec les transcriptions manuelles, les règles identifiant le locuteur courant sont les plus fiables (aucune fausse identification), tandis que les règles identifiant le locuteur précédent (43,1~\% de fausse identification) et suivant (20,2~\% de fausse identification) présentent un taux d'erreur plus élevé. Cette tendance se confirme avec l'utilisation de transcriptions automatiques, les règles de type \og courant \fg{} représentent 8,0~\% des erreurs, les règles de type \og suivant \fg{} 19,0~\% des erreurs et les règles de type \og précédent \fg{} 50,0~\% des erreurs. Le taux d'erreur global des règles sur le corpus de test avec les transcriptions manuelles est d'environ 13~\%, contre environ 18~\% pour les transcriptions automatiques. 


\begin{table}[h]
  \begin{center}

    \small
    \begin{tabularx}{\linewidth}{cccc|ccc}
      %\cline{3-6}
      \hline
      \hline
      & \multicolumn{3}{c|}{Transcriptions manuelles}  & \multicolumn{3}{c}{Transcriptions automatiques} \\ 
      & \emph{courant} & \emph{suivant} & \emph{précédent} & \emph{courant} & \emph{suivant} & \emph{précédent} \\ \hline 
      \emph{c1} & 115 (95,0\%) & 50 (55,0\%) & 7 (16,0\%) & 94 (84,0\%) & 38 (60,3\%) & 8 (21,0\%) \\ 
      \emph{c2} & - & - & - & 2 (1,7\%) & 3 (4,8\%) & - \\
      \emph{c3} & 7 (5,0\%) & 22 (24,8\%) & 18 (40,9\%) & 7 (6,2\%) & 10 (15,9\%) & 11 (29,0\%) \\
      \emph{c4} & - & - & - & - & - & - \\
      \emph{\#Mau. id.} & - & 16 (20,2\%) & 19 (43,1\%) & 9 (8,0\%) & 12 (19,0\%) & 19 (50,0\%) \\
      \emph{\#Indéfini} & - & 3 & 1 & - & 2 & 1 \\ \hline
      \emph{Total} & 122 & 91 & 45 & 112 & 65 & 39 \\
      \hline
      \hline

      \hline
    \end{tabularx}
  \end{center}
  \caption{Taux d'erreur des règles manuelles sur les transcriptions manuelles et automatiques (corpus d'évaluation 97-98-99 Hub-4e)}
  \label{tab:results_canseco}
\end{table}



Ces travaux pionniers démontrent qu'identifier les locuteurs d'un document à partir de sa transcription est possible. En revanche, ils ne prennent pas en compte les possibilités offertes par les techniques d'apprentissage automatique~: les corpus nécessitent des traitements manuels. Le temps de mise en place de telles règles peut être long suivant la quantité de corpus à analyser et demande une expertise du domaine pour pouvoir être réalisée. De plus, le passage d'un corpus à un autre est fastidieux~: il faut réécrire le jeu de règles pour l'utiliser sur des documents d'une autre langue ou provenant d'autres types d'émissions.

Ces travaux ne traitent pas de l'attribution globale des noms complets mais uniquement de l'utilisation des règles locales. Ce problème d'attribution globale est abordé dans les différentes approches statistiques dans la section qui suit.


\section{Approche statistique : N-grammes}

\label{sub:regles_statistiques}


Suite aux travaux de Canseco-Rodriguez (2005), le laboratoire de l'Université de Cambridge s'est intéressé à l'identification nommée du locuteur \cite{Tranter06}. Leurs travaux consistent à automatiser l'apprentissage des règles linguistiques et à mesurer l'impact de leur système d'identification nommée sur des données provenant de systèmes automatiques (classification en locuteur et transcription automatiques).

\subsection{Attribution locale~: utilisation de N-grammes}
\label{Tranter}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=1\columnwidth]{img/n_gram_tranter.pdf}
%\caption{Exemple de n-grammes}
%\label{fig:tranter_n_gram}
%\end{center}
%\end{figure}

À la différence des précédents travaux, Cambridge utilise des N-grammes pour modéliser les règles linguistiques. Ils sont obtenus via un corpus d'apprentissage dans lequel la liste des personnes (leurs noms complets) intervenant dans les enregistrements est connue à l'avance. Les occurrences de ces noms complets sont détectées grâce à cette liste. 

Le contexte lexical de chaque nom complet locuteur du document est analysé. Une fenêtre glissante est utilisée sur ce contexte lexical~: sa taille varie de 2 à 5 mots (en incluant le nom complet). Pour chaque contexte lexical, le système va générer un ensemble de N-grammes (de 2 à 5-grammes) et va leur assigner une probabilité de désigner de manière correcte le locuteur suivant, courant ou précédent. Un N-gramme (avec un minimum de 2 mots et un maximum de 5) est appris pour chaque glissement de la fenêtre. Chaque N-gramme apparaissant plus d'un certain nombre de fois (cinq dans le cas de ces travaux) est considéré comme une règle de prédiction et est donc retenu comme règle linguistique.

Comme dans les précédents travaux de Canseco-Rodriguez (2005), Tranter (2006) utilise des classes sémantiques pour généraliser les règles. Ces classes sont issues des données d'apprentissage et ont été complétées avec d'autres sources (journaux écrits notamment). Ces classes sont les suivantes : GOODBYE, HELLO, OKAY, THANKS, LOCATION, PERSON, PERSON'S, SHOW, SHOW'S and TITLE. Ils regroupent ceux utilisés par Canseco, la concept [agree] devient OKAY, [greet] devient THANKS, etc. En revanche là où Tranter différencie HELLO et GOODBYE, Canseco-Rodriguez les regroupait au sein d'un même concept [comm]. De plus, Tranter n'utilise pas la catégorie [quest] et les verbes.

\subsection{Attribution globale}

Deux processus d'attribution globale ont été étudiés pour cette approche basée sur les N-grammes. Tout d'abord, Tranter a utilisé une combinaison de probabilités pour combiner les règles qui fournissent la même hypothèse. Par la suite, Microsoft \cite{Chengyuan07} a mis en place un modèle d'entropie maximum pour combiner les règles et certaines informations supplémentaires comme le genre des locuteurs.

Nous commençons par décrire le premier processus mis en place par Tranter avant de décrire plus en détail les ajouts réalisés par Chengyuan.

\subsubsection{Combinaison des règles linguistiques}

Toutes les règles précédemment apprises sont utilisées sur le corpus de test. Lorsqu'une règle avec une probabilité $p_1$ est déclenchée et suggère un nom $n_1$ pour un locuteur $s_a$, le score pour l'hypothèse $s_a = n_1$ est incrémenté.
Lorsque deux règles sont déclenchées pour le même nom de locuteur, leurs probabilités sont combinées en utilisant la formule :
\begin{equation}\mathbf{}
  p_{1+2}=1 - (1-p_1)(1-p_2)
\label{eq:sl}
\end{equation}

\subsubsection{Modèle d'entropie maximum}

Les travaux de \cite{Chengyuan07} proposent de remplacer la combinaison présentée dans la formule \ref{eq:sl} par un modèle d'entropie maximum \cite{BergerEntropie96}. Cette approche permet de représenter les différentes règles au sein d'un modèle statistique et d'y ajouter des connaissances supplémentaires.

Pour déterminer la probabilité d'attribuer un nom complet $n$ à un locuteur $s$, un ensemble de règles (N-grammes) applicables $\mathcal{R}(n)$ est tout d'abord déterminé. La combinaison de Tranter (cf. équation \ref{eq:sl}) peut être réécrite de cette manière :

\begin{equation}\mathbf{}
  P(n|s) \propto 1 - \prod_{r\in \mathcal{R}(s)}(1-p_r)
\label{eq:sl_new}
\end{equation}

Le modèle d'entropie maximum proposé s'exprime de la manière suivante :

\begin{equation}\mathbf{}
  P(n|s)=\frac{1}{Z_\lambda(s)} \exp(\sum_{k=1}^{F} \lambda_k f_k(n,s))
\label{eq:maxent}
\end{equation}

Les N-grammes sont utilisés comme attributs (\emph{features}) $f_k(n,s)$ du modèle d'entropie maximum. $\lambda_k$ représente les paramètres du modèle (multiplicateur de Lagrange) et $Z_\lambda(s)$ est une constante de normalisation qui permet de s'assurer que la somme des probabilités est égale à un. Les paramètres du modèle sont appris en utilisant la méthode dite de \emph{Generalized Iterative Scaling} afin d'optimiser la probabilité \emph{a posteriori} des paramètres $\lambda$ étant donné les données d'apprentissage \cite{Chen2000}.% avec une distribution Gaussienne \emph{a priori} \cite{Chen2000}.
Conjointement au N-grammes, d'autres attributs sont utilisés par le modèle d'entropie. 

Une des première connaissances ajoutée est le genre des locuteurs. Un attribut modélisant la correspondance du genre du prénom du nom complet avec le genre du tour de parole est ajouté au modèle.  Des modèles acoustiques des voix des locuteurs cibles (lorsqu'il sont disponibles) sont eux ajoutés comme attributs. Ces modèles sont appris sur les échantillons de voix des locuteurs cibles via des mélanges de Gaussiennes à 192 composantes. De plus, la position du nom complet au sein de l'enregistrement est pris en compte (avant le premier tour de parole du locuteur désigné, après le dernier tour de parole du locuteur, etc.).

\subsection{Corpus}

Le corpus d'apprentissage utilisé par Tranter contient environ 70 heures d'enregistrements de journaux d'informations en langue anglaise provenant du corpus Hub-4 de 1996/1997. Ce corpus a été transcrit manuellement et les locuteurs identifiés (quand cela était possible). De plus, les erreurs de transcription des noms complets (mauvaise orthographe du prénom ou du nom) de locuteurs intervenant dans document ont été corrigées. Le corpus de développement correspond à celui utilisé pour la campagne RT-04 \cite{NIST2004} et se compose de 12 documents enregistrés en février 2001 et de novembre à décembre 2003. Le corpus d'évaluation correspond aussi à celui de la campagne RT-04, soit 12 émissions enregistrées en décembre 2003.

À noter que les corpus utilisés par Microsoft pour évaluer le système utilisant le modèle d'entropie maximum correspondent à ceux de Tranter mais avec un découpage légèrement différent.

\subsection{Analyse des données}

Afin de tester le système mis en place, les locuteurs de la référence sont comparés aux locuteurs de l'hypothèse générée. Trois cas différents pour ces locuteurs sont possibles :

\begin{itemize}
  \item Disponible~: le nom complet du locuteur de la référence est connu et est présent à l'identique dans la transcription de l'émission~;
  \item Indisponible~: le nom complet du locuteur de la référence est connu mais n'est pas présent à l'identique dans la transcription de l'émission. Plusieurs cas de figure sont possibles~: des personnes dont les noms sont cités dans d'autres émissions, l'utilisation de synonymes demandant une connaissance du contexte (le président = Bill Clinton) ou encore l'utilisation de tournures demandant un traitement supplémentaire (John Smith et sa femme Judy = Judy Smith)~;
  \item Non-nommé~: le locuteur n'a pu être nommé dans la référence.
\end{itemize}

À la différence des travaux de Canseco-Rodiguez, ces travaux ne prennent pas en compte les noms partiels, c'est à dire les identifications à l'aide d'un seul nom ou prénom. Le nom complet associé par le système d'INL à un locuteur doit correspondre exactement au nom renseigné dans la transcription. Le tableau \ref{tab:results_tranter_perf} présente les résultats pour les différents corpus utilisés par Tranter.

\begin{table}[h]
  \begin{center}

    \small
    \begin{tabular}{|c||c|c|c||c|c|}
      \hline
      Données & D & I & N & Nombre d'occurrences \\ \hline
      Apprentissage & 79,3\% & 4,0\% & 16,8\% & 6912 \\
      Développement & 78,2\% & 8,8\% & 13,0\% & 195 \\
      Évaluation & 76,8\% & 12,9\% & 10,3\% & 206 \\ \hline
      Évaluation (srap) & 47,3\% & 42,5\% & 10,3\% & 138 \\
      \hline

      \hline
    \end{tabular}
  \end{center}
  \caption{\% de données Disponibles (D), Indisponibles (I) et Non-nommées (N) dans les corpus de Tranter. Cela fixe la limite haute des performances atteignables, puisque les noms complets I ne peuvent être récupérés à partir de la transcription. Les résultats en utilisant les transcriptions automatiques (srap) sont aussi donnés.}
  \label{tab:results_tranter_perf}
\end{table}

La quantité de noms complets disponibles à partir de la transcription reste aux alentours de 78~\%. Dans les travaux de Microsoft, ce chiffre monte à 83~\% sur le corpus de test. L'utilisation de transcriptions automatiques multiplie par trois les noms complets non disponibles dans la transcription. Cela est du aux nombreuses erreurs de transcription des noms complets. 



\subsection{Résultats}

La figure \ref{fig:resultats_tranter} donne les différents résultats en fonction du type de transcription utilisée (\emph{srap} : transcription automatique, \emph{aseg} : segmentation et classification automatiques) et de l'usage des classes sémantiques ou non. 
Ces résultats montrent que l'utilisation de classes sémantiques pour généraliser les mots présents dans les règles apporte un gain significatif, que ce soit sur des transcriptions manuelles ou des transcriptions automatiques. Sur des transcriptions réalisées manuellement et sur le corpus de développement, à 95 \% de précision le rappel est de 60 \%. Sur les données d'évaluation, le rappel chute à 38 \% pour la même précision. Le tableau \ref{table:resultats_tranter} donne le rappel maximum obtenu sur chaque type de transcription (automatiques ou manuelles), ainsi que le rappel à 95\% de précision.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Transcription & Segmentation & Rappel maximum & Rappel à 95 \% de Précision \\
      \hline
      ref & ref & 64 \% & 38 \% \\
      auto & ref & 44 \% & 38 \% \\
      auto & auto & 38 \% & 26 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Résumé des résultats sur le corpus de test. Sont présentés le meilleur rappel obtenu et le rappel à 95 \% de précision}
  \label{table:resultats_tranter}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/resultats_tranter.pdf}
\caption{Résultats sur le corpus d'évaluation avec ou sans (mots) l'utilisation de concepts. Les résultats sont donnés pour les transcriptions de référence (ref), les transcriptions utilisant un SRAP (srap) pour obtenir les mots et les transcriptions utilisant un système de segmentation automatique pour les tours de parole et les locuteurs (aseg) }
\label{fig:resultats_tranter}
\end{center}
\end{figure}

Le taux d'erreur (DER : Diarization Error Rate \cite{TranterReynolds2006}) du système de segmentation/classification utilisé est de 6,9 \%. Le taux d'erreur du système de transcription (WER : Word Error Rate \cite{mccowan-rr-04-73}) est quant à lui de 12,6 \%. L'utilisation de transcriptions automatiques n'affecte pas le rappel à 95 \% de précision mais le rappel maximum possible (en faisant baisser la précision) passe de 64 \% à 44 \%. L'utilisation de transcriptions et de segmentations automatiques fait chuter le rappel à 26 \% (pour 95 \% de précision) et fait aussi chuter le rappel maximum possible à 34 \%.

Dans \cite{Chengyuan07} Microsoft a comparé le système de Tranter avec le système utilisant un modèle d'entropie maximum sur des transcriptions manuelles. Les résultats présentés dans la figure \ref{fig:resultats_microsoft} et le tableau \ref{table:resultats_microsoft} montrent qu'en utilisant les mêmes informations, le modèle d'entropie maximum est plus performant que la simple combinaison utilisée par Tranter. En revanche, l'ajout des modèles de locuteur n'apporte aucun gain significatif. L'utilisation d'informations sur les genres et la position des noms complets, donnent les meilleurs résultats.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/resultats_microsoft.pdf}
\caption{Résultats sur le corpus de test (transcription et segmentation/classification manuelles). Le modèle d'entropie maximum (maxent) et le modèle N-gramme utilisent les mêmes règles lexicales. Les résultats avec l'ajout de la position (pos), des données acoustiques (ac) et l'utilisation du genre (g) sont aussi donnés pour le modèle d'entropie maximum.}
\label{fig:resultats_microsoft}
\end{center}
\end{figure}



\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Système & Rappel \\
      \hline
      Tranter combinaison & 38 \% \\
      Entropie & 47 \% \\
      Entropie + pos. & 58 \% \\
      Entropie + pos. + ac. & 58 \% \\
      Entropie + pos. + ac. + genre & 67 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Résultat des différents systèmes sur le corpus de test (transcription et segmentation/classification manuelles) à 95 \% de précision. \emph{pos.} : position du nom complet dans l'enregistrement, \emph{ac.} : informations acoustiques sur les locuteurs}
  \label{table:resultats_microsoft}
\end{table}
 



La méthode à base de N-grammes sur fenêtre glissante permet d'automatiser l'apprentissage des règles que Canseco-Rodriguez avait définies à la main. Une propagation relativement simple des scores est effectuée au sein des classes de locuteurs dans les travaux de Tranter~: les différents scores obtenus pour un locuteur $n_1$ et une classe de locuteur $s_a$ sont cumulés. Les tests réalisés montrent de bonnes performances sur des données manuelles, mais aussi leurs limites sur les données automatiques, alors que les systèmes automatiques de transcription et de segmentation sont très performants. Dans \cite{Chengyuan07}, Microsoft a proposé une amélioration du processus d'attribution en utilisant un modèle d'entropie maximum et des connaissances supplémentaires comme le genre des locuteurs ou la position des noms complets dans l'enregistrement. Ces différents ajouts permettent d'améliorer le rappel de 38~\% à 67~\% à 95~\% de précision.

À noter que la détection des noms complets dans la transcription est réalisée grâce à une liste de locuteurs et la détection des autres classes sémantiques est réalisée à partir des données de l'apprentissage étiquetées manuellement. Afin de se rapprocher d'une utilisation "réelle" de l'identification nommée, il est possible d'utiliser un détecteur d'entités nommées pour automatiser cet étiquetage. 

%Ces travaux sont une extension des travaux menés par Cambridge dans \cite{Tranter06}. Ils améliorent les performances obtenues par Cambridge grâce au modèle d'entropie maximum et l'utilisation d'informations supplémentaires comme la position des locuteurs dans l'enregistrement ou leur genre. En revanche, aucune expérience sur des données automatiques n'a été conduite. De plus, les entités nommées et les concepts ont été étiquetés manuellement.

\section{Approche statistique : arbre de classification sémantique}
\label{sec:sys_id_lium}

Le LIUM \cite{MauclairOdyssey06,Esteve07} a proposé une alternative au modèle N-gramme en mettant en place un système d'INL basé sur un arbre de classification sémantique. Cet arbre est utilisé pour les attributions locales \cite{Kuhn1995} ainsi qu'un détecteur automatique d'entités nommées pour identifier les noms complets dans la transcription. Ces travaux sont les premiers à utiliser un détecteur d'entités nommées pour cette tâche. L'utilisation de ces deux systèmes automatiques et d'un processus de décision permet au système du LIUM de fonctionner de manière entièrement automatique pour identifier les locuteurs d'un document.

\subsection{Détection des entités nommées}

\label{sec:en_lium}

Le système de détection des entités nommées (EN) a été mis en place lors de la campagne d'évaluation ESTER I \cite{Galliano05} sur la détection des EN. C'est un système à base de règles. Quelques règles ont été inférées à partir du corpus d'apprentissage d'ESTER I et d'autres ont été développées manuellement (pour, par exemple, réaliser une grammaire pour détecter les dates). De plus, des listes de prénoms, de patronymes, de villes, de pays, etc. ont été ajoutées à la base de connaissances du système.

La liste des EN choisie pour la campagne ESTER I se compose de 8 catégories principales qui sont : les personnes, les lieux, les organisations, les groupes socio-politiques, les montants, les informations temporelles, les produits et les aménagements. Pour son système d'INL, le LIUM n'a retenu que 5 de ces catégories : les personnes, les lieux, les organisations les groupes socio-politiques et les informations temporelles \cite{Esteve07}. Les résultats sur le corpus de développement de la campagne ESTER I sont présentés dans le tableau \ref{table:resultats_ne_lium} en terme de précision et de rappel.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      Type d'EN & Rappel (\%) & Précision (\%) \\
      \hline
      \hline
      personne & 98,7 \% & 92,6 \% \\
      \hline
      lieu & 83,9 \% & 87,9 \% \\
      \hline
      organisation & 86,1 \% & 84,5 \% \\
      \hline
      groupe socio-politique & 84,0 \% & 91,8 \% \\
      \hline
      information temporelle & 96,7 \% & 87,7 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Performances du système de détection des EN du LIUM sur le corpus de développement ESTER I}
  \label{table:resultats_ne_lium}
\end{table}

Comme pour les autres systèmes d'INL, lorsqu'une EN est détectée, la suite de mots correspondant est remplacée par le type de l'EN dans la transcription.

%\begin{figure}
%\begin{center}
%\includegraphics[width=1\columnwidth]{img/pre_traitement.pdf}
%\caption{Pré-traitement su signal audio}
%\label{fig:pre_traitement}
%\end{center}
%\end{figure}

\subsection{Attributions}

Les arbres de classification sémantiques (SCT - Semantic Classification Tree) sont des arbres de décision binaire s'appuyant sur des patrons linguistiques comme paramètres de décision pour classer les échantillons. Dans le cadre de l'identification nommée, ils sont utilisés pour réaliser l'attribution locale des étiquettes \og{}précédent\fg{}, \og{}courant\fg{} et \og{}suivant\fg{}. Ces arbres sont activés sur le contexte lexical gauche et droit de chaque nom complet détecté par le système d'EN. Au maximum cinq mots à gauche et cinq mots à droite sont gardés pour l'apprentissage et l'utilisation de l'arbre. Le SCT attribue une probabilité à chaque étiquette possible pour un nom complet détecté dans la transcription. La figure \ref{fig:sct} donne un exemple d'arbre de classification. Les détails sur l'apprentissage de l'arbre sont décrits dans \cite{MauclairOdyssey06}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/decision_detail_resume_etiquette.pdf}
\caption{Exemple d'arbre de classification sémantique}
\label{fig:sct}
\end{center}
\end{figure}

Seule l'étiquette avec la plus grande probabilité est prise en compte, les autres probabilités sont ignorées. Lorsque deux décisions du SCT attribuent le même nom complet à un locuteur, les probabilités sont additionnées. On obtient alors, pour chaque nom complet candidat pour nommer un locuteur, un score qui peut être utilisé pour prendre une décision.

Dans l'exemple de la figure \ref{fig:attrib_locale_lium}, le tour de parole correspondant au locuteur numéro 2 (abbrégé LOCU 2 dans la figure) \emph{LOCU2} se voit affecter deux fois le nom complet \emph{Maude Bayeu}. Dans ce cas, le nom complet \emph{Maude Bayeu} aura pour score 1,22. À noter que puisque les probabilités de l'arbre sont additionnées, il n'est plus possible par la suite de parler probabilités, mais uniquement de scores.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/attribution_locale_lium.pdf}
\caption{Exemple d'attributions locales du système du LIUM}
\label{fig:attrib_locale_lium}
\end{center}
\end{figure}

Lors du processus d'attribution globale, le nom complet avec le plus grand score pour une classe donnée est attribué à cette classe. Chaque classe de locuteur est traitée indépendamment de l'autre, c'est à dire que rien n'empêche le système d'attribuer le même nom de locuteur à deux classes différentes. Si la segmentation et la classification sont considérées comme correctes, ce cas de figure devrait être impossible.

\subsection{Expériences et résultats}

Le système a été évalué sur des données provenant de la campagne d'évaluation des systèmes de transcriptions en français ESTER I \cite{Galliano05}. Ces données regroupent six radios différentes et correspondent à 81 heures d'enregistrements pour le corpus d'apprentissage (150 émissions), 12,5 heures pour le corpus de développement (26 émissions) et 10 heures pour le corpus de test (18 émissions). Toutes ces données ont été étiquetées en entités nommées grâce au système décrit en section \ref{sec:en_lium}.

Ce système a été comparé à celui de Tranter dans \cite{Esteve07}. Il a été évalué sur des données segmentées et transcrites manuellement, et sur des données segmentées manuellement mais transcrites automatiquement. Ces transcriptions automatiques sont celles fournies par le LIMSI lors de la campagne ESTER I et présentent un taux d'erreur mots de 11,9~\%. Les résultats sont exprimés en terme de précision et de rappel dans les figures \ref{fig:results_yannick_manu} et \ref{fig:results_yannick_auto}. Le système de Tranter à base de N-grammes sert de comparaison.


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/results_manu_yannick.pdf}
\caption{Résultats du système du LIUM et du système à base de N-grammes sur des transcriptions entièrement manuelles (segmentation/classification/transcription)}
\label{fig:results_yannick_manu}
\end{center}
\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/results_auto_yannick.pdf}
\caption{Résultats du système du LIUM et du système à base de N-grammes sur des transcriptions automatiques et une segmentation/classification manuelle}
\label{fig:results_yannick_auto}
\end{center}
\end{figure}

Sur des transcriptions manuelles les deux systèmes font presque jeu égal~: à 95~\% de précision le rappel est légèrement inférieur à 40~\%. En revanche, sur des transcriptions automatiques, le système à base de SCT est plus performant~: les N-grammes atteignent une précision maximum de 91~\% pour un rappel de 28~\% quand le système à base de SCT obtient un rappel de presque 40~\% à 95~\% de précision.

\section{Bilan}

\label{bilaninl}

Les travaux sur l'identification nommée du locuteur utilisant la transcription enrichie du signal sont relativement récents puisque les premiers travaux menés par Canseco-Rodriguez datent de 2004. Ces travaux reposent tous sur les mêmes hypothèses de base, à savoir que les locuteurs s'annoncent dans le document lui même, que leurs noms ont bien été transcrit et que le contexte lexical permet de déterminer s'il se rapporte au locuteur qui parle actuellement, à celui qui va parler juste après, ou à celui qui a parlé juste avant.

Canseco-Rodriguez a démontré que lorsqu'un locuteur était annoncé dans le document, il était possible, via son contexte lexical, de décider s'il désignait le locuteur précédent, courant ou suivant. Les travaux de Tranter et Microsoft ont montré que la majorité des locuteurs s'annoncent et que le rappel maximum qu'il est possible d'atteindre se situe entre 78~\% et 83~\%, en fonction des corpus. Deux des principales hypothèses ont donc été vérifiées. En revanche, la troisième hypothèse majeure, à savoir que les noms complets des locuteurs ont bien été transcrits n'a été qu'assez peu étudiée.

Dans le but d'automatiser au maximum les processus d'INL, plusieurs problèmes restent à résoudre. Tout d'abord, il faut disposer d'un détecteur d'entités nommées qui va permettre de s'affranchir des dictionnaires de classes sémantiques et des listes de locuteurs pré-établies utilisées par Canseco-Rodriguez ou encore Tranter. Les travaux du LIUM ont effectué un premier pas en ce sens, mais le détecteur d'entités nommées utilisé reste assez rudimentaire.

Ensuite, le processus d'attribution globale et la gestion des différents conflits qu'il engendre nécessiteraient d'être approfondis. Seuls les travaux de Microsoft ont proposé un modèle d'entropie maximum allant dans ce sens puisqu'il permet de combiner plusieurs informations (le genre, des modèles acoustiques, la position du locuteur dans l'enregistrement) pour renforcer les décisions du modèle N-gramme. Mais les conflits (plusieurs candidats possibles pour un même locuteur, avec éventuellement beaucoup d'incertitude) apparaissant pour nommer un même locuteur n'ont pas été étudiés. À chaque fois, le locuteur ayant la plus haute probabilité (ou le plus haut score pour les travaux du LIUM) est choisi, sans prendre en compte les conflits présents.

Pour finir l'hypothèse concernant la correcte transcription des noms complets dans l'enregistrement reste encore à étudier en profondeur. Tranter et le LIUM ont réalisé des expériences sur des transcriptions automatiques, en constatant que les résultats étaient nettement inférieurs à ceux réalisés sur des transcriptions manuelles. Ces résultats mériteraient d'être approfondis afin de valider cette dernière hypothèse.

%Deux principaux systèmes statistiques ont suivis les travaux de Canseco, à savoir celui à base de N-grammes de l'université de Cambridge et celui à base d'arbre de classification sémantique (SCT) de l'université du Mans. Les travaux de \cite{Esteve07} ont démontré la supériorité du système à base de SCT par rapport au système à base de N-grammes sur des transcriptions automatiques. Ce type de système étant amené à être majoritairement utilisé sur des transcriptions automatiques, l'utilisation de SCT semble pertinente pour la suite des travaux à mener dans ce domaine. Par rapport à ces systèmes précurseurs, Microsoft a ajouté la prise en compte du genre du locuteur et la prise en compte de la position du nom complet détecté dans l'enregistrement. Ces informations améliorant les performances, il faudra les prendre en compte.

%Les travaux du LIUM ont été les seuls à utiliser un système automatique de détection des entités nommées. Dans le but d'automatiser le plus possible le processus, l'utilisation d'un tel système est indispensable. De plus, l'affectation des étiquettes \og précédent \fg{}, \og courant \fg{} et \og suivant \fg{} a beaucoup été étudiée dans la littérature. En revanche, le processus de propagation de ces étiquettes au niveau des classes de locuteur et la résolution des conflits n'a pas été étudiée jusqu'ici. Pour finir, beaucoup de travaux ont été conduits sur des transcriptions partiellement ou totalement manuelles ; un travail sur des transcriptions entièrement automatiques est à réaliser.


\chapter{MILES\_NI : Un système d'INL par analyse conjointe du signal et de sa transcription}
\minitoc
\newpage

\label{chap:milesni}

Les travaux déjà effectués sur l'INL ont permis de prouver l'efficacité de l'approche utilisant la transcription du signal audio. Comme décrit dans la section \ref{bilaninl}, des progrès restent à réaliser pour automatiser le processus, notamment dans la détection des noms complets et des classes sémantiques. De plus, les travaux ont majoritairement étudié les techniques d'attribution locale (règles linguistiques, N-grammes, arbre de classification), mais peu de travaux sur le processus d'attribution globale ont été menés.

Dans cette thèse, nous commençons par décrire le système de détection des entités nommées utilisé, puis nous décrirons ensuite l'arbre de classification sémantique mise en \oe{}uvre pour réaliser les attributions locales. Nous finirons par expliquer le processus d'attribution globale basé sur la théorie des fonctions de croyance avant de présenter les résultats du système. Ces travaux font suite à ceux déjà menés au LIUM (cf. section \ref{sec:sys_id_lium}). 

%Le laboratoire d'informatique de l'université du Maine (LIUM) a présenté ses travaux sur l'identification nommée du locuteur dans \cite{MauclairOdyssey06} et \cite{Esteve07}.
%Les travaux du LIUM \cite{MauclairOdyssey06} ont été publiés en même temps que ceux de l'Université de Cambridge. Ils font aussi suite aux premiers travaux de Canseco. Le LIUM utilise un arbre de classification sémantique (SCT - Semantic Classification Tree \cite{Kuhn1995}) pour déterminer les règles linguistiques à utiliser pour attribuer l'étiquette \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} à un nom complet détecté dans la transcription. Ces règles linguistiques sont basées sur des expressions régulières construites autour des noms complets détectés. Ce système ayant servi de base aux travaux présentés ici, il est plus longuement présenté dans la section qui lui est consacrée \label{sec:sys_id_lium}.

\section{Détection des entités nommées}

Une des étapes fondamentales d'un système d'INL est la détection des noms complets et des différentes classes sémantiques dans la transcription. Dans le but d'automatiser au maximum les systèmes d'INL, cette phase doit être réalisée automatiquement. Les systèmes de détection des entités nommées répondent parfaitement à ce besoin~: les entités nommées de type personne vont permettre de détecter les noms complets dans la transcription, les autres entités nommées vont servir de classes sémantiques pour généraliser la transcription.

\subsection{La campagne d'évaluation Ester 2}

En 2010, lors de la campagne d'évaluation Ester 2 \cite{Ester2}, une tâche d'évaluation des systèmes de détection des entités nommées a été proposée. Cette tâche a permis d'évaluer les différents systèmes de détection des entités nommées sur des transcriptions manuelles, mais aussi sur des transcription automatiques provenant de SRAP. Plusieurs laboratoires universitaires français ont participé à cette tâche comme le LINA, le LIMSI, le LIA, le LI (Tours) ou encore le LSIS. Deux entreprises privées ont aussi participé à l'évaluation~: Synapse et Xerox.

La mesure utilisée pour l'évaluation est le Slot Error Rate (SER) décrit dans \cite{SER}. La liste des entités nommées à étiqueter pour la campagne Ester 2 est la suivante~: les personnes, les lieux, les organisations, les productions humaines (moyens de transports, récompenses, oeuvres artistiques, ...), les montants, le temps et les fonctions (politiques, administratives, ...).

Sur des transcriptions réalisées manuellement, Synapse et Xerox ont (de loin) les systèmes le plus performants avec un SER inférieur à 10~\%. Le troisième système est celui du LIA (LIA\_NE) qui obtient un SER de 23,9~\%. En revanche, sur des transcriptions réalisées automatiquement, c'est LIA\_NE qui arrive en première position avec un taux d'erreur de 43,4~\% sur les transcriptions du LIMSI qui affichent un taux d'erreur mot (WER) de 12,1~\% (soit le meilleur système de la campagne d'évaluation Ester 2). D'autres tests ont été réalisés avec des transcriptions contenant plus d'erreurs (17,83~\% et 26,09\% de WER)~: LIA\_NE obtient systématiquement les meilleurs performances. Les résultats sont présentés dans le tableau \ref{tab:ne_ester2}.

\begin{table}[h]
\begin{center}
\includegraphics[width=\columnwidth]{img/ne_ester2.pdf}
\caption{Performances de chaque participant à la tâche de détection des entités nommées de la campagne Ester 2. Les résultats sont donnés en terme de Slot Error Rate (\%S), Rappel (\%R) et Précision (\%P)}
\label{tab:ne_ester2}
\end{center}
\end{table}


\subsection{LIA\_NE}

LIA\_NE \cite{bechetc-icassp10} étant le système le plus performant sur des transcriptions automatiques, c'est tout naturellement qu'il a été choisi comme détecteur d'entités nommées pour les travaux de cette thèse. LIA\_NE a la particularité d'avoir été conçu spécifiquement pour être utilisé sur des sorties de transcriptions automatiques. Ces transcriptions ont des caractéristiques qui peuvent mettre à défaut les systèmes de détection d'EN prévus pour être utilisés sur des textes écrits tels que les livres ou les journaux. 

Les transcriptions automatiques proviennent de documents audio qui contiennent de la parole plus ou moins spontanée, et qui sont soumis à des phénomènes propres à la parole orale comme les disfluences ou les faux départs. Ces transcriptions ne sont pas parfaites et contiennent des erreurs commises par le système de reconnaissance automatique de la parole. La ponctuation et les majuscules sont généralement absents des transcriptions. Même si des modules de post-traitement existent pour les remettre, ils sont eux aussi sujets à erreurs. De plus, les transcriptions automatiques ne contiennent que des mots qui appartiennent au dictionnaire du SRAP alors que les EN sont essentiellement composées de noms propres pour lesquels il est très difficile d'obtenir une liste exhaustive. 

LIA\_NE utilise tout d'abord un processus génératif grâce à un étiqueteur basé sur des HMM (Hidden Markov Model) qui permet d'annoter le document avec les parties du discours et des étiquettes sémantiques (personne, organisation, lieu, produit). Ensuite un processus discriminatif basé sur les CRF \cite{CRFNE} (Conditional Random Field) est utilisé pour effectivement récupérer les entités nommées 

Afin d'apprendre les caratéristiques de ce modèle CRF, LIA\_NE repose sur la collecte d'un maximum de connaissances (de manière non supervisée) sur les entrées du dictionnaire du SRAP. Pour ce faire, LIA\_NE utilise deux méthodes~:
\begin{itemize}
    \item Un processus d'extraction automatique des EN sur de larges corpus de textes. Ce processus utilise un étiqueteur en parties du discours associé à des données étiquetées manuellement. Ils vont permettre d'apprendre un modèle CRF pour ensuite étiqueter automatiquement un corpus de 1,3 giga mots.
    \item L'extraction automatique de données provenant de l'encyclopédie en ligne Wikipedia. Pour chaque entrée de Wikipedia, un graphe est généré représentant toutes les formes de ce mot présentes dans Wikipedia. Ce graphe sera utilisé comme caraétéristique du modèle CRF. Par exemple, pour \emph{Paris}, 39 formes ont été extraites comme : \emph{Ville Lumière, Ville de Paris, Paname, Capitale de la France, Département de Paris,} etc. Ces différents mots et graphes extraits de Wikipedia sont ensuite associés à une des catégories d'EN de la campagne ESTER (personne, lieu, organisation, produit) grâce aux données d'apprentissage étiquetées manuellement et à des lexiques préalablement constitués au LIA.
\end{itemize}



Comme décrit dans le tableau \ref{tab:ne_ester2}, cette approche ayant obtenue les meilleurs résultats lors de la dernière campagne d'évaluation Ester 2, c'est tout naturellement qu'elle a été utilisée pour les travaux de cette thèse. Les 7 catégories d'entités nommées utilisées pour Ester 2 ont été gardées comme classes sémantiques. Nous discuterons dans X de l'apport de cette méthode pour l'identification nommée du locuteur, par rapport à une approche plus classique à base de règles manuelles prévues pour être utilisées sur des corpus écrits.

\todo{COntexte epac}

\section{Attributions locales : arbre de classification sémantique}

La phase de détection des entités nommées va ensuite permettre d'attribuer chaque entité nommée de type \emph{personne} au locuteur courant ou aux locuteurs contigus du tour de parole dans lequel l'EN a été étiquetée. Cette phase d'attribution locale est effectuée grâce à un arbre de classification sémantique (SCT - Semantic Classification Tree).

Les arbres de classification sémantiques  sont souvent utilisés dans le traitement du langage naturel. Par exemple, ils sont employés pour des systèmes de dialogue \cite{Kuhn1995}, pour l'estimation de modèles de langage hiérarchiques n-grammes \cite{Esteve2001}, ou pour la détection de noms propres inconnus \cite{Bechet2000}. Les SCTs sont des arbres de décision binaire s'appuyant sur des patrons linguistiques comme paramètres de décision pour classer les échantillons. Dans le cadre de l'identification nommée, ils sont utilisés pour réaliser l'attribution locale des étiquettes \og{}précédent\fg{}, \og{}courant\fg{} et \og{}suivante\fg{} décrites dans la section \ref{ssec:attributions_locales}.

\subsection{Arbre de décision binaire}

Là où les autres travaux utilisent des règles réalisées manuellement (cf. section \ref{sub:regles_manuelles}) ou des N-grammes (\ref{sub:regles_statistiques}), les travaux de cette thèse utilisent un type particulier d'arbre binaire, à savoir un arbre de classification sémantique.

Un arbre de décision binaire \cite{Breiman84,Cornejols02} est un type de classificateur qui va permettre d'affecter des classes à un échantillon grâce à une suite de décisions binaires de type oui/non. Les n\oe{}uds de l'arbre sont des tests auquel l'échantillon sera soumis, en fonction du résultat du test (oui ou non) l'échantillon sera ensuite testé sur la branche correspondante. Si cette branche ne contient qu'une feuille, alors elle correspond aux classes à attribuer. Si cette branche est un sous-arbre contenant d'autres tests, ils sont effectués jusqu'à atteindre une feuille de l'arbre.

Prenons l'exemple d'un arbre de décision binaire permettant de classer les phrases selon le fait qu'elles parlent ou non de l'identification nommée. Les deux classes possibles sont \emph{INL} et \emph{NOTINL}. Si une phrase contient les termes \og{}identification\fg{} et \og{}nommée\fg{} elle sera alors considérée comme appartenant à la classe \emph{INL}, sinon elle sera classée comme \emph{NOTINL}. L'arbre présenté dans la figure \ref{fig:arbre_binaire} modélise ce cas de figure.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/arbre_binaire.pdf}
\caption{Exemple d'arbre de décision binaire permettant de classer une phrase X}
\label{fig:arbre_binaire}
\end{center}
\end{figure}

%le nom détecté parmi les étiquettes previous, current, next et other. Les SCTs reposent sur l'utilisation d'expressions régulières. Afin d'être analysés par le SCT, des couples sont créés comprenant une occurrence de nom complet ainsi que son contexte lexical. Le but est de classer ces couples avec les 4 étiquettes previous, current, next et other (voir les feuilles de la figure 7.2) en utilisant les expressions régulières.

\subsection{Apprentissage}

\label{sct_apprentissage}

Dans les SCTs, les n\oe{}uds de l'arbre sont des expressions régulières construites lors de la phase d'apprentissage. Pendant le processus de construction du SCT, chaque n\oe{}ud est associé à une expression régulière contenant des mots et des caractères spéciaux (\emph{<}, \emph{>} et \emph{+}). Le < (respectivement >) se rapporte au commencement (respectivement la fin) d'une intervention d'un locuteur tandis que + se rapporte à n'importe quelle suite de mots. Par exemple, l'expression régulière \emph{< + de + >} correspond à chaque intervention contenant le mot \emph{de}, alors que \emph{< + en direct + de + >} correspond à chaque intervention contenant les mots \emph{en direct} et \emph{de} apparaissant dans cet ordre. 
%La figure \ref{fig:sct} montre une petite partie d'un tel arbre de classification avec un exemple de probabilités attribuées à chaque étiquette.


Le processus de construction du SCT doit choisir pour chaque n\oe{}ud l'expression régulière qui réduit au minimum un critère d'impureté. Ici le critère utilisé est le critère de Gini (se référer à \cite{Breiman84} pour de plus amples détails). Un autre critère communément utilisé est l'entropie de Shannon \cite{Shannon}.

Pour chaque niveau dans l'arbre, ce processus de construction ajoute un mot à l'expression régulière courante. Le critère d'impureté permet d'évaluer le degré de déterminisme associé à un n\oe{}ud : plus le critère d'impureté est bas, plus la classification est déterministe. Le fonctionnement et l'apprentissage des SCTs sont décrits de manière exhaustive dans \cite{KuhnPHD}.

Afin de préparer le corpus d'apprentissage pour construire l'arbre, plusieurs traitement sont réalisés sur le corpus~:

\begin{itemize}
    \item Le corpus est traité via le détecteur d'entités nommées décrit à la section précédente. Les mots composants les entités nommées retenues sont remplacées par leur type dans la transcription.
    \item L'arbre de classification sémantique apprend les expressions régulières grâce aux mots présents dans les contextes lexicaux gauche et droite de chaque nom complet détecté. Un maximum de 10 mots autour du nom complet détecté a été gardé~: au plus 5 mots à gauche et 5 mots à droite. Le nombre de mots à garder a été déterminé grâce au corpus de développement.
    \item Les transcriptions de référence utilisées pour l'apprentissage contiennent plus d'informations que celles produites par un système de transcription automatique. Les transcriptions sont donc normalisées afin de se rapprocher au maximum de celles qu'un système de transcription automatique peut fournir. Par exemple, la ponctuation et les majuscules ont été retirées.
    \item Les articles considérés comme non porteurs de sens (\textit{l', le, la, les, un, une, uns}) ont été éliminés du contexte analysé. En revanche, les mots comme \textit{de, dans, à} ont été gardés puisqu'ils peuvent potentiellement concerner une information importante comme un lieu, une radio ou une émission.
    \item L'arbre a besoin de savoir si un nom complet détecté correspond au locuteur suivant, précédent ou courant pour pouvoir réaliser son apprentissage. Cette phase d'étiquetage de chaque nom complet est réalisée automatiquement en comparant le nom complet détecté aux noms complets des locuteurs suivant, précédent et courant. Cette phase d'étiquetage n'est pas vérifiée manuellement, il est supposé que les étiquettes sont correctes.
\end{itemize}

En complément des expressions régulières, l'arbre peut intégrer des questions globales. Le système proposé par le LIUM utilise la position du nom complet dans le tour de parole comme question globale. La position correspond aux situations où le nom complet apparaît au début (dix premiers mots, soit la taille du contexte lexical), au milieu ou à la fin (dix derniers mots) d'un tour de parole. Une autre question globale permet de savoir si le tour de parole est considéré comme très court, à savoir plus petit que le contexte lexical utilisé par l'arbre (cinq mots à gauche du nom complet, cinq mots à droite).


\subsection{Étiquetage et attributions locales}

Les arbres de classification sémantique intègrent dans chaque n\oe ud une expression régulière. La suite d'expressions régulières activées depuis la racine jusqu'à une feuille de l'arbre permet de classifier les contextes lexicaux suivant les quatre étiquettes décrites précédemment. 

Pour pourvoir être utilisé par le SCT, un document doit être préparé de la même manière que le corpus ayant servi à l'apprentissage de l'arbre. Tout corpus soumis à l'arbre de classification devra donc être pré-traité comme décrit dans la section précédente. Ensuite, pour chaque nom complet détecté, un contexte lexical n'incluant pas plus de 10 mots est conservé. C'est sur ces portions de texte contenant un nom complet que l'arbre est utilisé. Il permet, pour chaque nom complet étiqueté par le détecteur d'entités nommées, d'attribuer une probabilité pour chaque étiquette possible~: \og{}locuteur précédent\fg{}, \og{}locuteur courant\fg{} ou \og{}locuteur suivant\fg{}. La figure \ref{fig:sct} donne un exemple de probabilités obtenues pour chaque étiquette.

À noter que le SCT ne choisit pas une étiquette parmi les quatre possibles, mais attribue une probabilité à chacune des étiquettes. Dans les travaux présentés dans cette thèse, les probabilités affectées à toutes les étiquettes sont prises en compte. En effet, nous estimons que même si une étiquette a une probabilité supérieure aux autres, cela ne signifie pas nécessairement que c'est la bonne étiquette et que l'on ne doit prendre en compte que cette dernière. 

Imaginons la répartition des masses de probabilités suivante pour un nom complet : courant 0,31, précédent 0,28, suivant 0,30, autre 0.11. Décider que le nom complet désigne le locuteur courant car c'est l'étiquette qui a la plus forte probabilité occulte complètement le fait que l'arbre est très incertain sur ces étiquettes. En effet, le conflit entre les différentes étiquettes est très élevé, et l'arbre affecte quasiment autant de croyance aux étiquettes courant, précédent et suivant. L'enjeu d'un processus d'attributions globales est de pouvoir prendre en compte ces croyances et incertitudes au sein des classes de locuteurs à nommer. Dans cette thèse, la théorie des fonctions de croyances est utilisée pour résoudre ce problème.


\todo{Prise en compte du genre}

\section{Attributions globales~: processus de décision et fonctions de croyances pour l'INL}

\label{sec:fct_croyances}

La section \ref{attributions_globales} décrit toutes les difficultés auxquelles la phase d'attribution globale a à faire face. Une de ces principales difficultés est de résoudre les conflits qui apparaissent à la suite de la phase d'attributions locales (c'est à dire plusieurs candidats possibles pour une même classe de locuteur anonyme). Ces conflits sont dus à des connaissances imparfaites provenant de l'arbre de décision. 

En effet, il arrive couramment que l'arbre de décision se trompe et désigne un locuteur comme étant par exemple le locuteur suivant (en lui affectant une forte probabilité), alors qu'il ne l'est pas. La théorie des fonctions de croyance va permettre de modéliser ces connaissances et ces incertitudes de manière à faciliter la phase d'attribution globale.

\subsection{Formalisme et notations}

\label{sec:notations}

Tout d'abord, il convient d'expliciter les différentes notations qui seront utilisées par la suite.

 Soit $\mathcal{E}=\{e_1,\ldots, e_I\}$ correspondant à l'ensemble des noms complets candidats pour nommer un locuteur. L'ensemble $\mathcal{O}=\{o_1,\ldots, o_J\}$ correspond aux occurrences successives des noms complets détectés dans les transcriptions, $\mathcal{T}=\{t_1,\ldots, t_K\}$ désigne l'ensemble des tours de parole dans l'ordre chronologique, et $\mathcal{C}=\{c_1,\ldots, c_L\}$ l'ensemble des locuteurs à nommer.  Le but est donc d'attribuer un nom complet issu de $\mathcal{E}$ aux locuteurs de $\mathcal{C}$. Chaque locuteur $c_l$ peut intervenir une ou plusieurs fois dans une émission, ce qui correspond donc à un ou plusieurs tours de parole: $c_l=\{ t \in {\cal T} \;| c_l\ \mbox{est le locuteur de } t\}$. Chaque tour de parole appartient à un et un seul locuteur, c'est-à-dire que les tours de parole sont successifs et ne peuvent être simultanés. Pour chaque occurrence d'un nom complet $o_j$ (pour $j= 1,\ldots, J$) détecté dans un tour de parole $t_k$, on désigne par $P(o_j,t_k)$ la probabilité (obtenue grâce à l'arbre de classification sémantique) que $o_j$ soit le locuteur du tour de parole $t_k$. Ainsi, $P(o_j,t_{k-1})$ et $P(o_j,t_{k+1})$ représentent la probabilité que $o_j$ soit le locuteur du tour respectivement précédent et suivant celui où il a été détecté. Par hypothèse, la probabilité que $o_j$ soit un autre locuteur est donc: $$1 -\displaystyle\sum_{r \in \{-1,0,1\}} P(o_j,t_{k+r})$$

\subsection{Prise en compte du genre}
\label{sec:genre}
Les travaux présentés dans cette thèse prennent en compte le genre des locuteurs pour filtrer les probabilités $P(o_j,t_k)$ données par l'arbre de classification sémantique. En effet, pour chaque classe, cette caractéristique est disponible car, d'une part, elle est déterminée de manière automatique lors des phases de segmentation et de classification (avec un taux d'erreur inférieur à 5 \% sur des données ESTER 1 phase II), d'autre part, les genres des noms complets extraits de la transcription peuvent être déterminés à travers celui de leur prénom associé. La comparaison de ces deux informations, obtenues de deux manières différentes, nous permet d'affiner le processus de décision. En cas d'incohérence, le couple prénom et patronyme n'est pas retenu et il est supprimé de la liste des candidats potentiels. Cependant, pour le cas des prénoms ambigus comme \og Dominique \fg, l'entité nommée sera conservée. 

Pour avoir la connaissance relative aux genres des prénoms, nous utilisons une base de données extraite du Web composée d'environ 20~000 prénoms. \`A chaque prénom est associé le nombre de fois où il a été attribué au genre féminin et au genre masculin depuis 1900 en France. Cette base de données ne semble pas exempte d'erreur~: par exemple le prénom \og Vincent \fg{} apparaît 227180 fois comme prénom masculin et 373 fois comme prénom féminin. Le genre retenu correspondra au genre majoritaire. Si ce dernier a une fréquence inférieure à 75 \%, alors le genre est considéré comme indéterminé.

Au niveau du processus de décision qui suit, le filtre suivant est appliqué: 
%CJ OK
%TAL
si le genre de l'occurrence $o_j$ (et donc du nom complet $e_i$ correspondant) et celui du tour de parole (et donc de la classe $c_l$ à laquelle il appartient) sont différents, les scores $P(o_j,t_r)$ ne sont pas pris en compte lors du processus de décision.
%TAL
%si le genre de l'occurrence $o_j$ (et donc du nom complet $e_i$ correspondant) et celui du tour de parole (et donc de la classe de locuteur $c_l$ à laquelle il appartient) sont différents, les probabilités $P(o_j,t_r)$ de la formule \eqref{eq:sl} ne sont pas prises en compte lors du calcul des scores. 
%Soit $g(e_i)$ et $g(t_r)$ les genres (féminin, masculin ou indéterminé) d'un nom complet $e_i$ (ou d'un tour de parole $t_r$), alors : $(o_j= e_i, \: t_{r}(o_j) \in c_l \mbox{ et } g(e_i)\neq g(c_l)) \Rightarrow  P(o_j,t_{r})=0.$



\subsection{Processus de décision}

\label{sec:decision}

Le but du processus de décision est d'assigner un nom complet $e_i$ à un locuteur $c_l$ donné. Pour ce faire, il faut pouvoir disposer d'une information (pour chaque $e_i$ d'un locuteur $c_l$) permettant de comparer ce candidat aux autres.candidats possibles pour $c_l$. Dans cette thèse, nous utilisons un score probabiliste obtenu grâce aux fonctions de croyances. Nous décrivons dans X la manière dont ce score est obtenu. Par la suite, $\mbox{SC}_l(e_i)$ désignera le score du nom complet candidat $e_i$ pour la classe de locuteur $c_l$.

Une fois ce score obtenu, il faut ensuite attribuer à chaque locuteur $c_l$ un nom complet $e_i$.  Soit $f: \mathcal{C}\rightarrow \mathcal{E}$ la fonction d'assignation des noms complets aux  locuteurs. Les locuteurs étant censés être tous différents, une caractéristique importante que l'on voudrait imposer pour $f$ est l'injectivité: $f(c_l)=f(c_l') \Rightarrow c_l=c_l'$. Chaque nom complet ne peut être attribué à plus d'un locuteur. Bien sûr, certains noms complets peuvent rester non attribués et inversement, certains locuteurs peuvent rester sans étiquette. 

L'algorithme utilisé est le suivant: tous les noms complets possibles sont pris en compte {\it a priori} et triés en fonction de leur score $\mbox{SC}_l(e_i)$. Premièrement, le nom complet avec le score maximum (noté $e_i^*$) est choisi, et si plusieurs locuteurs sont associés au même $e_i^*$, alors ce nom complet sera assigné au locuteur dont le score $SC_l(e_i^*)$ est maximum. Ensuite, tous les noms complets choisis sont supprimés de la liste relative aux locuteurs qui n'ont pas encore été nommés. Lors de l'itération suivante, les noms complets restants sont examinés de la même manière pour les locuteurs restants et ainsi de suite, jusqu'à ce que tous les locuteurs soient nommés ou que la liste des noms complets à attribuer soit vide. Le tableau \ref{tab:examplecomb1} montre un exemple de décision. Jacques Derrida étant choisi pour $c_{15}$ lors de la première itération, il ne peut donc pas être affecté à $c_{13}$ et $c_{14}$ lors de la deuxième itération. $c_{13}$ et $c_{14}$ sont alors nommés avec d'autres candidats potentiels.

%----------------------
\begin{table}[htbp]
\caption{Exemple du processus de décision avec deux itérations
(décision en gras, scores entre parenthèses).}
\begin{center}
{\small
\begin{tabular}{|c c c |} \hline
     Locuteur & $e_i^*$  (1ère itération)&    2ème itération      \\ \hline
    $c_{13}$ &J. Derrida ($0,10$) &  \textbf{N. Demorand }($0,25$)             \\
    $c_{14}$   &J. Derrida ($0,82$) & \textbf{A. Adler} (0,56) \\
    $c_{15}$ &\textbf{J. Derrida} ($0.95$) & - \\
     $c_{16}$ &\textbf{O. Duhamel } ($1.00$) & -  \\
     % $c_{17}$ &\textbf{Marc Kravetz} (0.94) & Jacques Derrida  (0.33) \\
\hline
\end{tabular}
}
\end{center}

%\caption{}
 \label{tab:examplecomb1}

\end{table}

Toute la difficulté du processus de décision réside dans le calcul de ce score $\mbox{SC}_l(e_i)$. C'est en effet sur lui que va s'appuyer la décision d'attribution finale. Il doit permettre de refléter les probabilités de l'arbre tout en prenant en compte l'importance des conflits qui peuvent apparaître au sein d'une classe. La théorie des fonctions de croyance permettant de modéliser la croyance et l'incertiude, c'est elle qui est utilisée dans cette thèse pour obtenir les scores $\mbox{SC}_l(e_i)$.

\subsection{Fonctions de croyances}

Dans cette thèse, l'interprétation proposée par Smets est adoptée~: le modèle des croyances transférables \cite{smets94}. Le but de ce modèle est de déterminer la croyance concernant différentes propositions, à partir d'un ensemble d'informations disponibles. Soit ${\cal E}$ un ensemble fini, appelé cadre de discernement de l'expérience. La représentation de l'incertitude se fait grâce au concept de la fonction de croyance, définie comme une fonction $m$ de $2^{\cal E}$ dans $[0,1]$ telle que:
\begin{equation}
\sum_{A\subseteq {\cal E}} m(A)=1.
\end{equation} 

La quantité $m(A)$ représente la part de croyance allouée exactement à la proposition $A$. Les sous-ensembles $A $ de $\cal E$ tels que $m(A)>0$ sont les {\em éléments focaux} de $m$.
%La quantité $m(\emptyset)$, qui peut être %différente de $0$, représente la croyance allouée à une %proposition extérieure à $\cal E$.
Une structure de croyance est à support simple si elle est focalisée au maximum sur $\cal E$ et un seul sous-ensemble $A$. Elle est alors définie par:

\begin{equation}
\left\{\begin{array}{l}
m(A) = w, \quad w \in [0,1] \\
m({\cal E}) = 1-w
\end{array}
\right.
\end{equation}

L'ignorance totale est représentée par la fonction de croyance vide telle que $m(\Omega)=1$. Si tous les éléments focaux de $m$ sont des singletons, $m$ devient alors une mesure de probabilité (dite masse bayésienne).

La théorie des croyances possède plusieurs outils d'agrégation permettant de combiner des fonctions de croyance définies sur un même cadre de discernement \cite{smets94}. En particulier, la combinaison de deux structures $m_1$ et $m_2$ définies de façon ``indépendante" sur $\cal E$ utilisant l'opérateur binaire conjonctif non normalisé $\cap$ définit une fonction résultante $m'$ par la formule suivante \cite{smets94}:

\begin{equation} \label{eq:comb_Dempster}
\forall A\subseteq {\cal E}, \; m'(A)=\sum_{B \cap C =
A}m_1(B)m_2(C).
\end{equation}

On peut alors définir la combinaison de $n$ structures $m_1,\ldots, m_n$ sur $\cal E$ par : $m= m_1 \cap \ldots \cap m_n$, telle que

\begin{equation}\label{eq:combindetail}
m(A)= \sum_{A_1 \cap \ldots \cap A_n = A} \prod_{i=1}^{n} m_i(A_i)
\quad \forall A \subseteq \Omega.
\end{equation}

Si on obtient une fonction de croyance $m$ non normalisée (i. e. telle que $m(\emptyset)\neq 0$), on peut la convertir par la procédure de normalisation de Dempster \cite{shafer76} en une structure normalisée $m^*$ en répartissant proportionnellement la masse de l'ensemble vide sur les autres éléments focaux :

\begin{equation}\label{eq:norm_Dempster}
m^*(A)=\left\{\begin{array}{ll}
\displaystyle\frac{m(A)}{1-m(\emptyset)}& \mbox{si } A \neq \emptyset\\
0 & \mbox{si } A = \emptyset.
\end{array}
\right.
\end{equation}

%{\bf décision....}
Une fonction de croyance $m$ décrit l'état d'une croyance sur un phénomène. Une fois qu'une structure $m$ est définie, il est possible de la transformer en distribution de probabilité, en particulier pour des aspects décisionnels.  Une de ces distributions, appelée probabilité {\it pignistique}, consiste à répartir équitablement la masse d'un sous-ensemble de $\Omega$ entre ses éléments. Cette distribution, notée $P_{m}$, est définie pour tout $\omega \in \Omega$ par \cite{smets94}:

\begin{equation}\label{eq:pig} P_{m}(\{\omega\})= \sum_{A \subset
\Omega}\frac {m^*(A)}{|A|} \delta_{A}(\omega),
\end{equation}
 o\`u  $|A|$ est le cardinal de $A$,
 $\delta_A(\omega)=1$ si $\omega \in A$ et $\delta_A(\omega)=0$ si $\omega \notin A$ .

\subsection{Définition des masses de croyance}

Comme décrit dans \ref{attributions_globales}, plusieurs occurrences correspondant à des noms complets différents peuvent être détectées. Plusieurs noms complets sont donc en compétition ou en conflit en tant que locuteur courant potentiel (ou précédent, ou suivant).

On s'intéresse à un tour $t_k$ appartenant au locuteur $c_l$ et ayant $n_k$ occurrences. Soient $n_{k+r}$, le nombre d'occurrences tours précédent ($r=-1$) et suivant ($r=1$) de ce tour. Soient $\{o_{j,r}^k\},$ avec $ r=-1,0,1$ et $j=1,\ldots, n_{k+r}$, les occurrences de noms complets détectés dans ces trois tours. Chaque occurrence $o_{j,r}^k$, correspondant à une étiquette $e_i$, représente une certaine information sur le locuteur du tour $t_k$ qui peut être décrite par une masse de croyance $m^{jr}_{t_k}$ sur $\cal E$ à support simple, focalisée sur $e_i$ et $\cal E$:

%\begin{equation}\label{eq:putaindequationcruciale}
    %\left\{\begin{array}{c}
        %m^{jr}_{t_k}(\{e_i\})= \alpha_{il} P(o_{j,r}^k,t_{k-r}) \mbox{  si  } o_{j,r}^k=e_i \\
        %m^{jr}_{t_k}(\mathcal{E})= 1-
        %\alpha_{ij}P(o_{j,r}^k,t_{k-r}),
    %\end{array}\right.
%\end{equation}

%où $\alpha_{il} \in [0,1]$ est une mesure de compatibilité de genre entre $e_i$ et $c_l$. Si les genres sont connus avec certitude, $\alpha_{il}=0$ si $g(e_i)\neq g(c_l)$ et $\alpha_{il}=1$ si $g(e_i)= g(c_l)$. Si les prénoms sont ambigus (comme Dominique) ou non spécifiés, ou si le genre du locuteur est incertain, $\alpha_{il}\in ]0,1[$ est estimé à partir d'une base de prénoms et du corpus d'apprentissage. 


\begin{equation}\label{eq:putaindequationcruciale}
    \left\{\begin{array}{c}
        m^{jr}_{t_k}(\{e_i\})= P(o_{j,r}^k,t_{k-r}) \mbox{  si  } o_{j,r}^k=e_i \\
        m^{jr}_{t_k}(\mathcal{E})= 1-
        P(o_{j,r}^k,t_{k-r}),
    \end{array}\right.
\end{equation}


%Le tableau \ref{tab:conflitdempster}
%présente la masse de croyance concernant le locuteur du tour
%$t_{k+1}$ dans l'exemple vu en section \ref{sec:critique}. La masse
%de croyance du nom complet Jean-Claude Pajak se maintient tandis que
%celle des autres candidats est considérablement réduite.
%-------------
%\begin{table}[htbp]
%\caption{Calcul de score dans un tour de parole: méthode proposée.}
%\begin{center}
%{\small
%\begin{tabular}{|c| c|} \hline
%      Eléments focaux & $m_{t_{k+1}}^*(\{e_i\})$ \\\hline
%      \hline
%      Oscar Temaru  ($e_1$)& $0,011$ \\
%  Hamid Karzaï ($e_2$)& $0,011$\\
% Jacques Chirac ($e_3$)& $0,047$\\
%    \textbf{Jean-Claude Pajak}&\textbf{0,904}\\
%    \hline
%    ${\cal E}$ & $0,027$\\
%     \hline
%\end{tabular}
%}
%\end{center}
%\label{tab:conflitdempster}
%\end{table}
%----------------

\subsection{Combinaison par tour de parole et par locuteur}

La première étape de la combinaison consiste à agréger les informations au sein d'un tour de parole donné. La combinaison des $n_{k-1}+n_k+n_{k+1}$ masses ciblées sur le tour $t_k$ et obtenues par l'équation \ref{eq:putaindequationcruciale} peut se faire par la règle conjonctive de Dempster {\it non normalisée} (cf. équations \ref{eq:comb_Dempster}- \ref{eq:combindetail}), afin d'assurer l'associativité et la commutativité de la combinaison: on obtient une masse de croyance $m_{t_k}$ sur l'identité du locuteur du tour $t_k$, définie par 

\begin{equation} \label{eq:massetour}
m_{t_k} =\displaystyle \bigcap_{r=-1}^{1}\bigcap_{j=1}^{n_{k+r}}
m^{jr}_{t_k}
\end{equation}

et la masse de croyance normalisée $m^*_{t_k}$ correspondante (cf. équation \ref{eq:norm_Dempster}).

La deuxième étape de la combinaison consiste à agréger les résultats obtenus par tour de parole pour l'ensemble de l'émission. Plusieurs possibilités sont envisageables {\it a priori} pour combiner ces informations. Il est possible de procéder de façon  analogue à celle de l'équation \ref{eq:sl}. Pour l'assignation d'un nom complet $e_i$ à un locuteur $c_l$ donné, nous pourrions calculer un ``score" pour chaque $e_i$, dénoté $sm_l(e_i)$, comme la somme des masses de croyance concernant les tours de parole du locuteur $c_{l}$ :

\begin{equation}\label{eq:sml}
sm_{l}(e_i)= \sum_{t_k \in c_l } m^*_{t_k}(\{e_i\}).
\end{equation}

Les scores obtenus ne sont donc pas nécessairement normalisés (i.e.  $\leq 1$), ce qui rendra là encore difficile l'interprétation des résultats lors du processus de décision. Nous préférons rester dans le cadre de la théorie des croyances: une manière assez naturelle consiste en effet à continuer de combiner toutes les masses de croyance relatives au même locuteur $c_l$ par la même règle conjonctive de Dempster et donc à fusionner toutes les masses de croyance relatives aux différents tours de parole $t_k$ de ce locuteur. On obtient alors une masse de croyance globale $M_l$ sur l'identité du locuteur $c_l$ de l'émission, définie par

\begin{equation} \label{eq:masseglobale}
M_{l} =\displaystyle \bigcap_{t_k \in c_l} m_{t_k}
\end{equation}

et la masse de croyance normalisée $M^*_{l}$ correspondante.


\subsection{Règle de décision}

La méthode exposée en section \ref{sec:decision} est utilisée comme base pour la décision d'affectation. Les masses de croyance $M_l$ sont transformées en une probabilité pignistique $P_{M_l}$ (cf. équation \ref{eq:pig}) et la règle $\mathbf{R}$ suivante est utilisée :

    \begin{equation}\label{eq:maxcroyance}
        \begin{array}{c}
            \forall c_l \in \mathcal{C}\setminus \mathcal{D},  \displaystyle e_i^*=\arg \max_{e_i \in \mathcal{E}} \mbox P_{M_l}(e_i) \Rightarrow f(c_l)=e_i^*\\
            \forall c_l \in \mathcal{D}, f(c_l)= \mbox{Anonyme} \\
        \end{array}
    \end{equation}

Ensuite, comme certains noms complets sont attribués lors de cette première étape à plusieurs locuteurs différents, il faut là encore réorganiser le partage des noms complets entre les locuteurs anonymes. Le processus de décision proposé en \ref{sec:decision} est appliqué, en remplaçant les scores $\mbox{SC}_l$ par les probabilités pignistiques $P_{M_l}$. 

En reprenant l'exemple proposé en \ref{sec:decision}, le nom complet ``Jacques Derrida" est là encore assigné aux départ aux trois locuteurs $c_{13}$, $c_{14}$ et $c_{15}$ (cf. tableau \ref{tab:examplecombdempster}). Mais finalement,``Jacques Derrida" est également assigné à $c_{15}$, car c'est pour ce locuteur qu'il obtient la plus grande probabilité pignistique.
%----------------------
%\begin{table}[htbp]
%\caption{Décision avec deux itérations (décision en gras,
%probabilités pignistiques $P_{M_l}(e_i^*)$ entre parenthèses).}
%\begin{center}
%{\small
%\begin{tabular}{|c c c |} \hline
%     Locuteur & $e_i^*$  (1ère itération)&    2ème itération      \\ \hline
%    $c_{13}$ &J. Derrida ($0,89$) &  \textbf{N. Demorand }($0,11$)             \\
%    $c_{14}$   &J. Derrida ($0,71$) & \textbf{A. Adler} $(0,25)$ \\
%    $c_{15}$ &\textbf{J. Derrida} ($0,99$) & - \\
%     $c_{16}$ &\textbf{O. Duhamel } ($0,88$) & -  \\
%     % $c_{17}$ &\textbf{Marc Kravetz} (0.94) & Jacques Derrida  (0.33) \\
%\hline
%\end{tabular}
%}
%\end{center}
%
%%\caption{}
% \label{tab:examplecombdempster}
%
%\end{table}


\section{Évaluation du système proposé}
\subsection{Description des corpus}

\label{corpus}

L'évaluation du système proposé est réalisée à partir d'émissions radiophoniques en français de la campagne ESTER 1 phase II \cite{Galliano06,Gravier04}. La majorité de ces émissions contient essentiellement de la parole lue ou préparée, et peu de parole spontanée~: 15 \% du corpus correspond à des interventions de personnes parlant au téléphone.

Les émissions proviennent de cinq radios françaises et de Radio Télévision Marocaine et durent de 10 à 60 min. Elles sont réparties en trois corpus utilisés pour l'apprentissage de l'arbre de classification, le développement et l'évaluation du système. 
Le corpus de développement a été utilisé pour fixer les différents paramètres du système comme la taille du contexte lexical de l'arbre ou le poids donné aux échantillons lors de l'apprentissage (\textit{cf.} \ref{sct_apprentissage}). 

Le corpus d'apprentissage contient 76 heures de données (75~095 segments et 7~416 tours de parole) dans lesquels 11~292 noms complets sont détectés. 755 locuteurs différents interviennent dans ce corpus dont 40 qui n'ont pu être nommés. Le corpus de développement contient 30 heures (27~149 segments et 2~931 tours de parole) dans lesquels 4~533 noms complets ont été détectés. 359 locuteurs différents interviennent dans ce corpus dont 38 qui n'ont pu être nommés. Le corpus d'évaluation contient 10 heures (10~335 segments et 1~082 tours de parole) dans lesquels 1~541~noms complets ont été détectés. 213~locuteurs différents interviennent dans ce corpus dont 24 qui n'ont pu être nommés. 26,5 \% de ces locuteurs sont communs au corpus d'apprentissage seul et 28,4 \% sont communs aux corpus d'apprentissage et de développement. Ce découpage correspond au découpage de la campagne d'évaluation officielle ESTER 1 PHASE II 2005.

Les transcriptions fournies avec les corpus ont été créées pour l'évaluation des tâches de segmentation et de classification en locuteurs, ainsi que pour la tâche de transcription. Les références proposées sont d'une grande qualité, les annotateurs ont essayé de nommer le maximum de locuteurs par des identifiants permettant d'en déduire leurs noms complets. Ces noms complets ont été extraits automatiquement et n'ont pas fait l'objet de validation manuelle approfondie. 

Le tableau~\ref{corp_stat}  montre la répartition \textit{a priori} des quatre étiquettes calculée pour le corpus de test. L'étiquette \textit{\og autre \fg } est la plus fréquente et représente 79,5 \% des cas ; vient ensuite l'étiquette \textit{\og tour suivant \fg } avec 16,5 \%, tandis que les deux dernières étiquettes \textit{\og tour précédent \fg } et \textit{\og tour courant \fg } sont les moins fréquentes~: environ 2~\% chacune. 

\begin{table}
\begin{center}
{\begin{tabular}{|l|c|}
\cline{2-2}
  \multicolumn{1}{c|}{} &  \textbf{\textit{évaluation}}\\ 

 \hline
\textit{Tour précédent} & 2,0 \% (31) \\
\textit{Tour courant}   & 2,0 \% (30) \\
\textit{Tour suivant}   & 16,5 \% (255)\\
\textit{Autre}          & 79,5 \% (1~225) \\
\hline
 \hline
\textit{Total}          & 100 \% (1~541) \\
\hline
\end{tabular}}
\end{center}
\caption{\label{corp_stat}Répartition des étiquettes sur le corpus d'évaluation, statistiques sur les noms complets (fréquence et effectif).}
\end{table}




%\section{Arbres de classification sémantique (SCT) et décision locales}
%\label{sec:sct}
%
%Comme l'Université de Cambridge, le LIUM a généralisé l'approche de Canseco en automatisant l'apprentissage des règles linguistiques (grâce à l'arbre de classification sémantique) et en automatisant le processus de décision consistant à attribuer un nom complet à une classe de locuteur anonyme.
%L'arbre de classification utilisé est un arbre de décision binaire construit à partir du corpus d'apprentissage. Chacun des noeuds de l'arbre représente une expression régulière. Lorsque le contexte lexical (vingt mots à gauche et vingt à droite) d'un nom complet correspond à une des expressions régulières de l'arbre, une probabilité que ce soit le locuteur \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} lui est attribué, en fonctions des données apprises sur le corpus d'apprentissage. La décision \og locale \fg{} ( \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} ) avec le plus grand score est reportée au sein de la classe de locuteur anonyme correspondant. Lorsque le même nom complet est attribué plusieurs fois à une classe de locuteur, les probabilités de l'arbre de classification sont cumulées pour obtenir son score. Finalement, pour chaque classe de locuteur, le nom complet avec le plus grand score est attribué. À noter que l'arbre de classification permet de prendre en compte des informations plus \og globales \fg{} que le contexte lexical seul. Le LIUM utilise ainsi la position du nom complet dans le tour de parole (au début de celui-ci, à la fin celui-ci ou dans un tour de parole très court) comme information supplémentaire lors de l'apprentissage et du test de l'arbre.
%
%Ils ont réimplémenté le système à base de N-grammes de \cite{Tranter06} et l'ont comparé à l'approche basée sur les SCT. Les évaluations ont été conduites sur le corpus d'évaluation de la campagne ESTER 1 avec une liste fermée de locuteurs provenant des corpus d'apprentissage, de développement et de test. Cette liste contient 1007 locuteurs. Les résultats ont montrés que le système à base de N-gramme et celui à base de SCT se comportait de manière similaire sur des transcriptions réalisées entièrement manuellement. En revanche, sur des transcriptions automatiques (en gardant la segmentation et la classification de référence), le système à base de SCT s'est avéré beaucoup plus robuste.

%\section{Système de décision global}
%
%Ces travaux sont les premiers à utiliser un système automatique de détection des entités nommées. Ils automatisent complètement le processus d'identification nommée, de l'apprentissage au test : toutes les étapes sont automatiques.
%En revanche, le système de décision est relativement sommaire : seule l'étiquette avec le score maximal est pris en compte, les probabilités des autres étiquettes pour un nom complet sont tout simplement ignorées. De plus la prise de décision au niveau de la classe de locuteur se réduit à attribuer le nom de locuteur ayant un score maximum : l'incertitude au sein d'une même classe (plusieurs locuteurs possibles avec des scores élevés) n'est pas prise en compte.
%
\subsection{Métriques utilisées}

\label{sec:metrique}

Un système d'identification nommée est évalué en comparant l'hypothèse générée par celui-ci à la référence distribuée avec le corpus. Cette comparaison met en évidence 5 cas d'erreur ou de succès possibles relatifs aux situations suivantes~:
\begin{itemize}
\item l'identité proposée est correcte ($C_1$)~: le système propose une identité correspondant à celle indiquée dans la référence ;
\item erreur de substitution ($S$)~: le système propose une identité différente de l'identité présente dans la référence ;
\item erreur de suppression ($D$) : le système ne propose pas d'identité alors que le locuteur est identifié dans la référence ;
\item erreur d'insertion ($I$)~: le système propose une identité alors que le locuteur n'est pas identifié dans la référence ;
\item il n'y a pas d'identité ($C_2$)~: le système ne propose pas d'identité et la référence ne contient pas d'identité.
\end{itemize}


Une mesure de Précision et de Rappel peut être définie à partir des 5 cas d'erreur~:
\begin{equation}
	\label{eq:PR}
	P = \frac{C_1}{C_1+S+I} \ \ ; \ \ R = \frac{C_1}{C_1+S+D}
\end{equation}

%Dans tous les précédents articles \cite{Tranter06,Esteve07,Chengyuan07} traitant de l'identification nommée du locuteur, les résultats sont présentés sous la forme de mesures de précision et de rappel.

Comme il a été proposé dans \cite{Tranter06}, ces valeurs peuvent être complétées par un taux d'erreur $Err$ global également calculé à partir de ces 5 erreurs. Ce taux s'inspire du calcul du WER utilisé pour l'évaluation de la transcription. Il a l'avantage de mesurer la qualité des résultats du système d'identification nommée en une seule valeur, facilitant les comparaisons entre les systèmes par rapport aux mesures de précision et de rappel.
\begin{equation}
	\label{eq:PR}
	Err = \frac{S+I+D}{S+I+D+C_2+C_1} \ \ ;
\end{equation}


Les erreurs peuvent être calculées en terme de durée ou en terme de nombre de locuteurs.
Pour une évaluation en durée, dans le cas où un locuteur parlant 90\% du temps est correctement nommé et que les six autres locuteurs parlant seulement 10\% du temps ne le sont pas, le système présentera un taux d'erreur de 10\%. 

Pour une évaluation en terme de nombre de locuteurs, dans le même cas de figure, le système aura un taux d'erreur de 87,5\%.

D'un point de vue applicatif, la métrique exprimée en durée est préférable si les locuteurs considérés comme importants correspondent aux locuteurs s'exprimant beaucoup. En revanche, si l'application cherche à nommer le plus possible de locuteurs, il est plus intéressant d'évaluer les performances en terme de nombre de locuteurs.

\subsection{Protocole d'évaluation}




\subsection{Résultats avec liste de locuteurs globale et prise en compte du genre via le prénom}

Le tableau \ref{tab:results_belief} présente les résultats du système sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs du corpus d'apprentissage ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste de prénoms décrite dans la section \ref{sec:genre}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 82,96\% & 92,64\% & 15,76\% & 15,64\% \\ \hline
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 77,20\% & 93,28\% & 22,80\% & 29,03\% \\ \hline
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle en utilisant une liste de locuteurs}
\label{tab:results_belief}
\end{table}




\section{Bilan}
      Les travaux ont beaucoup porté sur la manière d'étiqueter les suivant/précédent/courant/autre => Un papier montre que les SCT > N-grammes. Aller plus loin : améliorer le processus de décision qui vient après ces décisions locales
      Utiliser une analyse conjointe du signal sonore et du texte pour améliorer les décisions "aveugles" (renforcer les décisions avec le genre des locuteurs)

Les noms complets des locuteurs sont présents dans la transcription, et ils est possible de les affecter à tour de parole contigu


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% DERNIER CHAPITRE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{MILES\_NI : avancées et limites}

\minitoc
\newpage

\section{Détection des entités nommées~: une étape fondamentale}

\subsection{Nemesis, un outil prévu pour le TAL}

\subsection{Némésis et LIA\_NE~:  expériences et résultats}

\section{Système de décision~: modélisation}

Comme mentionné dans la section \ref{chap:liumni}, les travaux de cette thèse font suite à ceux précédemment menés au LIUM \cite{MauclairOdyssey06,Esteve07}. Nous commençons tout d'abord par rappeler et formaliser le processus de décision utilisé dans les précédents travaux, pour ensuite présenter la solution intermédiaire développée pendant cette thèse. Nous finissons par exposer le cheminement qui nous a conduit à utiliser la théorie des fonctions de croyance.

\subsection{Utilisation d'un maximum}

\label{sec:maximum}

Le processus de décision utilisé dans les travaux du LIUM est assez rudimentaire.

En effet, lors de la propagation des attributions locales aux tours de paroles puis aux classes de locuteur, une simple somme des probabilités $P(o_j,t_{r})$ (cf. \ref{sec:notations} pour la signification des notations) était effectuée. Ce score concernant un locuteur $e_i$ pour une classe donnée $c_l$ s'exprime comme ceci~:

\begin{equation}s_{l}(e_i)= \sum_{\left\{(o_j, t_r)  | o_j= e_i, \: t_r \in c_l \right\}} P(o_j,t_{r})
\label{eq:sl}
\end{equation}

Ensuite, le nom complet $e_i$ ayant le score maximum pour une classe de locuteur donnée $c_l$ lui était attribué. 
Rappelons que puisque tous les locuteurs sont censés être différents, la fonction d'assignation des noms complets aux locuteurs $f(c_l)=f(c_l') \Rightarrow c_l=c_l'$ doit être injective (cf. \ref{sec:decision}).

Soit ${\cal D}= \{c_l \in \mathcal{C} | \; \forall e_i\in \mathcal{E}, s_l(e_i)=0\}$, l'ensemble des locuteurs n'ayant pas de candidats potentiels. La règle $\mathbf{R_1}$ permettant d'affecter le nom complet $e_i$ ayant le score maximal $s_l(e_i)$ est définie comme ceci~:
    \begin{equation}\label{eq:maxscore}
        \begin{array}{c}
            \forall c_l \in \mathcal{C}\setminus \mathcal{D},  \displaystyle e_i^*=\arg \max_{e_i \in \mathcal{E}} \mbox s_l(e_i) \Rightarrow f(c_l)=e_i^*\\
            \forall c_l \in \mathcal{D}, f(c_l)= \mbox{Anonyme} \\
        \end{array}
    \end{equation}

Cette méthode de combinaison a plusieurs inconvénients. Tout d'abord, le fait de réaliser la somme des probabilités issues de l'arbre ne permet plus de travailler dans l'espace probabiliste. En effet, une fois les sommes effectuées, seuls des scores difficilement comparables entre eux sont disponibles.


Ensuite, la règle $\mathbf{R_1}$ ne prend pas en compte la contribution du score $s_l(e_i)$ au sein d'une classe de locuteur. Prenons l'exemple décrit dans le tableau \ref{tab:examplecomb}. En utilisant la règle $\mathbf{R_1}$, \emph{Jacques Derrida} est attribué à la classe de locuteur $c_{13}$ puisque c'est pour cette classe qu'il obtient le score maximum. $\beta_{il}$ désigne le pourcentage du score que représente $s_l(e_i^*)$ au niveau de la classe de locuteur $c_l$. Même si \emph{Jacques Derrida} a le score le plus élevé pour $c_{13}$, il ne représente que $35 \%$ des scores totaux de cette classe. En revanche, au sein de la classe $c_{15}$, \emph{Jacques Derrida} représente $80 \%$ des scores. Avant d'utiliser les fonctions de croyance, nous avons tout d'abord proposé une méthode permettant de normaliser les scores pour prendre en compte ces différentes aspects.

\begin{table}[htbp]
\caption{Exemple d'une assignation initiale multiple}
\begin{center}
{\small
\begin{tabular}{|c c c c|} \hline
    Locuteur & nom complet $e_i^*$ & $s_l(e_i^*)$     &  $\beta_{il}$\\ \hline
    $c_{13}$ &Jacques Derrida  &    $\mathbf{8,58}$ &  $35 \%$\\
    $c_{14}$   &Jacques Derrida &  $1,67$ &  $56 \%$ \\
    $c_{15}$ &Jacques Derrida  & $4,94$ & $\mathbf{80 \%}$ \\
\hline
\end{tabular}
}
\end{center}

\label{tab:examplecomb}

\end{table}


\subsection{Normalisation des scores}

\label{sec:normalisation}

Comme décrit dans la section précédente, la contribution du score de chaque nom complet au sein de la classe de locuteur (modélisé par $\beta_{il}$) peut être une information intéressante à prendre en compte lors du processus de décision. Mais l'utilisation de $\beta_{il}$ comme unique critère de décision ne peut être une option. En effet, dans le cas où un mauvais nom complet serait le seul candidat pour une classe donnée, $\beta_{il}$ vaudrait 100 \%. Ce nom complet serait alors systématiquement choisi pour la classe de locuteur concernée, même si $s_l(e_i)$ est très faible pour cette classe. Il faut donc prendre en compte conjointement $s_l(e_i)$ et $\beta_{il}$. Pour ce faire, nous proposons l'utilisation d'une nouvelle règle $\mathbf{R_2}$~:

\begin{equation} \label{eq:SC}
   \mbox {SC}_l(e_i)=s_{l}(e_i) \beta_{il}
\end{equation}

Cette règle permet de normaliser le score de chaque locuteur en fonction du score global de la classe. Lorsqu'un nom complet est le seul candidat pour une classe, son score reste inchangé. Lorsqu'il y a plusieurs candidats pour une classe, la normalisation permet de prendre en compte la contribution du score par rapport à l'ensemble des scores de la classe. Les scores en forte concurrence sont alors pénalisés. Cette normalisation est particulièrement efficace lorsque plusieurs noms complets potentiels ont des scores élevés.

Reprenons l'exemple décrit dans le tableau \ref{tab:examplecombbeta} en utilisant cette fois ci ${SC}_l(e_i)$ pour prendre la décision. Le tableau \ref{tab:examplecombbeta} décrit ce cas de figure.

\begin{table}[htbp]
\caption{Exemple d'une assignation initiale multiple prenant en compte le conflit}
\begin{center}
{\small
\begin{tabular}{|c c c c c|} \hline
    Locuteur & nom complet $e_i^*$ &$s_l(e_i^*)$     &  $\beta_{il}$    & $\mbox{SC}_l(e_i^*)$       \\ \hline
    $c_{13}$ &Jacques Derrida  &    $\mathbf{8,58}$ &  $35\%$       &  $3,00$     \\
    $c_{14}$   &Jacques Derrida &  $1,67$ &  $56\%$ &  $0,94$\\
    $c_{15}$ &Jacques Derrida  & $4,94$ & $\mathbf{80\%}$ &  $\mathbf{3,95}$\\
\hline
\end{tabular}
}
\end{center}

\label{tab:examplecombbeta}

\end{table}

Dans le précédent exemple, \emph{Jacques Derrida} était affecté à la classe de locuteur $c_{13}$. Avec l'utilisation de ${SC}_l(e_i)$ pour prendre la décision, \emph{Jacques Derrida} est maintenant attribué à la classe $c_{15}$~: celle ou il obtient un score $s_l(e_i^*)$ élevé et pour laquelle il y a peu de conflit ($\beta_{il}$ est égal à $80 \%$).

\subsection{Expériences et résultats}

\label{sec:exp_normalisation}

\subsection{Critiques et théorie des fonctions de croyance}



Plusieurs critiques de natures diverses peuvent être émises quant à la pertinence de la méthode de combinaison exposée précédemment en section \ref{sec:maximum}, même si elle a donné de bons résultats (cf. section \ref{sec:exp_normalisation}). Premièrement, la notion de score est difficile à interpréter, ces quantités obtenues ne représentent pas de degré de confiance, ni de probabilité que tel nom complet soit tel locuteur. Elles conduisent à un manque de lisibilité de la décision. L'équation \ref{eq:SC} sur laquelle s'appuie la décision représente un compromis qu'il est difficile de justifier.

Mais la critique principale concerne la méthode même de combinaison : le conflit d'informations au sein d'un tour de parole donné n'est pas pris en compte. L'information disponible n'est pas combinée dans sa globalité de façon satisfaisante. La méthode de combinaison proposée est susceptible de propager des imperfections pour l'étape suivante de la décision. En effet, l'affectation d'un nom complet à un locuteur ne tient pas compte des liens entre les différentes informations fournies par l'arbre de classification, en particulier quand un même locuteur prononce plusieurs noms complets et peut donc aboutir à des résultats erronés. Le tableau \ref{tab:conflit} présente un exemple de tour de parole $t_k$ où $8$ noms complets sont détectés. Les probabilités indiquées correspondent au locuteur du tour suivant $t_{k+1}$, qui est de genre masculin. L'un des noms complets (féminin) est donc éliminé. Certaines occurrences sont redondantes, car elles correspondent à un même nom complet (\emph{Jean-Claude Pajak} et \emph{Jacques Chirac}) et une seule occurrence a une probabilité élevée. Il reste donc $4$ noms complets en compétition, ce qui représente une incompatibilité importante. Or, la contribution de ce tour produit des scores élevés pour \emph{Jean-Claude Pajak} ($1,25$) et \emph{Jacques Chirac} ($0,87$), scores semblables à celui qu'on obtiendrait si on disposait d'une information sans ambigüité, comme par exemple un tour ne contenant qu'une seule occurrence de probabilité élevée sans concurrence. En effet, même si \emph{Jacques Chirac} obtient un score relativement élevé ($0,87$) avec la méthode de combinaison proposée, le fait que ce score provienne de 3 scores relativement faibles de l'arbre ($0,29$) est complètement occulté.

 Cet exemple met en évidence le fait que cette méthode ne prend pas en compte la contradiction des informations délivrées par certains tours de parole. Même si une solution intermédiaire a été présentée dans \ref{sec:normalisation}, il convient de réajuster le calcul des scores en tenant mieux compte de l'incertitude de ces informations. Un formalisme probabiliste basé par exemple sur des probabilités conditionnelles peut être envisagé pour ce type de situations, mais le manque d'informations {\it a priori} rend ce type de modélisation difficile. Bien que les sorties de l'arbre soient de nature probabiliste, la théorie des croyances nous a paru mieux adaptée et moins contraignante, grâce en particulier à la souplesse de son utilisation. C'est pourquoi c'est cette théorie qui a finalement été retenue dans les travaux présentés dans \ref{sec:fct_croyances}.

%---------------
\begin{table}[htbp]
\caption{Contribution du score dans un tour de parole (le genre du
locuteur $t_{k+1}$ est masculin).}
\begin{center}
{\small
\begin{tabular}{|c| c |c| c|} \hline
      Occurrence $o_j$ & sexe & $P(o_j,t_{k+1})$ & score\\\hline
      \hline
      Oscar Temaru  & M &$0,29$  & $0,29$\\\hline
  Hamid Karzaï & M &$0,29$  &$0,29$\\ \hline
 Jacques Chirac & M & $0,29$ & $0,87$\\
   Jacques Chirac & M &$0,29$ &\\
    Jacques Chirac  & M &$0,29$ &\\\hline
    Jean-Claude Pajak & M &$0,29$ &\\
    Jean-Claude Pajak& M &\textbf{0,96} &$1,25$\\
    \hline
    {\it Véronique Rebeyrotte}& \textit{F }&\textit{0,29} & $-$\\
     \hline
\end{tabular}
}
\end{center}
\label{tab:conflit}
\end{table}
%-----------------------

\section{Applications et connaissances a priori}
liste de locuteurs, genre des locuteurs

\subsection{Résultats avec liste de locuteurs globale}

Le tableau \ref{tab:results_liumni} présente les résultats du système sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles ou totalement automatiques. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs d'ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste des locuteurs (le genre des locuteurs étant extrait des transcriptions de référence).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 87,36\% & 95,69\% & 11,67\% & 14,11\% \\ \hline
\textbf{A} & \textbf{A} & 29,31\% & 59,98\% & 64,49\% & -\% \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 85,99\% & 95,72\% & 12,88\% & 18,26\% \\ \hline
\textbf{A} & \textbf{A} & 29,48\% & 69,42\% & 64,46\% & -\% \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs}
\label{tab:results_liumni}
\end{table}

\subsection{Résultats avec liste de locuteurs par show}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 88,67\% & 97,31\% & 10,45\% & 12,45\% \\ \hline
\textbf{A} & \textbf{A} & 28,80\% & 69,65\% & 64,91\% & -\% \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 86,85\% & 96,56\% & 12,10\% & 17,01\% \\ \hline
\textbf{A} & \textbf{A} & 28,83\% & 73,77\% & 64,84\% & -\% \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs du show}
\label{tab:results_liumni_liste_show}
\end{table}


\subsection{Impact de la prise en compte du genre}

Le tableau \ref{tab:results_liumni_sansgenre} présente les mêmes résultats que le tableau \ref{tab:results_liumni} sans utiliser l'information sur le genre des locuteurs. La liste des locuteurs d'ESTER 1 (tous corpus confondus) est utilisée, mais sans prendre en compte le genre du locuteur.

Lorsque le genre n'est pas utilisé pour affiner les décisions, les performances du système se dégradent, que ce soit sur les transcriptions automatiques ou manuelles. Sur les transcriptions manuelles, le taux d'erreur augmente d'environ 3\% en absolu. Sur les transcriptions automatiques, le taux d'erreur augmente très peu sur les corpus de développement, mais augmente de 4\% sur le corpus de test. Le genre des locuteurs étant détecté automatiquement, il est donc soumis à des erreurs \todo{donner le taux d'erreur}. Mais même si cette détection comporte des erreurs, l'utiliser améliore quand même les performances.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 84,30\% & 90,60\% & 14,55\% & 17,84\% \\ \hline
\textbf{A} & \textbf{A} & 28,56\% & 54,08\% & 65,65\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 84,12\% & 93.40\% & 14,68\% & 20,33\% \\ \hline
\textbf{A} & \textbf{A} & 24,61\% & 58,82\% & 68,78\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs, sans prise en compte du genre}
\label{tab:results_liumni_sansgenre}
\end{table}


\subsection{Système ouvert sans liste de locuteurs et de leur genre}

\ref{tab:results_liumni_sansliste} \ref{tab:results_liumni_genre_prenom}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 67,23\% & 93,12\% & 30,04\% & 28,63\% \\ \hline
\textbf{A} & \textbf{A} & 20,26\% & 76,84\% & 72,73\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 71,46\% & 95,03\% & 26,11\% & 27,80\% \\ \hline
\textbf{A} & \textbf{A} & 20,60\% & 64,03\% & 72,32\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système de base avec une transcription enrichie manuelle ou automatique sans liste de locuteurs et sans genre}
\label{tab:results_liumni_sansliste}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 70,97\% & 97,89\% & 26,55\% & 22,82\% \\ \hline
\textbf{A} & \textbf{A} & 20,91\% & 80,64\% & 72,10\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 74,58\% & 97,85\% & 23,23\% & 24,90\% \\ \hline
\textbf{A} & \textbf{A} & 23,77\% & 74,53\% & 69,40\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique sans liste de locuteurs, et avec prise en compte du genre par le prénom}
\label{tab:results_liumni_genre_prenom}
\end{table}


\section{Métriques~: de la pertinence des métriques utilisées}


\section{Transcriptions automatiques~: performances et limites}

\subsection{Système de segmentation et de classification automatique du LIUM}

Comme décrit dans \ref{ssec:ral}, la tâche de segmentation et de classification en locuteur consiste à annoter des régions du signal audio. Ces annotations peuvent être de plusieurs types : des étiquettes représentant les différents locuteurs, le genre des locuteurs, le type de canal (radio, studio) ou encore le type d'environnement sonore comme le bruit, la musique, \dots Dans la tâche de segmentation, les étiquettes représentant les locuteurs sont anonymes. Le système du LIUM est entièrement décrit dans \cite{liumspkdiarization}, ce qui suit en donne un court aperçu.

Le système utilisé au LIUM (LIUM\_SpkDiarization) utilise une segmentation basée sur GLR (Generalized Likelihood Ratio) et BIC \cite{Siegler97automaticsegmentation} (Bayesian Information Criterion). Une première passe permet de détecter les points de rupture du signal constituant les frontières des segments. Ces points de rupture sont détectés en utilisant la mesure de vraisemblance GLR calculée (grâce à des GMM à matrice de covariance pleine) sur une fenêtre de 5 secondes tout le long du signal. Lorsque la mesure atteint un maximum dans cette fenêtre de 5 secondes, le système considère que c'est un point de rupture. Une seconde passe est ensuite effectuée afin de fusionner les segments correspondant au même locuteur, du début à la fin de l'enregistrement. La mesure utilisée pour le regroupement est la distance BIC utilisant des matrices de covariance pleines. 


L'algorithme utilisé pour regrouper les segments au sein de classes se base sur une classification hiérarchique. Au départ, chaque classe est constituée d'un unique segment, chaque classe étant modélisée avec une GMM à matrice de covariance pleine. La mesure utilisée pour regrouper les classes entre elles, et pour stopper le regroupement est la mesure BIC. Les classes les plus proches sont regroupées tant que la distance BIC entre ces classes est inférieure à 0.

La détection du genre est réalisée grâce à des GMM (avec une diagonale à 128 composantes) pour chacun des quatre combinaisons possibles entre le genre (homme/femme) et le canal (studio/téléphone). Chaque classe est étiquetée en fonction de la GMM qui maximise la vraisemblance avec la classe en question. Chaque modèle a été préalablement appris en utilisant le corpus d'apprentissage de la campagne ESTER.

\subsection{Système de transcription automatique du LIUM}

\subsection{Légende}

\textit{\textbf{Trans.}: Transcription \textbf{M}anuelle ou \textbf{A}utomatique.\newline
\textbf{Seg/Class.}: segmentation/classification manuelles ou automatiques.\newline
\textbf{R, P}: rappel et précision calculés en en durée.\newline
\textbf{ErrDur}: Taux d'erreur en durée.\newline
\textbf{ErrLoc} : Taux d'erreur en nombre de locuteurs.\newline}

\subsection{Corpus et données}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Corpus & Courant & Suivant & Précédent & Autre \\
      \hline
      Train & 3,9 & 16,6 & 19,2 & 32,5 \\
      Dev & 8,3 & 17,6 & 9,3 & 45,2 \\
      Test & 26,7 & 17,1 & 12.3 & 39,3 \\      
      \hline
    \end{tabular}
  \end{center}

  \caption{Taux d'erreur des étiquettes courantes / suivantes / précédentes des locuteurs faisant partie de la liste de locuteur fermée ESTER1}
  \label{table:etiquettes_sct}
\end{table}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Corpus & Courant & Suivant & Précédent & Autre \\
      \hline
      Train & 17,3 & 46,3 & 51,9 & 6,8 \\
      Dev & 42,1 & 46 & 40 & 13,2 \\
      Test & 52,1 & 42,4 & 42 & 10,7 \\      
      \hline
    \end{tabular}
  \end{center}

  \caption{Taux d'erreur des étiquettes courantes / suivantes / précédentes des locuteurs en liste ouverte}
  \label{table:etiquettes_sct}
\end{table}

% Résultats sans filtrage de liste de locuteurs sur du manuel (même pas sur le genre) & LIA_NE
%
% Train ESTER 1 (Train spkid)
% Next : 934/2019 = 46.2605250123824 %
% Previous : 188/362 = 51.9337016574586 %
% Current : 36/208 = 17.3076923076923 %
% Other : 666/9766 = 6.81957812819988 %

% Test ESTER 1 (Dev spkid)
%
% Next : 139/302 = 46.0264900662252 %
% Previous : 26/65 = 40 %
% Current : 8/19 = 42.1052631578947 %
% Other : 168/1274 = 13.1868131868132 %

% Test ESTER 2 sans africa (Test spkid)
%
% Next : 132/311 = 42.443729903537 %
% Previous : 37/88 = 42.0454545454545 %
% Current : 12/23 = 52.1739130434783 %
% Other : 132/1238 = 10.6623586429725 %



% Résultats avec filtrage via liste de locuteurs sur du manuel (mais pas avec la prise en compte du genre) & LIA_NE
%
% Train ESTER 1 (Train spkid)
%
% Next : 211/1273 = 16.5750196386489 %
% Previous : 41/213 = 19.2488262910798 %
% Current : 7/179 = 3.91061452513966 %
% Other : 647/1988 = 32.5452716297787 %


% TEST ESTER 1 (Dev spkid)
%
% Next : 34/193 = 17.6165803108808 %
% Previous : 4/43 = 9.30232558139535 %
% Current : 1/12 = 8.33333333333333 %
% Other : 161/356 = 45.2247191011236 % 

% Test ESTER 2 sans africa (Test spkid)
%
% Next : 36/211 = 17.0616113744076 %
% Previous : 7/57 = 12.280701754386 %
% Current : 4/15 = 26.6666666666667 %
% Other : 126/321 = 39.2523364485981 %

\subsection{Utilisation d'une liste de locuteurs globale et de leur genre}

Le tableau \ref{tab:results_fctcroyance} présente les résultats du système utilisant les fonctions de croyances. Les expériences sont menées sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles ou totalement automatiques. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs d'ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste des locuteurs (le genre des locuteurs étant extrait des transcriptions de référence).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 86,16\% & 94,38\% & 12,75\% & 15,35\% \\ \hline
\textbf{A} & \textbf{A} & 21,45\% & 49,96\% & 71,59\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 85,49\% & 95,01\% & 13,34\% & 18,67\% \\ \hline
\textbf{A} & \textbf{A} & 25,63\% & 61,20\% & 67,95\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs et la détection du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Utilisation d'une liste de locuteurs par show et de leur genre}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 87,01\% & 95,75\% & 11,95\% & 14,11\% \\ \hline
\textbf{A} & \textbf{A} & 20,95\% & 61,53\% & 72,01\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 86,35\% & 96,10\% & 12,56\% & 17,43\% \\ \hline
\textbf{A} & \textbf{A} & 24,97\% & 64,90\% & 68,33\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs spécifique au show et la détection du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Influence de la prise en compte du genre}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 80,19\% & 89,33\% & 18,32\% & 19,09\% \\ \hline
\textbf{A} & \textbf{A} & 20,70\% & 45,15\% & 72,81\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 82,24\% & 91,16\% & 16,45\% & 21,58\% \\ \hline
\textbf{A} & \textbf{A} & 24,61\% & 58,82\% & 68,78\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs et sans prise en compte du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Système ouvert sans liste de locuteurs et sans genre}

\ref{tab:results_fctcroyance_sansliste}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 65,74\% & 91,96\% & 31,63\% & 29,46\% \\ \hline
\textbf{A} & \textbf{A} & 19,53\% & 70,52\% & 73,39\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 69,85\% & 94,10\% & 27,79\% & 28,22\% \\ \hline
\textbf{A} & \textbf{A} & 20,59\% & 61,42\% & 72,31\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique sans liste de locuteurs et sans genre}
\label{tab:results_fctcroyance_sansliste}
\end{table}


% \section{Commandes}
% 
% Un mot entre guillemets : \quotes{mot}.
% 
% \todo{écrire cette thèse}
% 
% \tocite{une référence}
% 
% \textit{et caetera} : a, b, c\etc
% 
% \section{Références natbib}
% 
% \citet{exemple1}
% 
% \citep{exemple1}
% 
% \citep[avant][après]{exemple1}
% 
% \citet{exemple2}
% 
% \citep{exemple2}
% 
% \citep[avant][après]{exemple2}
% 
% \section{Les maths}
% 
% \begin{align}
% x^2-y^2&=(x+y)\times(x-y)\label{equation:trivia}\\
% \max_{i<5}f(i)&=\argmin_{j<18} g(j)\nonumber
% \end{align}
% 
% Voilà l'équation~\ref{equation:trivia}.
% 
% \section{Figures et tables}
% 
% Les figures sont centrées, comme on peut le voir dans la figure~\ref{figure:logo_lia}.
% 
% \cfigurex{logos/logo_uapv}{Titre court pour la table des illustrations.}{\label{figure:logo_lia}Caption de la figure\etc c'est le logo du LIA.}{.5}
% 
% Les tables ont un trait en haut et en bas, comme le démontre la table~\ref{table:stats}.
% 
% \ctablexx{lr}{Titre court pour la liste des tables.}{Caption de la table\etc quelques statistiques.}{table:stats}{
% \textbf{Aliment} & \textbf{Quantité} \\
% \hline
% Chips & 198 \\
% Poulet & 1 \\
% }

\postdocument
\end{document}
