
\input{headers}

\begin{document}
%\layout

\dominitoc

\begin{titlepage}

\input{cover}

%\chapter*{Remerciements}

%\chapter*{Résumé}

\tableofcontents

\postheader

\chapter{Introduction}
\minitoc

\chapter{Reconnaissance automatique de la parole}
\minitoc
\newpage

La reconnaissance automatique de la parole (RAP) a pour but de transcrire sous forme textuelle un signal audio. La RAP est un problème complexe, c'est pourquoi seuls des sous-problèmes ont été résolus à ce jour. Des contraintes sont imposées afin de réduire la complexité du problème en ciblant des applications bien particulières. Un système idéal serait capable de transcrire n'importe quel locuteur (qu'il le connaisse ou non), dans n'importe quel environnement sonore (studio, téléphone, dans la rue, etc.) en utilisant un registre de langage et une locution spontanés. Malheureusement, un tel système n'existe pas et des contraintes doivent lui être ajoutées pour fonctionner. Ces contraintes peuvent être la mise à contribution du locuteur pour apprendre sa voix afin de mieux la reconnaître, ou encore la réduction du nombre de phrases que le système va être capable de reconnaître.

Les systèmes de RAP peuvent êtres classés selon plusieurs critères~: 

\begin{itemize}
  \item \textbf{le mode d'élocution} : cela peut être des syllabes ou des mots isolés, des mots connectés entre eux mais avec des pauses artificielles ou alors une parole quasi naturelle, appelée parole continue. 
    
    À noter que pour la parole continue, la distinction est faite entre la parole dite \og préparée \fg{} et la parole dite \og spontanée \fg{} \cite{Bazillon08}. La parole \og préparée \fg{} s'apparente à l'élocution qu'aura un journaliste lorsqu'il présente les informations : les répétitions, les faux départs et autres hésitations du langage parlé y sont peu présentes. La parole \og spontanée \fg{} concerne quant à elle le type de parole utilisée lors des conversations entre plusieurs personnes : les répétitions, hésitations et autres irrégularités du langage parlé y sont très présentes.

  \item \textbf{la taille du vocabulaire} : tous les systèmes ont besoin d'un vocabulaire plus ou moins important. La taille d'un vocabulaire peut être inférieure à 1~000 mots pour les systèmes à petit vocabulaire comme les serveurs vocaux mais peut dépasser les 60~000 mots pour les systèmes de dictée vocale ou de transcription de journaux d'information. En fonction de la taille de ce vocabulaire, différentes couvertures grammaticales sont utilisées. Les systèmes à petit vocabulaire utilisent des grammaires contraintes. En revanche, les systèmes à grands vocabulaires utilisent des modèles de langage (cf. section \ref{ssection:modele_langage}) qui permettent une plus grande couverture grammaticale. 

  \item \textbf{la dépendance (ou non) vis-à-vis des locuteurs} : les serveurs vocaux doivent par exemple être indépendants de la personne qui va l'utiliser, tandis que les systèmes de dictée vocale peuvent se permettre d'être dépendant du locuteur.
  \item \textbf{l'environnement} protégé ou non : les serveurs vocaux vont devoir être capable de fonctionner dans un environnement bruité (on ne peut connaître à l'avance quel sera l'environnement sonore du locuteur qui interrogera le serveur), en revanche les systèmes de dictée vocale fonctionneront en environnement plus protégé.
\end{itemize}

Les applications de la RAP sont multiples et peuvent aller de la simple commande vocale (reconnaissance de mots isolés) à la transcription complète d'une émission de radio. Nous donnons par la suite un aperçu des quatre grands types de systèmes qui existent en reconnaissance de la parole.

\section{Différents types de systèmes}

Les progrès réalisés par les techniques de RAP ont permis aux systèmes d'évoluer et d'être de plus en performants. Des systèmes de commandes vocales ne reconnaissant que de simples mots aux systèmes de transcriptions de journaux d'informations la complexité des SRAP ne cesse d'augmenter. Nous décrivons ici les principaux systèmes mis en place au fil des années.

\subsection{Commandes vocales}

Les systèmes à commandes vocales sont des systèmes que l'on trouve abondamment dans les systèmes embarqués. Ils vont permettre une interaction entre l'utilisateur et la machine grâce à des commandes vocales. Ces commandes sont de simples mots isolés que l'utilisateur doit prononcer pour interagir avec le système. Ils se caractérisent par une taille de vocabulaire réduite et une indépendance vis-à-vis du locuteur. Les cas d'utilisation sont nombreux et il serait impossible de tous les citer ; en voici un bref aperçu :

\begin{itemize}
  \item jeux vidéo : la console portable DS de Nintendo embarque un micro pour permettre l'interaction homme/machine grâce à de mots simples.
  \item téléphones portables : beaucoup de téléphones récents permettent de naviguer dans l'interface grâce à la voix.
  \item aide aux handicapés : de tels systèmes peuvent pallier à un organe défaillant ou peuvent être utilisés dans des systèmes destinés à la ré\-éducation d'enfants sourds.
\end{itemize}

\label{lexique}

Comme tous les systèmes de RAP, ces systèmes se basent sur un vocabulaire (aussi appelé lexique) préalablement constitué. Ce lexique va définir la liste des mots que le système va être capable de reconnaître. Aucun autre mot que ceux contenus dans le lexique ne pourra être reconnu par le système.
Bien qu'il soit techniquement possible d'avoir un vocabulaire assez important, il est généralement limité à une centaine de mots. En effet, les personnes se servant de ces systèmes ne sont de toute façon pas capable de mémoriser plus d'une centaine de commandes. À noter que ces mots doivent être contrastés au niveau phonétique de manière à réduire les risques d'ambiguïté lors de la reconnaissance.

%Bien que d'autres méthodes aient été employées, comme la comparaison de références par programmation dynamique \cite{Tubach}, les systèmes utilisant des méthodes statistiques sont majoritairement utilisés depuis les années 1980 \cite{Rabiner89}. Certains de ces système utilisent des réseaux neuronaux, mais la plus part ont comme base les Modèles de Markov Cachés \cite{Gagnoulet89} (cf. \ref{ssec:trans_auto}). 

\subsection{Systèmes de compréhension}

Les systèmes de compréhension vont permettre un dialogue contraint avec une machine. L'utilisateur va devoir prononcer une suite de mots-clefs que le système est capable de reconnaître. En plus du système de reconnaissance vocale, un dispositif de compréhension des phrases doit être utilisé afin d'interpréter les phrases et de réagir en conséquence. Comme dans les systèmes à commandes vocales, le vocabulaire est restreint à quelques centaines de mots et le système est indépendant du locuteur. Afin de faciliter la gestion du dialogue par le système, les phrases acceptables se limitent à des schémas grammaticaux simplifiés. La mode d'élocution utilisé est généralement une parole continue et spontanée.

Bien que peu naturelles, ces restrictions syntaxiques et sémantiques sont viables si l'application est bien ciblée sur un domaine particulier. Les applications choisies pour ce type de systèmes le sont d'ailleurs en conséquence. Elles se limitent généralement à l'interrogation d'une base de données, de standards téléphoniques automatisés pour des renseignements météo ou des réservations de places.

Ces systèmes ont connu un important essor avec le lancement du projet DARPA aux États-Unis en 1977. Plusieurs laboratoires participent au projet DARPA comment CMU, MIT, BBN, SRI ou encore Bell Labs. Par exemple, le système SPHINX développé au CMU utilise un vocabulaire d'environ mille mots (issus du corpus \og Resource Management Database \fg{} \cite{Pri88}); ce système a notamment le mérite de permettre aux locuteurs une parole continue et ne nécessite pas d'apprentissage préalable du locuteur à reconnaître. Le taux de reconnaissance sur les mots est excellent, il est de l'ordre de 96 \% \cite{Lee1990}.

En France, la recherche s'est orientée vers des systèmes capables de gommer les erreurs de reconnaissance. En effet, même avec des erreurs de décodage du signal sonore, il est souvent possible de comprendre le sens d'une phrase. Ces erreurs de \og bas niveau \fg{} pourraient être récupérées grâce à des systèmes de plus haut niveau. C'est dans cette optique qu'ont été conçus les systèmes KEAL au CNET \cite{MercierKeal}, Esole au LIMSI \cite{Tubach}, Myrtille au CRIN \cite{Pierrel82} et Arial au CERFIA \cite{Perennou82}.

\subsection{Systèmes de dictée automatique}

Les systèmes de dictée automatique ont pour but de transcrire un texte dicté par un locuteur de la façon la plus fidèle possible. Comme pourrait le faire une secrétaire, le texte transcrit doit respecter les règles orthographiques et grammaticales propres à la langue concernée. La compréhension du texte à transcrire n'est pas requise, et c'est d'ailleurs ce qui amène les erreurs les plus perturbantes pour l'utilisateur. En effet, les systèmes de dictée automatique ne sont pas capable de distinguer les différents sens d'un même mot.


Ce type de système se trouve à la charnière entre l'oral et l'écrit. En effet, même si l'interaction avec le système se fait grâce à la voix, le registre de langue utilisé sera plus proche du registre écrit. Ces systèmes sont souvent utilisés pour transcrire des rapports, des comptes-rendus ou rédiger des lettres. Quoiqu'il en soit, la personne est consciente qu'elle s'adresse à un ordinateur et adaptera donc sa locution en conséquence. De plus, les tournures agrammaticales ou les hésitations seront beaucoup moins nombreuses que dans un dialogue spontané entre deux personnes. La complexité est donc moindre que s'il fallait retranscrire un dialogue tel quel. 

En revanche, l'exercice même de la dictée sous-entend l'utilisation de plusieurs dizaines voire plusieurs centaines de milliers de formes fléchies. Les mots sont pris en contexte, et régis par une syntaxe aussi libre que la grammaire de la langue naturelle le permet. Le vocabulaire doit lui aussi être conséquent et est constitué de plusieurs milliers de mots (de 5~000 à plus de 60~000 mots). Avec ce type de système, une séance de dictée ressemblera plus à un médecin qui dicte un compte-rendu d'opération à sa secrétaire qu'à une maîtresse d'école qui fait faire un exercice de français à ses élèves. L'utilisateur n'est donc pas comme l'enseignant, même si il a bien en tête ce qu'il doit dire à la machine, il improvise quand même un minimum. Il doit donc pouvoir se tromper, corriger des mots ou remanier une tournure.

Pour obtenir de bonnes performances en temps réel, ces systèmes à grand vocabulaire sont fortement dépendants du locuteur. Une phase d'apprentissage est requise afin de permettre au logiciel d'apprendre des modèles spécifiques de la voix de la personne qui l'utilise. Cette phase d'apprentissage peut durer jusqu'à une heure.

Historiquement, c'est l'équipe de recherche IBM dirigée par F. Jelinek qui est la première à avoir développé un système grand vocabulaire (\emph{Tangora} 5~000 mots en 1985, 20~000 en 1987) pour la dictée vocale. Par la suite, l'ensemble des grands systèmes développés se sont inspirés du système \emph{Tangora}. Avec un taux de réussite supérieur à 95~\% pour un vocabulaire de 20 000 mots, \emph{Tangora} est un système multilingue existant notamment pour l'anglais \cite{Averbuch87}, l'italien \cite{Alto87}, le français \cite{CerfDanon90} ou encore l'allemand \cite{Wothke89}.


Au milieu des années 90 sont apparues des campagnes d'évaluation ayant pour but de tester les performances des systèmes sur des corpus lus. Par exemple, les données disponibles pour la campagne américaine Hub-3 \cite{Pallet96} sont des extraits de journaux comme le \emph{Wall Street Journal} ou le \emph{New York Times}. Ces extraits sont lus et enregistrés dans différentes conditions de manière à constituer des corpus audio. Le langage utilisé est donc celui utilisé pour de l'écrit. De ce fait, ce type de campagne évalue les performances de systèmes s'apparentant à la dictée vocale. Les performances deviennent rapidement excellentes, le système d'IBM affiche des taux d'erreur de l'ordre de 7~\% \cite{Bahl95}. En France, la première campagne lancée par l'AUPELF-UREF consistait à transcrire des textes lus du journal \emph{Le Monde} en utilisant le corpus BREF \cite{Lamel91}. Comme pour la campagne Hub-3, les résultats sont très bons puisque le système du LIMSI affiche des taux d'erreurs de l'ordre de 11~\% \cite{Gauvain94speaker-independentcontinuous}. Suite à ces succès, de nouveaux systèmes permettant de transcrire des enregistrements non préparés ont vu le jour.

\subsection{Systèmes de transcription enrichie}

\label{ssec:trans_broadcast}

Les systèmes de transcription enrichie ont pour but de transcrire des documents audio non préparés en essayant d'extraire le maximum d'informations de l'enregistrement. Ces informations concernent notamment les locuteurs du document mais aussi les zones de musique ou de publicité. On entend par non préparé un document qui n'a pas été produit dans le but d'être transcrit, à la différence des corpus décrits dans la section précédente. Ceci correspond notamment aux enregistrements de journaux radiophoniques ou d'émissions de télévision. Le mode d'élocution n'est pas contraint et la parole peut être préparée (journaliste qui présente les informations) ou spontanée (interview d'un invité). Le vocabulaire utilisé est très grand, il dépasse généralement les 65~000 mots. Ce type de système est complètement indépendant du locuteur et doit pouvoir s'adapter à l'environnement utilisé lors de l'enregistrement (environnement bruité, studio, téléphone, etc.). 

Un système de transcription enrichie fait intervenir plusieurs modules, dont les deux principaux sont un module de reconnaissance automatique du locuteur et un module de reconnaissance automatique de la parole. Ces modules sont décrits plus en détail dans les sections suivantes.

%Les campagnes d'évaluation se sont adaptées et de nouveaux corpus constitués d'enregistrements de journaux d'informations radiophoniques ont été constitués. Même si ces corpus contiennent beaucoup de parole qui a été préparée et écrite à l'avance par les journalistes, elle est néanmoins convertie à l'oral. À la différence des corpus précédents qui n'étaient constitués que de corpus écrits ensuite lus tel quel. De plus, ces corpus contiennent aussi de la parole plus spontanée, puisque beaucoup d'émissions font intervenir des invités ou interrogent des personnes dans la rue. À noter que ces corpus sont aussi fortement multi-locuteurs. Avec ces nouvelles campagnes d'évaluations, les systèmes sont confrontés à des corpus qui se rapprochent de plus en plus de dialogues complètement spontanés entre plusieurs personnes (même si comme nous l'avons fait remarquer, une partie de la parole est encore relativement préparée).

Aux États-Unis, le DARPA puis le NIST ont organisés des campagnes d'évaluations des systèmes de transcription d'émissions \cite{Graff96the1996} à partir de 1996. En France, c'est la campagne ESTER \cite{Gravier04,Galliano06} organisée par l'AFCP, la DGA et ELRA qui a permis de réaliser les premières évaluations des systèmes de reconnaissance Français sur des journaux d'informations radiophoniques. Une deuxième campagne d'évaluation Française nommée ESTER II \cite{Ester2} a ensuite été menée de 2008 à 2009 de manière à mesurer les progrès des différents systèmes.
%Cette campagne s'est aussi caractérisée par l'évaluation des systèmes de détection des Entités Nommées.


\section{Transcription automatique de la parole}
\label{ssec:trans_auto}

Les techniques de transcription automatique de la parole sont au c\oe{}ur du processus de production d'une transcription~: ce sont elles qui vont permettre de fournir le texte correspondant aux paroles prononcées. Les travaux présentés dans cette thèse s'appuient sur ces techniques, il convient donc de les décrire en détail ici. 


Les systèmes de transcription automatique de la parole utilisés actuellement sont des systèmes fondés sur des méthodes statistiques. Ces fondements ont été élaborés par Jelinek et ses collègues chez IBM \cite{Jelinek1976}, au milieu des années 1970. La figure \ref{fig:principes_srap} donne un aperçu du fonctionnement actuel d'un système de transcription automatique de la parole. 
 
\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/sys_rap}

\caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
\label{fig:principes_srap}
\end{center}

\end{figure}

\subsection{Principes généraux} % (fold)
\label{ssub:principes_generaux}


L'objectif d'un système de RAP probabiliste est d'associer une séquence de mots $\hat{W} = w_1 w_2 ...wk$ (avec $w_i$ qui est un mot de cette séquence) à une séquence d'observations acoustiques $X$. Le système recherche la séquence de mots qui maximise la probabilité \emph{a posteriori} $P(W|X)$, où $P(W|X)$ est la probabilité d'émission de $W$ sachant $X$ . On obtient, après application de la règle de Bayes :

\begin{equation}
  \label{eq:bayes}
  \hat{W} = \underset{W}{arg\ max}\ P(W|X)\ =\ \underset{W}{arg\ max}\ \frac{P(W)P(X|W)}{P(X)}
\end{equation}

Comme la séquence d'observations acoustiques $X$ est fixée, $P(X)$ peut être considérée comme une valeur constante inutile dans l'équation \ref{eq:bayes}. On a donc :

\begin{equation}
  \label{eq:bayes_simplifie}
  \hat{W} =\ \underset{W}{arg\ max}\ P(W)P(X|W)
\end{equation}

Deux types de modèles probabilistes sont utilisés pour la recherche de la séquence de mots la plus probable : des modèles acoustiques qui fournissent la valeur de $P(X |W)$, et un modèle de langage qui fournit la valeur de $P(W)$. $P(X|W)$ peut se concevoir comme la probabilité d'observer $X$ lorsque $W$ est prononcée, alors que $P(W)$ se réfère à la probabilité que $W$ soit prononcée dans un langage donné. La difficulté pour obtenir un système de RAP performant est de définir les modèles les plus pertinents possibles pour le calcul de $P(W)$ et $P(X|W)$ (voir figure \ref{fig:principes_srap}).

\subsection{Modèles acoustiques}

Il est impossible pour un système de transcription d'exploiter le signal de la parole directement. En effet, il contient bon nombre d'informations qui ne sont pas utiles au décodage de la parole (mais qui le seront pour la reconnaissance du locuteur, cf \ref{ssub:decoupage_regroupement}) et qui peuvent le parasiter : des informations sur le locuteur, sur les conditions d'enregistrement, etc. Le signal de la parole étant très variable et redondant, il nécessite de toute façon des traitements spécifiques avant de pouvoir être exploité. Ces traitements (appelés paramétrisation) vont consister à extraire du signal les paramètres qui sont dépendants de la parole prononcée.

Plusieurs techniques de paramétrisation existent, les plus utilisées sont :
\begin{itemize}
  \item PLP (\emph{Perceptual Linear Prediction}) : domaine spectral \cite{Hermansky91}
  \item LPCC (\emph{Linear Prediction Cepstral Coefficient}) : domaine temporel \cite{Markel1982}
  \item MFCC (\emph{Mel Frequency Cepstral Coefficients}) : domaine cepstral \cite{Bridle74,Mermelstein76}
\end{itemize}

Le signal de la parole est modélisable par un ensemble d'unités acoustiques, qui peuvent être considérées comme des sons élémentaires de la langue. Classiquement, l'unité choisie est le phonème. Le phonème est la plus petite unité discrète ou distinctive d'un mot, auquel elle peut faire perdre son sens par substitution. Par exemple, \emph{"chou"} et \emph{"pou"} se distinguent par leurs phonèmes initiaux. Dans le cadre des systèmes de transcription de la parole actuels, ces unités acoustiques sont modélisées par des Modèles de Markov Cachés (MMC) \cite{Rabiner89}. 

L'apprentissage de ces modèles acoustiques s'effectue grâce a des données \emph{a priori} annotées. Cet apprentissage permet d'obtenir une modélisation du message de la parole. Différentes techniques d'apprentissage et d'adaptation sont utilisées, parmi les plus connues figurent notamment :

\begin{itemize}
  \item l'apprentissage par maximum de vraisemblance \cite{Dempster1977} (\emph{Maximum Likelihood : ML}), 
  \item l'algorithme EM \cite{Dempster77maximumlikelihood} (\emph{Expectation/Maximisation}), 
  \item l'estimation par information mutuelle maximale \cite{Valtech97} (\emph{Maximum Mutual Information Estimation : MMIE}), 
  \item l'adaptation par maximum \emph{a posteriori} \cite{Gauvain94maximuma} (\emph{Maximum \emph{a posteriori} probability : MAP}), 
  \item et l'adaptation par régression linéaire \cite{Gales98maximumlikelihood} (\emph{Maximum Likelihood Linear Regression : MLLR}).
\end{itemize}

\subsection{Modèles de langage}
\label{ssection:modele_langage}

Les systèmes de transcription ont besoin de contraintes linguistiques propres à la langue à décoder. Ces contraintes vont êtres utilisées par le système pour guider le décodage dans la sélection d'hypothèses acoustiques concurrentielles. Ces contraintes sont introduites par les modèles de langage dans les systèmes de transcription. Ces modèles sont des modèles probabilistes, et la probabilité d'apparition de la séquence de mots $W$ s'exprime de la façon suivante :

\begin{equation}
P(W^k_1) = P(w_1) \prod_{i=2}^{k} P(w_i|h_i)
\label{eq4}
\end{equation}

Où $h_i$ correspond aux mots précédents $w_i$. $h_i$ est aussi appelé l'historique du mot $w_i$. On a donc $h_i = w_1, ..., w_{i-1}$.

Les modèles utilisés en RAP sont des modèles de langage de type \textit{n-gramme} \cite{Jelinek1976}. Bien qu'ancien, ce type de modèle constitue toujours l'état de l'art.

Ils correspondent à une modélisation stochastique du langage, où l'historique d'un mot est représenté par les $n-1$ mots qui le précèdent. La formule \ref{eq4} peut donc s'écrire~:

\begin{equation}
P(W^k_1) = P(w_1) \prod_{i=2}^{n-1} P(w_i|w_1, ..., w_{i-1}) \prod_{i=n}^{k} P(w_i|w_{i-n+1}, ..., w_{i-1})
\label{eq5}
\end{equation}

Les modèles couramment utilisés dans les SRAP sont généralement des modèles d'ordre 3 ou 4 ($n=3$ ou $n=4$). Nous parlons de modèle \textit{trigramme} ou \textit{quadrigramme} (pour $n=1$ \textit{unigramme}, pour $n=2$ \textit{bigramme}, ...).
Dans le cas d'un modèle quadrigramme, l'équation \ref{eq5} s'écrit :

\begin{equation}
P(W^k_1) = P(w_1) P(w_2|w_1) P(w_3|w_1,w_2) \prod_{i=4}^{k} P(w_i|w_{i-3}w_{i-2}w_{i-1})
\label{eq6}
\end{equation}

Les modèles de langage $n-grammes$ ont la particularité d'être assez souples, puisqu'ils permettent de modéliser des phrases grammaticalement incorrectes. En revanche, ils ne s'interdisent pas non plus de produire des phrases dénuées de tout sens.
En effet, ces modèles de langage sont capables d'attribuer une probabilité à des mots inconnus. Le système est donc capable de probabiliser des phrases qu'il n'a jamais observées dans son corpus d'apprentissage, tout en privilégiant les séquences de mots les plus fréquemment observées. 

\subsection{Évaluation}

La métrique utilisée dans les différentes campagnes (cf. section \ref{ssec:trans_broadcast}) pour évaluer les transcription est le taux d'erreur mot (WER -- Word Error Rate). Pour calculer ce taux d'erreur, la meilleure hypothèse de transcription du système est alignée avec les transcriptions réalisées manuellement (transcriptions dites de référence). Trois types d'erreurs sont alors possibles. Lorsqu'un mot apparaît dans l'hypothèse de reconnaissance alors qu'il n'y a aucun mot correspondant dans la référence, il s'agit d'une insertion. Si, au contraire, il y avait un mot dans la référence et que ce mot n'apparaît pas dans l'hypothèse, nous parlons de suppression. Quand un mot de la référence est remplacé par un autre mot lors du décodage, c'est une substitution. Le taux d'erreur est ensuite déterminé par la formule \ref{eq:wer}. 

\begin{equation}\mathbf{}
  WER=\frac{\mathrm{nb.~insertions + nb.~suppressions + nb.~substitutions}}{\mathrm{nombre~de~mots~de~la~r\acute ef\acute erence}}
\label{eq:wer}
\end{equation}  

À titre d'exemple, lors de la première campagne ESTER \cite{Galliano05}, le meilleur système (celui du LIMSI) atteignait un taux d'erreur mot de 11,9~\%. L'écart entre les différents systèmes était cependant relativement important, puisque le deuxième système, celui du LIUM, affichait un taux d'erreur mot de 23,6~\%. Lors de la campagne ESTER II \cite{Ester2}, le premier système était toujours le LIMSI avec un taux d'erreur de 12,1~\% . En revanche l'écart avec le système du LIUM s'est réduit puisque son taux d'erreur n'était plus que de 17,8~\%.


\section{Transcription enrichie}

\label{sec:trans_enrichie}

Avec les campagnes d'évaluation des systèmes de transcription d'émissions est apparue la notion de transcription enrichie. En effet, les corpus étant constitués d'enregistrements de journaux radiophoniques, d'autres informations que la simple transcription en mots peuvent être extraites des corpus. Par rapport aux corpus de dictée vocale, les documents sont multi-locuteurs. Au sein d'un même enregistrement, des dizaines de locuteurs différents peuvent parler tout au long du fichier. De plus, ces locuteurs peuvent s'exprimer depuis un téléphone ou en studio. Toutes ces informations permettent d'étayer, d'enrichir la transcription. De nouvelles tâches sont donc apparues dans les campagnes permettant d'évaluer l'enrichissement des transcriptions, comme la reconnaissance du locuteur ou encore la détection des entités nommées. La reconnaissance du locuteur est utilisée pour segmenter le document en locuteurs ainsi que pour détecter leur genre ou l'environnement à partir duquel ils s'expriment (studio ou téléphone). La détection des entités nommées est quant à elle utilisée pour étiqueter des groupes de mots particuliers comme les noms de personnes ou encore les lieux.


La figure \ref{fig:etapes_transcription_enrichie} décrit les différentes composantes des transcriptions réalisées pour les campagnes de type ESTER. Lors de la réalisation de ces transcriptions, il est possible de distinguer deux grandes parties que nous détaillons ci-dessous. 

\subsection{Segmentation et classification}
Cette première partie consiste à découper le fichier de manière à obtenir des portions homogènes, c'est à dire concernant le même locuteur et les mêmes conditions d'enregistrement. Ce premier niveau de découpage est communément appelé découpage en segments. Contrairement à ce que l'on pourrait penser, les segments ne représentent généralement pas des phrases, mais plutôt des groupes de souffle : ils commencent et s'arrêtent lorsqu'un silence significatif est détecté. Ce premier découpage est effectué pour des raisons techniques, et notamment pour faciliter la tâche des systèmes de transcription automatique (cf. \ref{ssec:trans_auto}), qui doivent décoder segment par segment pour éviter des problèmes d'explosion combinatoire.

Tous les segments contigus appartenant au même locuteur définissent un tour de parole de ce locuteur. Un même locuteur qui parle à plusieurs endroits d'un enregistrement aura donc plusieurs tours de parole au sein de cet enregistrement. En reconnaissance du locuteur (cf. \ref{ssub:decoupage_regroupement}), on dit que tous les segments (et donc les tours de parole) d'un même locuteur sont regroupés au sein d'une même classe. À chaque locuteur du document correspond donc une classe de locuteur.

Au niveau purement acoustique, il est aussi possible de détecter le genre (masculin ou féminin) du locuteur qui s'exprime \cite{liumspkdiarization}. Cette information peut être ajoutée à la transcription enrichie, comme c'est le cas dans la figure \ref{fig:etapes_transcription_enrichie}.

À noter que pour apprendre et évaluer les systèmes de segmentation et de classification automatiques, des données de références doivent être disponibles. Ces données sont réalisées manuellement par un humain écoutant les enregistrements audios. Il découpe alors le fichier en segments et tours de paroles, puis identifie le nom des différents locuteurs.

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/systeme_transcription_description}

\caption{Les étapes de réalisation d'une transcription enrichie}
\label{fig:etapes_transcription_enrichie}
\end{center}

\end{figure}


\subsection{Transcription et entités nommées}

La deuxième partie consiste tout simplement à écrire les paroles prononcées par les différents locuteurs afin d'obtenir la transcription en mots du signal. Lorsque cette transcription est réalisée automatiquement, la ponctuation ainsi que les majuscules ne sont pas requises. En revanche, des techniques existent pour rajouter la ponctuation et les majuscules aux transcriptions afin de se rapprocher, tant que faire se peut, d'une transcription réalisée manuellement \cite{Beeferman98cyberpunc,Huang2002}.

Les transcriptions, qui plus est les transcriptions de journaux radiophoniques, contiennent un grand nombre d'entités nommées (noms de personnes, lieux, radios, etc). Des systèmes de détection des entités nommées permettent de les repérer dans les transcriptions (cf. section \ref{ssec:EN}). Ce sont des informations supplémentaires qui vont pouvoir être utilisées en recherche d'informations, ou plus précisément dans le cadre des présents travaux, pour identifier les locuteurs d'un document.


%\section{Architecture détaillée}

%Les différentes campagnes d'évaluations des systèmes de RAP actuelles met\-tent l'accent sur la réalisation d'une transcription enrichie (cf. \ref{ssec:trans_broadcast}) via l'extraction d'informations sur les locuteurs ou encore la détection des entités nommées. Lors de la réalisation d'une transcription enrichie, plusieurs systèmes entrent en jeu. Les systèmes de transcription automatique de la parole évidemment, mais aussi des systèmes de détection d'entités nommées et de reconnaissance automatique du locuteur. Afin de situer le contexte des travaux présentés dans cette thèse, ces différents domaines de recherche sont présentés ci-dessous.



\section{Détection des entités nommées}
\label{ssec:EN}

%Avec l'avènement d'Internet, la quantité de données textuelles à disposition augmente de jour en jour de façon exponentielle. Comme c'est le cas pour les données audio, ces données sont, à cause de leur quantité, impossible à indexer efficacement par un humain. L'extraction d'informations et le traitement de ces données est pourtant capitale pour bon nombre de scientifiques et d'industriels. 

Les transcriptions produites par les SRAP contiennent un grand nombre d'informations qui sont très coûteuses à indexer manuellement (l'émission, le sujet traité, les personnes concernées, etc.). Ces informations peuvent être extraites automatiquement pour venir enrichir la transcription grâce à des techniques issues du Traitement Automatique des Langues Naturelles (TALN).

Dans le but d'enrichir une transcription, une des étapes est généralement de détecter les entités nommées. Ces entités nommées vont permettre d'extraire un certain nombre d'information des transcriptions, comme les noms de personne, les lieux, les noms d'organisations, etc.

\subsection{Catégorisation}

Par la richesse des informations qu'elles contiennent, les entités nommées (EN) sont des éléments très importants pour les systèmes d'extraction d'information. Dans les années 1980, les campagnes d'évaluation MUC ont permis de définir ce qu'était une tâche de reconnaissance des entités nommées. Pour MUC-6 \cite{MUC6,MUC6results}, les EN sont les noms propres, les acronymes et éventuellement d'autres mots qui rentrent dans les catégories suivantes :

\begin{itemize}
  \item \emph{Organisation} : regroupe les entreprises, les institutions gouvernementales et les autres organisations ;
  \item \emph{Person} : regroupe les noms de personnes ou de familles ;
  \item \emph{Location} : regroupe les noms de lieux politiquement ou géographiquement définis (villes, pays, régions, etc.) ;
  \item \emph{Time} : regroupe les dates et données temporelles ;
  \item \emph{Number} : regroupe les données numériques comme les sommes d'argent ou les pourcentages.
\end{itemize}

Il existe des typologies des EN beaucoup plus complètes que celle des campagnes MUC comme celle de Paik et al. \cite{Paik96} ou de Coates-Stephens \cite{Coates92}. Par exemple, celle de Coates-Stephens définit 8 catégories d'EN :
\begin{itemize}
  \item les noms de personnes ;
  \item les noms de lieux ;
  \item les noms d'organisations ;
  \item les noms d'origines (noms d'habitants de pays, de villes, de régions, etc.) ;
  \item les noms de législations (\emph{loi Évin}), d'indices boursiers (\emph{Nikkei, CAC 40, Dow Jones}) ;
  \item les noms d'événements (guerres, révolutions, catastrophes, salons, JO, etc.) ;
  \item les noms d'objets.
  
\end{itemize}

Elles couvrent donc les noms propres et des formes linguistiques spécifiques.

\subsection{Les différents types de systèmes}

Selon Poibeau \cite{Poibeau2002} et Sekine et Eriguchi \cite{Sekine2000}, les systèmes de détection des EN peuvent êtres classés en trois grands types :\\

\textbf{Les systèmes fondés sur des règles écrites \og à la main \fg{}}\\
Dans ces systèmes, le concepteur doit élaborer un ensemble de règles qui seront ensuite utilisées pour détecter les entités nommées. Historiquement, cette technique fût la première utilisée dans le domaine. Bien que depuis la campagne d'évaluation MUC-6, l'apprentissage automatique ait fait son apparition dans ce domaine, les systèmes à base de règles écrites \og à la main \fg{} restent encore très utilisés aujourd'hui.\\

\textbf{Les systèmes à base d'apprentissage automatique}\\
Ces systèmes utilisent des techniques d'apprentissage automatique pour apprendre un modèle capable d'étiqueter des entités nommées à partir d'un corpus d'apprentissage. Ces travaux sont largement inspirés des techniques utilisées en reconnaissance automatique de la parole, et utilisent diverses techniques d'apprentissage : modèles de Markov cachés, modèle d'entropie maximum, arbres de décision, etc. Le système (LIA\_NE) développé par Béchet au Laboratoire d'Informatique d'Avignon (LIA) pour ESTER2 \cite{Ester2} fait partie de ce type de systèmes.\\ 

\textbf{Les systèmes mixtes}\\
Ces systèmes utilisent généralement des lexiques initiaux. Parmi ces systèmes, Poibeau \cite{Poibeau2002} distingue deux approches. La première consiste à apprendre automatiquement des règles, puis à utiliser un expert pour les réviser. Dans la seconde, un ensemble de règles de base est constitué par le concepteur, puis étendu (semi-)automatiquement par inférence, afin d'obtenir une meilleure couverture. C'est le cas de Nemesis, le système développé par Fourour \cite{Fourour04} au Laboratoire d'Informatique de Nantes Atlantique (LINA).\\



\section{Reconnaissance automatique du locuteur}
\label{sec:ral}

La Reconnaissance Automatique du Locuteur (RAL) regroupe un ensemble de méthodes capables d'extraire les caractéristiques vocales propres à chaque individu à partir d'un document audio. La RAL joue un rôle important dans l'enrichissement d'une transcription, puisqu'elle va permettre d'apporter des informations sur les différents locuteurs du document.
  

\subsection{Caractéristiques et variabilité}
\label{ssec:carac_variabilite}

Toutes les techniques de RAL se basent sur l'extraction des caractéristiques (\emph{features}) du signal audio \cite{Atal76,Doddington85,hollien_acoustics_of_crime90,oshaughnessy86,Sambur75}. Ces caractéristiques doivent être robustes par rapport aux conditions d'enregistrement et fournir le plus de renseignements possibles concernant l'identité du locuteur.

Une question importante est celle de la variabilité des caractéristiques mesurées. On parle de variabilité intra-locuteur lorsque l'on s'intéresse à la variation des paramètres mesurés pour un même locuteur, et de variabilité inter-locuteur si on considère des locuteurs différents \cite{Doddington85,hollien_acoustics_of_crime90,Rosenberg91}. Pour la reconnaissance du locuteur, on cherche à extraire des caractéristiques du signal de parole qui présentent une forte variabilité inter-locuteur (pour pouvoir différencier les locuteurs entre eux) et une faible variabilité intra-locuteur (pour garantir la robustesse du système).

\subsection{Applications}

Des serveurs vocaux aux terminaux mobiles en passant par les dispositifs de sécurité, la RAL est utilisée dans de nombreux domaines. Certains serveurs vocaux l'utilisent notamment pour détecter le genre du locuteur qui s'exprime. Par exemple, grâce à la plateforme MISTRAL, la société Calistel \cite{MeignierMistral} propose de router les appels provenant de locuteurs masculins vers des opératrices féminines et inversement, afin d'obtenir un taux de satisfaction des appels plus important. Mais les applications de la RAL les plus connues sont certainement celles qui tournent autour de la biométrie. 

Dans les applications biométriques, un système de RAL va essayer de reconnaître, grâce à sa voix, un locuteur qui cherche à s'identifier auprès d'un terminal. Même si ces systèmes donnent de bons résultats, ils ne sont pas parfaits. Les systèmes à l'état de l'art comme MISTRAL ont un taux d'erreur aux alentours de 5~\% \cite{MeignierMistral}. Ils ne peuvent donc pas être considérés comme aussi sûr que les empreintes digitales et doivent être utilisés avec prudence à des fins judiciaires par exemple \cite{Boe99, Bonastre2003}. En revanche, avec l'arrivée des terminaux mobiles dans la vie de tous les jours, la RAL s'invite avec succès dans les téléphones portables de dernière génération \cite{Larcher_JEP10}.


L'utilisation de la RAL dans les applications que nous venons de citer peut se découper en quatre grandes catégories décrites ci-dessous. Ces catégories sont d'ailleurs sujet à évaluation dans les différentes campagnes comme NIST \cite{Alvin04nistspeaker} et ESTER \cite{Gravier04}.

%En plus de la difficulté à différencier deux locuteurs différents, la RAL est aussi confrontée à des problèmes intra-locuteurs. En effet, la voix d'un locuteur peut fortement varier d'un enregistrement à l'autre. Le niveau de fatigue, l'émotion, le contexte d'enregistrement ou encore l'âge d'un locuteur sont autant de variations qui peuvent venir perturber la RAL. Mais la biométrie n'est pas la seule application possible, de par ses caractéristiques, la RAL est utilisée dans bien d'autres domaines.

\subsection{Identification automatique du locuteur}

L'identification automatique du locuteur (IAL) a été une des premières utilisations de la RAL \cite{Atal76}. En IAL, la liste des locuteurs à identifier est connue du système. Le système doit pouvoir décider, à partir d'un échantillon de voix, à quelle identité connue du système correspond l'échantillon. La figure \ref{fig:principe_ial} décrit le principe général de l'IAL.

L'identification automatique du locuteur se découpe en deux étapes : une étape d'apprentissage et une étape de test. À partir d'un ensemble d'enregistrements de voix de chaque locuteur, le système apprend un modèle pour chaque locuteur lors de l'étape d'apprentissage. Lors de l'étape de test, le système confrontera l'échantillon de voix qu'il recevra aux différents modèles qu'il aura déjà appris afin de déterminer si l'identité du locuteur est déjà connue.

En fonction de l'application, deux types de décisions sont possibles. Prenons le cas d'un centre d'appel téléphonique qui enregistre les conversations de ses employés. Un système de IAL peut être utilisé pour classer chacun des enregistrements en fonction de l'employé concerné. Dans ce cas, la liste des locuteurs possibles est connue du système (tous les employés), et aucun autre locuteur non employé ne peut être concerné. Dans ce cas, l'application présuppose que l'ensemble des locuteurs possible est fermé, et connu du système. Le système de IAL choisira alors, parmi la liste, le modèle de locuteur le plus ressemblant à l'enregistrement de test.  

En revanche, dans une application utilisant un ensemble ouvert de locuteurs possibles, le système de IAL ne connait pas tous les locuteurs possibles. Dans ce cas, en plus de déterminer le locuteur le plus vraisemblable, le système a la possibilité de rejeter l'échantillon de test en ne renvoyant aucune identité connue pour cet échantillon. 

\begin{figure}[h]

\begin{center}
\includegraphics[width=1.0\columnwidth]{img/ial}

\caption{Principe de l'identification automatique du locuteur}
\label{fig:principe_ial}
\end{center}

\end{figure}

\subsection{Vérification automatique du locuteur}

La vérification automatique du locuteur (VAL) permet de décider si l'identité revendiquée par un locuteur est compatible avec sa voix. Il s'agit donc de trancher entre deux hypothèses : soit le locuteur est bien le locuteur autorisé (on l'appelle aussi locuteur client), c'est à dire que son identité correspond à celle qu'il revendique, soit le locuteur est un imposteur qui cherche à se faire passer pour la personne qu'il n'est pas. À partir d'un échantillon de voix de référence, et d'un échantillon de voix de test, le système va donc devoir dire si oui ou non les deux locuteurs correspondent \cite{Atal76}.

Les systèmes de VAL sont très dépendants des différences entre les échantillons de voix de référence et les échantillons de tests (cf. \ref{ssec:carac_variabilite}). Accepter un locuteur qui devrait être rejeté peut avoir de lourdes conséquences, en particulier dans les applications où un haut niveau de sécurité est demandé \cite{Bonastre2003} (contrôle aux frontières, système bancaire, identification judiciaire, etc.). 

\subsection{Suivi de locuteur}

\label{ssec:sl}

Le suivi de locuteur (SL) se base sur la tâche de vérification du locuteur. À partir d'un enregistrement de référence d'un locuteur, le système va devoir déterminer si le locuteur intervient dans le document. Si c'est le cas, il devra être capable de préciser où et quand ce locuteur intervient dans l'enregistrement. À noter qu'à part le locuteur à suivre, aucun autre locuteur du document n'est connu du système. Plusieurs méthodes différentes décrites dans \cite{EURECOM269} peuvent être employées pour réaliser cette tâche, la figure \ref{fig:suivi_locu} propose un aperçu général du principe du SL.

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/suivi_locu}

\caption{Principe du suivi de locuteur}
\label{fig:suivi_locu}
\end{center}

\end{figure}

\subsection{Segmentation et classification en locuteur}

\label{ssub:decoupage_regroupement}

La tâche de segmentation et de classification en locuteur consiste à délimiter l'intervention de chaque locuteur dans le document audio. À la différence du suivi de locuteur, aucune information \emph{a priori} sur les locuteurs du document n'est disponible pour cette tâche. Plus précisément, aucune information sur le nombre de locuteurs n'est disponible, ni leur identité, ni des modèles de voix qui auraient pu être appris au préalable. 

Les travaux fondamentaux de la segmentation et de la classification en locuteur ont été réalisés au début des années 1990 par la société BBN sous la direction de H. Gish \cite{Siu1991,Siu1992}. Ces travaux consistent à indexer les différents échanges radio entre pilotes et contrôleurs aériens. Ces échanges sont enregistrés puis segmentés automatiquement. Ces enregistrements peuvent contenir plusieurs dialogues contrôleur/pilote. La segmentation est ensuite utilisée pour reconstruire ces dialogues.

Depuis, les techniques ont évoluées \cite{TranterReynolds2006} et le champ d'application de la segmentation en locuteurs s'est étendu, elle se retrouve intégrée dans le cadre plus vaste de l'indexation en locuteurs de bases de données de documents multimédia \cite{PhdMeignier}. Les documents traités sont divers et variés, on peut notamment citer les conversations téléphoniques, les enregistrements de journaux télévisés ou radiophoniques, les films ou encore les enregistrements de réunion.

La figure \ref{fig:seg_classif} décrit les principes du processus de segmentation et de classification qui s'effectue en plusieurs étapes :

\begin{itemize}
  \item La \textbf{paramétrisation} va extraire les caractéristiques acoustiques du document audio. Ces caractéristiques seront représentées via des vecteurs acoustiques qui pourront ensuite être exploités par les autres phases du processus.
  \item La \textbf{segmentation} va s'attacher à trouver des points de ruptures, des frontières, au sein de l'enregistrement. Ces frontières peuvent être de différentes natures : changement de locuteur, changement de canal de transmission (studio, téléphone), présence d'un long silence, musique, etc. Les portions de signal entre ces différentes frontières sont appelées segments. Ces segments contiennent des données homogènes : même locuteur, même canal de transmission.
  \item La \textbf{classification} groupe les segments locuteur par locuteur jusqu'à découvrir le nombre de classes correspondant au nombre de locuteurs intervenant dans le document. Chaque classe contient à la fin de la classification l'ensemble des segments prononcés par un locuteur. La classe définit alors l'intervention du locuteur dans le document.
  \item La \textbf{resegmentation} affine les frontières des segments. Dans cette étape, le document est de nouveau découpé en segments en fonction des données contenues dans les classes.
\end{itemize}

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.8\columnwidth]{img/segmentation-classification}

\caption{Principe de la segmentation et classification en locuteur}
\label{fig:seg_classif}
\end{center}

\end{figure}

À noter que les classes de locuteur produites sont anonymes. En effet, ce type de système étiquette chacune des classes avec un nom unique, mais dénué de tout sens. Le processus qui consiste à étiqueter ces classes avec le vrai couple prénom/nom du locuteur auquel elles se rapportent est appelé \textbf{identification nommée du locuteur}. C'est à cette tâche que s'intéressent les travaux de cette thèse. 


%Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.

%Dans un enregistrement audio, chaque prise de parole d'un locuteur est appelée \og tour de parole \fg. Ces tours de paroles commencent quand un locuteur prend la parole et s'arrêtent lorsqu'un autre locuteur prend la parole (ou lorsqu'un jingle ou une publicité coupe l'intervention). Ces locuteurs peuvent n'intervenir qu'une fois dans le fichier, auquel cas ils n'auront qu'un tour de parole. Mais, comme les journalistes ou les présentateurs, beaucoup de locuteurs sont présents tout au long du fichier et prennent la parole à plusieurs endroits dans l'enregistrement. Ces différents tours de paroles appartiennent au même locuteur, ils sont alors regroupés au sein de la même \og classe de locuteur \fg{}. La figure \ref{fig:decoupage_tours_classes} donne un exemple de découpage en tours de parole et de regroupement en classes de locuteurs.



%On peut noter sur la figure \ref{fig:decoupage_tours_classes}, qu'en plus de regrouper les tours de parole par classe de locuteur (classe du Locuteur 1, classe du Locuteur 2), le genre du locuteur est aussi mentionné. Cette information vient enrichir la transcription et pourra être utile par la suite. Les classes de locuteur sont pour l'instant anonymes. En effet, la transcription ne nous permet pas de dire que c'est \og Monsieur untel \fg{} qui parle, elle permet juste de dire que c'est le même locuteur qui parle à deux endroits différents. La solution idéale serait de pouvoir nommer les classes de locuteurs par le prénom et le patronyme du locuteur concerné (couple que nous appellerons par la suite nom complet), c'est ce que les techniques d'identification nommée du locuteur cherchent à réaliser.


%\chapter{Systèmes automatiques}
%\minitoc
%
%\label{sec:sys_auto}
%
%Depuis les débuts de l'informatique, l'Homme a toujours essayé de l'utiliser pour automatiser des tâches humaines. Que ce soit de la simple calculatrice à la reconnaissance de la parole, l'informatique est de nos jours utilisée dans tous les domaines. Mais pour pouvoir réaliser ces tâches, les programmes informatiques doivent avoir une certaine connaissance des actions à effectuer pour pouvoir les réaliser.
%
%Il existe deux principales façons d'insuffler ces connaissances à un programme. La première est de tout simplement lui donner les règles qui vont lui permettre de réaliser les tâches qu'il a à accomplir. Dans l'exemple de la calculatrice, il suffit de lui fournir les différentes formules arithmétiques pour qu'elle puisse fonctionner. Ce type de procédé marche bien lorsqu'il existe des règles formelles pour réaliser une tâche. Mais ces règles ne sont pas toujours disponibles ou peuvent varier fortement en fonction du contexte, c'est par exemple le cas en reconnaissance de la parole.
%
%La reconnaissance de la parole fait intervenir plusieurs domaines complexes à gérer pour un système automatique. Il faut tout d'abord transformer le signal audio en paramètres exploitables par un programme informatique. Il faut ensuite transformer ce signal audio en phonèmes, puis les phonèmes en mots et les mots en phrases. Ces étapes dépendent de beaucoup de paramètres comme les conditions d'enregistrement, la langue utilisée dans le document, l'accent des personnes qui parlent ou leur façon d'utiliser la langue (néologismes, mauvaises constructions grammaticales). En plus de variabilité de ces paramètres, il est impossible de définir des règles formelles pour encadrer chacun d'eux.
%
%Prenons l'exemple d'un enregistrement dans lequel les locuteurs parlent français. Même s'il existe une grammaire et un orthographe bien définis pour la langue française écrite, ce n'est pas le cas à l'oral. En effet les contractions (cheval sera prononcé ch'val), les faux départs et les répétitions sont légions. En plus de ces différences entre le français écrit et le français parlé, les locuteurs peuvent très bien faire des fautes de grammaire ou employer des mots qui ne font pas parti du dictionnaire courant. Toutes ces variations dans l'utilisation de la langue française rendent difficile l'utilisation de règles formelles pour reconnaître les mots prononcés. C'est pour pallier à ces problèmes que les systèmes de reconnaissance automatique de la parole utilisent des techniques d'apprentissage automatique qui vont permettre au système d'apprendre automatiquement de nouvelles connaissances en fonction des données qui lui seront fournies.
%
%\section{Apprentissage automatique}
%\subsection{Généralités}
%
%L'apprentissage automatique (qui est un sous-domaine de l'Intelligence Artificielle) a pour but de permettre à une machine, un programme, de s'adapter et d'évoluer grâce aux données qui lui sont fournies, de manière à remplir des tâches complexes qu'il serait difficile, voir impossible, de remplir avec des méthodes plus traditionnelles. Un des défis de l'apprentissage automatique est d'extraire les données pertinentes présentes dans les corpus d'apprentissage afin d'en tirer des généralités applicables à d'autres données. L'apprentissage automatique repose sur la notion de classification. La classification va viser à étiqueter chaque donnée en l'associant à une classe. Ces classes peuvent être connues à l'avance ou déterminées automatiquement par le système. C'est ce qui va différencier l'apprentissage supervisé de l'apprentissage non supervisé. 
%
%\subsection{Apprentissage supervisé}
%\label{sssec:apprentissage_supervise}
%L'apprentissage supervisé va permettre d'apprendre des règles à partir de données d'apprentissage servant d'exemple. Ces données sont généralement validées et étiquetées par des êtres humains. C'est à partir de ces exemples déjà classés que le système va apprendre des règles essayant de reproduire la classification à partir des données. Par exemple, les arbres de décision décrits dans \ref{sec:sct} sont des outils utilisant un apprentissage supervisé, puisqu'ils vont permettent de ranger des données de test dans des classes prédéfinies. Ils auront été appris grâce à des données d'apprentissage étiquetées au préalable avec les bonnes classes.
%
%\subsection{Apprentissage non supervisé}
%
%L'apprentissage non supervisé (aussi appelé classification automatique) consiste à trier un groupe hétérogène de données en regroupant les données homogènes au sein d'une même classe, et les données hétérogènes au sein d'autres classes. À la différence de l'apprentissage supervisé, il n'y a pas de \og sortie attendue \fg{}. C'est à dire que c'est au système d'établir ses propres classes, au lieu de devoir classer les données dans des classes prédéfinies comme pour l'apprentissage supervisé. En reconnaissance automatique de la parole, et plus précisément dans le domaine de la reconnaissance du locuteur, les modèles à mélange de gaussiennes (aussi appelés GMM) sont un bon exemple de système d'apprentissage non supervisé. Ils permettent de représenter une distribution multidimensionnelle quelconque par une somme pondérée de distributions gaussiennes. Chaque distribution gaussienne étant caractérisée par un vecteur moyen et une matrice de covariance.

%\subsection{Transcription enrichie}
%\label{sub:transcription_enrichie}

%Transcrire un signal audio consiste d'une part à retranscrire les mots qui ont été prononcés mais aussi à enrichir cette transcription avec différentes informations éventuellement disponibles comme le début et la fin de chaque intervention, le nom des différents locuteurs ou encore leur genre. Ces transcriptions peuvent être réalisées par un humain (elles seront appelées transcriptions manuelles) ou de manière entièrement automatique (elles seront appelés transcriptions automatiques) : en fonction de cela, les informations disponibles dans la transcription peuvent varier, c'est notamment le cas du nom des locuteurs. En effet, un humain pourra essayer, en fonction du contexte, de nommer les locuteurs de l'enregistrement à l'inverse d'un système automatique qui utilisera des étiquettes anonymes en guise de nom de locuteur (comme locu1, locu2, ...). C'est précisément à cet aspect de la transcription enrichie (à savoir le nommage des locuteurs) que les travaux sur l'identification nommée du locuteur s'intéressent. Il convient tout d'abord d'expliquer plus précisément les différents aspects d'une transcription enrichie.



% \subsubsection{Transcription et étiquetage des entités nommées}
% 
%   La transcription du signal audio consiste tout simplement à écrire les mots qui ont été prononcés par les locuteurs. Que cette transcription soit réalisée par un humain ou par un système automatique de reconnaissance de la parole (RAP), elle consiste à remplir les différents tours de parole avec les mots correspondants. QUand les transcriptions sont réalisées par des systèmes de RAP, elles sont beaucoup moins riches que celles produites par un humain. En effet, en plus des erreurs de transcription, elles ne contiennent généralement pas de ponctuation. Ce manque peut être comblé par des post-traitements qui essaient de remettre la ponctuation dans la transcription. 
% 
% Un autre type de post-traitement utile pour l'identification nommée du locuteur est la détection des entités nommées. En effet, pour pouvoir attribuer un nom complet à un locuteur à partir de la transcription, il va d'abord falloir être capable de détecter ce nom complet dans la transcription. Pour ce faire, il existe des systèmes de détection des entités nommées qui vont être capable de détecter, en plus des noms complets, différentes entités nommées comme les lieux, les organisations ou encore les radios. La figure \ref{fig:etapes_transcription_enrichie} montre toutes les étapes pour obtenir une transcription enrichie en entités nommées.


% \subsection{Les systèmes automatiques : reconnaissance de la parole et détection des entités nommées}
% 
% À l'heure où l'informatique et Internet sont partout, les quantités de données numériques ne cessent de croître. Ces grandes collections de données sont difficilement indexables manuellement, il faut donc, pour faciliter la recherche et l'accès à l'information, qu'elles soient traitées de manière automatique. Les enregistrements audio sont traités à l'aide de systèmes de reconnaissance automatique de la parole (SRAP), ces systèmes sont constitués de plusieurs composantes comment le montre la figure \ref{fig:principes_srap}.
% 
% \begin{figure}
% 
% \begin{center}
% \includegraphics[width=0.9\columnwidth]{img/sys_rap}
% 
% \caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
% \label{fig:principes_srap}
% \end{center}
% 
% \end{figure}
% 
% \subsubsection{Principes généraux} % (fold)
% \label{ssub:principes_generaux}



% subsubsection principes_généraux (end)

% \subsubsection{Segmentation et classification} % (fold)
% \label{ssub:les_systemes_de_segmentation_et_de_classification_en_locuteurs}
% 
% Afin de traiter de manière efficace un signal complexe comme le signal audio, les SRAP ont besoin de segmenter le signal en parties homogènes appelées segments. Ces segments se doivent d'être cohérents, un même segment doit avoir les mêmes conditions d'enregistrement, doit être prononcé par le même locuteur, ... Ces segments sont caractérisés par des conditions acoustiques spécifiques comme la présence de parole, la nature de la parole (téléphonique ou enregistrement studio), la présence de musique, le genre du locuteur ou encore son identité ou une étiquette anonyme. Comme décrit dans \ref{ssub:decoupage_regroupement}, l'identification nommée utilise les tours de parole. Un tour de parole n'est en fait qu'une suite de segments dont l'identité ou l'étiquette sont identiques.
% 
% 
% Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.
% 
% 
% Les méthodes qui donnent les meilleurs résultats sur les journaux radiophoniques utilisent un regroupement BIC suivi d'un regroupement de type CLR [1, 2]. C'est ce type de système \ref{article sylvain} qui sera utilisé pour les travaux du présent papier.
% 
% Ces méthodes sont évaluées selon une métrique nommée DER (Diarization Error Rate). TODO : expliquer et donner la formule

% subsubsection les_systèmes_de_segmentation_et_de_classification_en_locuteurs (end)

% \subsubsection{Transcription automatique de la parole} % (fold)
% \label{ssub:transcription_automatique_de_la_parole}
% 
% Les systèmes de transcription automatique de la parole sont évalués selon une métrique nommée WER (Word Error Rate). TODO : expliquer et donner la formule

% subsubsection transcription_automatique_de_la_parole (end)


% \subsubsection{Détection des entités nommées} % (fold)
% \label{ssub:detection_des_entites_nommees}

% subsubsection détection_des_entités_nommées (end)


\chapter{L'identification nommée du locuteur}
\minitoc
\newpage

L'identification nommée du locuteur (INL) consiste à nommer les locuteurs d'un document audio en leur attribuant un prénom et un patronyme. Ces informations viennent enrichir la transcription et peuvent être utilisées dans plusieurs domaines d'applications. 

\section{Applications}

Disposer des noms complets des intervenants d'un document audio peut être utilisé à des fins de recherche d'informations. En effet, les bases de données audio et vidéo ne cessent de croître avec la numérisation massive des documents audiovisuels. Ainsi, les besoins en indexation et recherche automatiques croissent de la même manière. Par exemple en France, l'Institut National de l'Audiovisuel (INA) a lancé un plan de sauvegarde et de numérisation de ses différents documents en 1999. Ce plan a pour but de numériser plusieurs millions d'heures d'enregistrements de radio et de télévision. Il doit permettre de ne pas perdre les données qu'elles contiennent à cause de l'obsolescence des supports. Suite au lancement de ce plan de sauvegarde, de nombreux travaux ont été menés à l'INA sur l'indexation automatique des documents \cite{Veneau2001,Allauzen2003,Joly2004,Poli2007}. L'utilisation de l'INL pour ce type de bases de données pourrait apporter un réel plus pour l'indexation. Elle permettrait par exemple de faciliter la recherche des différents intervenants dans les documents indexés ou encore de remplir automatiquement les grilles de programmes avec le nom des intervenants.

Les récentes campagnes d'évaluation des systèmes de reconnaissance automatique de la parole (cf. \ref{ssec:trans_broadcast}) s'intéressent à l'enrichissement de la transcription (cf. \ref{sec:trans_enrichie}) par diverses informations comme les entités nommées ou encore les informations sur les locuteurs du document. Cette transcription contient notamment une segmentation et une classification en locuteurs (cf. \ref{ssub:decoupage_regroupement}) qui permettent de délimiter les interventions des différents locuteurs dans l'enregistrement. En revanche, les informations fournies par le processus de segmentation et de classification en locuteur ne permettent pas d'identifier les locuteurs du document par leur prénom et patronyme. Les locuteurs sont uniquement identifiés par des labels anonymes du type \emph{Locuteur1}, \emph{Locuteur2}. L'INL s'inscrit parfaitement dans l'enrichissement de la transcription en permettant de remplacer ces labels anonymes par la \og vraie identité \fg{} des locuteurs. On entend par \og vraie identité \fg{} du locuteur son prénom et son patronyme. Ce couple prénom/patronyme sera par la suite désigné comme étant le \textbf{nom complet} du locuteur.

Afin de pouvoir attribuer aux locuteurs d'un document leurs nom complets, il faut obtenir leurs prénoms et patronymes. Deux principales approches sont possibles : disposer d'informations a priori sur les locuteurs comme dans le cas du suivi de locuteur (cf. \ref{ssec:sl}) ou utiliser les informations fournies par le document lui même pour déterminer l'identité des locuteurs. Nous commençons par présenter la première approche avant de nous intéresser plus longuement à la deuxième approche qui constitue le c\oe{}ur de cette thèse.

%Contrairement à la reconnaissance de la parole et aux tâches de reconnaissance du locuteur présentées dans le chapitre précédent, l'identification nommée de locuteurs est un domaine de recherche relativement récent. En effet, ce sont les travaux du LIMSI et plus particulièrement de Leonardo Canseco-Rodriguez \cite{CansecoRodriguez2005}, qui ont été les premiers à s'intéresser à l'identification nommée du locuteur utilisant la transcription enrichie (réalisée manuellement) d'enregistrements radiophoniques.

\section{Utilisation de connaissances a priori}
\label{sub:suivi_locuteur}

Obtenir l'identité d'un locuteur a d'abord été réalisé avec des méthodes purement acoustiques \cite{Bimbot2004}. Le suivi de locuteur (décrit dans le chapitre \ref{sec:ral}) peut être étendu à $n$ locuteurs afin d'identifier tous les locuteurs d'un document. Bien qu'utilisées depuis des années, ces méthodes souffrent de plusieurs inconvénients. 

Tout d'abord, il est absolument nécessaire de connaître les personnes que l'on cherche à identifier. En effet, les systèmes doivent commencer par apprendre des modèles acoustiques correspondant à chaque locuteur du document pour ensuite les associer aux différents intervenants de l'enregistrement traité. Ils ne peuvent donc identifier que des personnes dont ils possèdent déjà les modèles. De plus, lorsque l'on peut apprendre ce modèle acoustique, il faut posséder une quantité de données suffisante pour pouvoir l'apprendre : plusieurs minutes sont un minimum.

Pour finir, il faut que les conditions acoustiques des données d'apprentissage soient similaires à celles sur lesquelles le système va chercher à détecter les locuteurs : des données trop éloignées dans le temps (et donc avec une voix qui peut avoir changée) ou des conditions d'enregistrement différentes (studio ou téléphone par exemple) dégraderont les performances d'identification (cf \ref{ssec:carac_variabilite}).

Dans l'hypothèse d'une utilisation dans les récentes campagnes d'évaluation traitant de journaux radiophoniques (comme ESTER ou les campagnes NIST), il faudrait disposer des enregistrements de tous les locuteurs (présentateurs, invités, intervenants, etc.) en quantité suffisante. S'il semble plausible de pouvoir les obtenir pour les journalistes qui sont souvent présents dans les émissions de radios, il semble beaucoup plus compliquer de les récupérer pour certains invités peu connus ou pour les inconnus interrogés par téléphone. 

Ces différentes limitations ont motivé les récents travaux ne nécessitant pas de connaissances a priori sur les locuteurs pour réaliser l'identification. Ces travaux se basent sur la transcription du signal audio pour identifier les locuteurs d'un enregistrement. 

\section{Utilisation des informations de la transcription}

Puisque le but de ces méthodes est de n'utiliser aucune connaissance a priori sur le document traité, elles doivent extraire les noms complets à partir des informations disponibles dans le fichier audio. La transcription en mots du signal est une source d'informations de choix pour trouver les prénoms et les patronymes des locuteurs : dans beaucoup d'enregistrements (et notamment ceux provenant des journaux radiophoniques) les noms complets des intervenants font partie de la transcription. L'identification nommée consiste donc à détecter les noms complets présents dans la transcription, pour ensuite les attribuer aux différents locuteurs du document. 

La figure \ref{fig:principe_inl} donne un aperçu de ce que l'identification nommée à partir de la transcription cherche à réaliser. 

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/discussion_nommee}

\caption{Aperçu global de l'identification nommée du locuteur utilisant la transcription}
\label{fig:principe_inl}
\end{center}

\end{figure}


\subsection{Présence des noms complets}

\label{ssec:presence_noms_complets}

À partir du moment où les noms complets des locuteurs sont cités dans le document lui même, il semble possible de les exploiter pour identifier les différents intervenants. C'est l'hypothèse de base de toutes les méthodes utilisant la transcription pour réaliser l'identification nommée : les noms complets des locuteurs du document sont présents dans le document lui même.

Même si cette hypothèse est très forte, elle permet d'exploiter un grand nombre de documents. Les journaux d'informations (qu'ils soient radiophoniques ou télévisuels) sont par exemple tout à fait adaptés à ce style de techniques. Dans ce type de document, la majorité des locuteurs s'annoncent ou sont annoncés. C'est systématiquement le cas des présentateurs et des invités par exemple. Dans les enregistrements de réunions, les locuteurs sont présentés et se passent la parole : il est donc possible de récupérer leurs noms complets. De manière plus générale, tout type de document où les locuteurs s'annoncent ou sont annoncés vont pouvoir être exploités avec les techniques d'identification nommée basées sur la transcription du signal audio.

Les systèmes d'INL devront donc être capable de détecter ces noms complets dans la transcription du signal pour pouvoir ensuite les exploiter. C'est le travail des systèmes de détection des entités nommées présentés dans la partie \ref{ssec:EN}.

Mais disposer de documents dans lesquels les noms complets des locuteurs sont cités n'est pas suffisant, il faut ensuite être capable d'attribuer ces noms complets aux différents intervenants du document. La partie suivante décrit le principe d'attribution commun à toutes les méthodes d'INL basées sur la transcription.


\subsection{Attributions locales}
\label{ssec:attributions_locales}
La deuxième hypothèse sur laquelle reposent les techniques utilisant la transcription est qu'il est possible d'exploiter les noms complets contenus dans la transcription pour identifier les locuteurs du document. La technique utilisée par tous les systèmes d'identification nommée à partir de la transcription est la même : attribuer un nom complet détecté dans le document au locuteur qui est en train de parler (tour de parole courant), au locuteur qui parle ensuite (tour de parole suivant) ou au locuteur qui vient de parler (tour de parole précédent). La figure \ref{fig:segment} résume le principe de cette attribution, que l'on doit au LIMSI et Leonardo-Canseco \cite{CansecoRodriguez2005}.


\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/segment.pdf}
\caption{Principe de base des systèmes d'identification nommée basés sur la transcription}
\label{fig:segment}
\end{center}
\end{figure}

Typiquement, un nom complet peut être attribué au locuteur courant lorsque celui-ci s'annonce ou se présente. Par exemple les journalistes ont pour habitude de finir leurs interventions en rappelant leur identité, comme par exemple \emph{\og c'était Paul Dupond en direct de \ldots \fg}. Ici, le nom complet \emph{Paul Dupond} devrait être attribué au locuteur qui a prononcé cette phrase (le locuteur courant). A l'inverse, un locuteur prononçant cette phrase \emph{\og Nous écoutons maintenant, Jean Durand.\fg{}} annonce une personne qui va parler après lui. Le nom complet \emph{Jean Durand} devra donc être attribué au locuteur suivant. Pour finir, une phrase du type \emph{\og Merci Maude Bayeu, nous passons maintenant à la météo \fg} rappelle le locuteur qui vient de s'exprimer juste avant. Le nom complet \emph{Maude Bayeu} sera donc attribué au locuteur précédent.

Un des problèmes que les systèmes d'INL ont a résoudre est d'attribuer un nom complet détecté au tour de parole correspondant (précédent, courant ou suivant) ou, à défaut, de ne pas réaliser d'attribution si le nom complet ne correspond pas au locuteur d'un des tours de parole courant ou contigu. C'est par exemple le cas des noms complets qui ne font pas partie du document (\emph{Jean-Jacques Rousseau} aura par exemple peu de chance d'être un des locuteurs du document) ou qui font référence à des locuteurs présents plus loin dans le fichier. En effet, même si un nom complet détecté correspond bien à un des locuteurs du document, il ne sera pas exploité par ces techniques s'il ne correspond pas à un des locuteurs des tours de parole contigus ou courant. Ce cas est appelé \og autre \fg sur la figure \ref{fig:segment}.

Ce type d'attribution sera par la suite appelée \og attribution locale \fg. En effet, on attribue ici un nom complet à un des tours de parole contigu au tour de parole où le nom complet a été détecté. Aucune attribution au niveau du fichier entier n'est pour l'instant effectué, seuls les tours de paroles contigus sont concernés. La propagation de ces attributions locales au sein du fichier entier est une étape appelée \og attribution globale \fg.

\subsection{Attributions globales}

\label{attributions_globales}

Afin de bien comprendre la problématique de l'attribution globale des noms complets, il faut placer les attributions locales dans le contexte d'une transcription enrichie (cf. \ref{sec:trans_enrichie}).

En effet, comme décrit dans la partie précédente, chaque nom complet détecté dans un tour de parole est attribué ou tour de parole courant, précédent ou suivant. Chacun de ces tours de parole appartient à une classe de locuteur dite anonyme, que l'INL chercher à nommer en lui attribuant un et un seul nom complet. Or, plusieurs noms complets peuvent être candidats pour la même classe de locuteur. Comme le montre la figure \ref{fig:decisions_locales_conflit}, il est tout à fait possible que plusieurs noms complets différents soient en conflit au sein d'un même tour de parole. Dans l'exemple, deux noms complets sont possibles pour le tour de parole du locuteur appartenant à classe numéro 2 (LOCUTEUR 2) : Maude Bayeu et Pierre Moscovici.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/decision_multiple.pdf}
\caption{Exemple de décisions locales conflictuelles}
\label{fig:decisions_locales_conflit}
\end{center}
\end{figure}

Chaque tour de parole appartenant à une même classe de locuteur, les conflits des tours de parole vont se répercuter au sein de la classe de locuteur. En effet, plusieurs noms complets peuvent être candidats pour la même classe de locuteur, il faudra alors choisir quel nom complet attribuer à cette classe. La figure \ref{fig:processus_entier} montre un exemple du processus global, avec tous les conflits que cela engendre.


Dans ce processus, à l'étape 1, la transcription est enrichie avec une segmentation et une classification en locuteurs ainsi qu'avec une détection des entités nommées. Chacune des entités nommées de type \emph{PERSONNE} est ensuite attribuée localement au tour de parole correspondant dans l'étape 2. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/entire_process_fr.pdf}
\caption{Vue globale du processus attribution d'un nom complet à une classe de locuteur anonyme}
\label{fig:processus_entier}
\end{center}
\end{figure}

Par exemple, le nom complet \emph{C} est attribué aux tours de parole 5 et 11, qui correspondent respectivement à deux classes de locuteur différentes (classe du locuteur 3 et classe du locuteur 1). Un nom complet ne pouvant être attribué qu'à une classe de locuteur, \emph{C} se retrouve donc en conflit entre la classe du locuteur 3 et la classe du locuteur 1. Le nom \emph{B} quant à lui se retrouve aussi en conflit, puisqu'il est attribué aux tours de parole 1 et 8 qui appartiennent respectivement à la classe du locuteur 1 et à la classe du locuteur 2.

À l'issue de cette seconde étape, plusieurs noms complets sont donc possibles pour une même classe de locuteur : la classe du locuteur 1 peut être nommée avec \emph{A},\emph{B} ou \emph{C}, la classe du locuteur 2 avec \emph{A} ou \emph{B} et la classe du locuteur 3 avec \emph{C}. En revanche les classes de locuteur 4 et 5 n'ont pas de candidats possibles. Les méthodes d'INL utilisent un processus de décision (lors de l'étape 3) permettant de régler les conflits entre ces attributions globales afin de nommer chaque classe avec un et un seul nom complet.

Une fois les conflits de la phase d'attribution globale résolus, la dernière étape consiste à répartir au sein du fichier les différentes décisions. 


Même si les façons de réaliser les différentes étapes différent entre les méthodes d'identification nommée, les principes qui viennent d'être décrits sont communs à toutes. Le traitement d'un fichier audio avec un système d'identification nommée se résume donc aux étapes qui sont décrites dans la figure \ref{fig:fonctionnement_general}.

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/fonctionnement_general.pdf}
\caption{Aperçu global d'un processus d'identification nommée du locuteur}
\label{fig:fonctionnement_general}
\end{center}
\end{figure}


\section{Approche symbolique}
\label{sub:regles_manuelles}

Deux principaux types approches ont été étudiées pour l'attribution locale des noms complets, à savoir une approche de type symbolique avec l'utilisation de règles écrites à la main, et plusieurs approches à base de méthodes statistiques. Nous décrivons ici la première approche utilisée dans la littérature, à savoir l'approche symbolique. Les méthodes à base de systèmes statistiques sont décrites dans la suite de cette thèse. 

\subsection{Règles linguistiques}

 Les travaux réalisés par le LIMSI \cite{CansecoRodriguez2005,CansecoRodriguez2006} ont été les premiers à montrer que le couple prénom/patronyme d'un locuteur apparaissant dans un contexte lexical donné permettait d'identifier de manière précise l'identité des locuteurs s'exprimant dans les tours de parole contigus. Ces travaux sont à la base de toutes les techniques d'INL, puisque ce sont eux qui ont défini le principe de l'attribution locale décrit ci-dessus. 
 
Ces travaux s'appuient sur l'utilisation de règles linguistiques pour réaliser l'attribution locale des noms complets. L'idée est d'utiliser le contexte linguistique de chaque nom complet pour lui attribuer une des trois étiquettes (\og locuteur précédent \fg, \og locuteur courant \fg ,\og locuteur suivant \fg). Ces règles ont été définies manuellement après analyse d'un corpus de langue anglaise (\emph{Hub-4e}). Ce corpus est un corpus qui a été réalisé manuellement et qui concerne des émissions de radio et de télévisions américaines (ABC, CNN, CSPAN et NPR) enregistrées entre 1993 et 1998. Comme décrit dans \ref{ssec:presence_noms_complets}, ce type de corpus se prête particulièrement à l'utilisation de méthodes d'INL basées sur la transcription.

Dans le but de réaliser des règles les plus génériques possibles, certains mots du contexte linguistique ont été remplacés par des mots plus génériques appelés concepts. Ces concepts vont permettre de regrouper au sein d'une même graphie plusieurs mots différents. Par exemple, tous les noms lieux sont remplacés par le concept [location]. De ce fait, les deux règles \og reporting from Bagdad \fg et \og reporting from Paris \fg peuvent être scindées en une seule règle \og reporting from [location] \fg. Plusieurs dictionnaires de concepts sont utilisés pour remplacer les mots. Les dictionnaires utilisés sont au nombre de 6 :


\begin{itemize}
	\item \textbf{noms complets} (prénom et patronyme) de locuteurs ([name]) : ce dictionnaire contient les noms anglais les plus communs, y compris les personnes célèbres et les personnalités internationales. Ces noms ont été récupérés via des sites internet et complétés par les identités des locuteurs présents dans les transcriptions de référence (tout corpus confondu).
	\item \textbf{toponymes} [(location]) : ce dictionnaire est constitué des lieux géographiques les plus courants comme les villes, les régions, les états, les monuments... Ils ont été récupérés à partir de journaux en ligne. La plus part se situent aux États-Unis.
	\item \textbf{noms d'émissions} ([show]) : ce dictionnaire contient le nom des émissions et des radios. Les informations ont été extraites des transcriptions de référence (tous corpus confondus).
	\item \textbf{professions} ([title]) : ce dictionnaire est composé des titres (Monsieur, Madame, ...), des rangs militaires et des professions. Ils ont été récupérés à partir de sites internet.
	\item \textbf{mots gérant la communication} : c'est un dictionnaire global incluant une variété de termes utilisés pour gérer la communication. Il contient une sélection des termes les plus fréquemment utilisés pour les remerciements ([thanks]), les salutations ([greet]), les consentements ([agree]), les questions ([quest]), les réflexions personnelles ([reflect]) et les pronoms démonstratifs ([dem]). Cet ensemble de concept a été récupéré via internet.
  \item \textbf{conjugaisons} : ce dictionnaire contient la liste complète des verbes anglais. Ils sont conjugués et classés comme verbes à l'infinitif ([infverb]), au passé ([pasverb]), au présent ([preverb]) ou au participe ([parverb]). Cette liste a été récupérée via internet.
\end{itemize}

À partir du corpus de développement de 150 heures d'enregistrements de radio et de télévision en langue anglaise (corpus Hub-4e), les règles les plus utiles pour déterminer l'identité des locuteurs ont été extraites. Elles sont décrites dans le tableau \ref{table:canseco_regles}. Au final, douze règles sont utilisées pour désigner le locuteur courant, 34 pour le suivant et 6 pour le précédent.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      3162 & [title] [name] \\
      848 & I am [name] \\
      673 & [show]'s [name] \\
      382 & [agree] [name] \\
      293 & [name] [show] [location] \\
      186 & [show]'s [name] reports \\
      176 & [thanks] [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Règles les plus utiles pour déterminer l'identité d'un locuteur sur le corpus de développement}
  \label{table:canseco_regles}    
\end{table}

En plus de ces règles, d'autres règles comprenant un caractère joker (*) ont été utilisées. Ce caractère va permettre de remplacer n'importe quel mot ou suite de mots, de manière à généraliser encore un peu plus les règles. Des exemples de règles avec joker sont données dans le tableau \ref{table:canseco_joker}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      458 & with [comm] us * [name] \\
      109 & joining * [name] \\
      108 & [name] * joins \\
      45 & with * [comm] me \\
      24 & [comm-agreement] * [name] reporting \\
      24 & we are joined * by [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Exemple de règles utilisant des joker (*) sur le corpus de développement}
  \label{table:canseco_joker}    
\end{table}

\subsection{Évaluation}

L'évaluation a été réalisée sur un corpus de test de 10 heures de données en langue anglaise (corpus Hub-4e, années 1997, 1998 et 1999). Deux types de transcriptions enrichies ont été utilisées : transcriptions manuelles et automatiques. Le taux d'erreur mot pour les transcriptions automatiques est de 18\%, le système utilisé est celui présenté dans \cite{Lamel02lightlysupervised}. 

Les erreurs réalisées par le système de transcription automatique, et notamment les erreurs de découpage en tours de paroles (produits par le système de segmentation et de classification en locuteur) posent problème pour l'évaluation des règles. En effet, lorsqu'un tour de parole produit par le système automatique correspond à plusieurs tours de paroles la référence, comment considérer la validité de la règle ? Doit-elle être considérée comme bonne, fausse ? Pour répondre à ce problème, deux types de tours de parole ont été pris en compte dans les transcriptions réalisées automatiquement. À savoir les tours de paroles \og purs \fg, c'est à dire les tours de paroles qui correspondent aux tours de parole de la référence et les tours de paroles \og impurs \fg, c'est à dire les tours de paroles qui regroupent en plusieurs tours de parole de la référence (à cause d'une erreur du système de segmentation et de classification). Afin de calculer les erreurs commises par le système, différents cas de figure ont été pris en compte~:

\begin{itemize}
  \item \textbf{C1} : l'identité extraite de la transcription est associée avec un tour de parole pur et correspond exactement à l'identité indiquée dans la référence (prénom et patronyme).
  \item \textbf{C2} : l'identité extraite de la transcription est associée avec un tour de parole impur et correspond exactement à une des identités indiquées dans la référence (prénom et patronyme).
  \item \textbf{C3} : l'identité extraite de la transcription est associée avec un tour de parole pur et correspond partiellement à l'identité indiquée dans la référence.
  \item \textbf{C4} : l'identité extraite de la transcription est associée avec un tour de parole impur et correspond partiellement à une des identités indiquées dans la référence.
  \item \textbf{Undef} : la règle correspond a un locuteur inconnu dans la référence (ces cas sont exclus des résultats de test)
  \item \textbf{False id} : Aucun des cas ci-dessus ne s'applique, c'est donc une erreur d'association avec un tour de parole.
\end{itemize}

\subsection{Résultats}

Le tableau \ref{tab:results_canseco} résume les résultats sur les transcriptions manuelles et automatiques. Sur le corpus d'évaluation avec les transcriptions manuelles, les règles identifiant le locuteur courant sont les plus fiables (aucune fausse identification), tandis que les règles identifiant le locuteur précédent (43,1\% de fausse identification) et suivant (20,2\% de fausse identification) présentent des taux d'erreur plus élevés. Cette tendance ce confirme avec l'utilisation de transcription automatiques, les règles de type \og courant \fg représentent 8,0\% des erreurs, les règles de type \og suivant \fg 19,0\% des erreurs et les règles de type \og précédent \fg 50,0\% des erreurs. Le taux d'erreur global des règles sur le corpus de test avec les transcriptions manuelles est d'environ 13\%, contre environ 18\% pour les transcriptions automatiques. 


\begin{table}[h]
  \begin{center}

    \small
    \begin{tabularx}{\linewidth}{cccc|ccc}
      %\cline{3-6}
      \hline
      \hline
      & \multicolumn{3}{c|}{Transcriptions manuelles}  & \multicolumn{3}{c}{Transcriptions automatiques} \\ 
      & \emph{courant} & \emph{suivant} & \emph{précédent} & \emph{courant} & \emph{suivant} & \emph{précédent} \\ \hline 
      \emph{c1} & 115 (95,0\%) & 50 (55,0\%) & 7 (16,0\%) & 94 (84,0\%) & 38 (60,3\%) & 8 (21,0\%) \\ 
      \emph{c2} & - & - & - & 2 (1,7\%) & 3 (4,8\%) & - \\
      \emph{c3} & 7 (5,0\%) & 22 (24,8\%) & 18 (40,9\%) & 7 (6,2\%) & 10 (15,9\%) & 11 (29,0\%) \\
      \emph{c4} & - & - & - & - & - & - \\
      \emph{\#False id} & - & 16 (20,2\%) & 19 (43,1\%) & 9 (8,0\%) & 12 (19,0\%) & 19 (50,0\%) \\
      \emph{\#undef.} & - & 3 & 1 & - & 2 & 1 \\ \hline
      \emph{Total} & 122 & 91 & 45 & 112 & 65 & 39 \\
      \hline
      \hline

      \hline
    \end{tabularx}
  \end{center}
  \caption{Taux d'erreur des règles manuelles sur les transcriptions manuelles et automatiques (corpus d'évaluation 97-98-99 Hub-4e)}
  \label{tab:results_canseco}
\end{table}



Ces travaux démontrent qu'identifier les locuteurs d'un document à partir de sa transcription est possible. En revanche, ils nécessitent un traitement manuel du corpus~: les règles et les dictionnaires de concepts sont réalisés par un humain. Le temps de mise en place de telles règles peut être long suivant la quantité de corpus à analyser et demande une expertise du domaine pour pouvoir être réalisée. De plus, le passage d'un corpus à un autre est fastidieux~: il faut réécrire le jeu de règles pour l'utiliser sur des documents d'une autre langue ou provenant d'autres types d'émissions.

De plus, ces travaux ne traitent pas de l'attribution globale des noms complets mais uniquement de l'utilisation des règles locales. Ce problème d'attribution globale est abordé dans les différentes approches statistiques décrites ci-dessous.


\section{Approche statistique : N-grammes}

\label{sub:regles_statistiques}


Suite aux travaux de Canseco (2005), le laboratoire de l'Université de Cambridge s'est intéressé à l'identification nommée du locuteur \cite{Tranter06}. Les travaux de Cambridge consistent à automatiser l'apprentissage des règles linguistiques et à mesurer l'impact de leur système d'identification nommée sur des données provenant de systèmes automatiques (classification en locuteur et transcription automatiques).

\subsection{Utilisation de N-grammes}
\label{Tranter}
\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/n_gram_tranter.pdf}
\caption{Exemple de n-grammes}
\label{fig:tranter_n_gram}
\end{center}
\end{figure}

À la différence des précédents travaux, Cambridge utilise des N-grammes pour modéliser les règles linguistiques (cf. figure \ref{fig:tranter_n_gram}). Comme décrit dans \ref{ssection:modele_langage}, les N-grammes sont des outils très utilisés en modélisation du langage, ceux utilisés par Cambridge vont de 2 à 5 mots. Ils sont obtenus via un corpus d'apprentissage dans lequel la liste des personnes (leurs noms complets) intervenant dans les enregistrements est connue à l'avance. Les occurrences de ces noms complets sont détectées grâce à cette liste et les N-grammes sont appris en utilisant le contexte du nom complet détecté (une fenêtre de cinq mots incluant le nom complet est utilisée). Chaque N-gramme apparaissant plus d'un certain nombre de fois (cinq dans le cas de ces travaux) est considéré comme une règle de prédiction et est donc retenu comme règle linguistique.

Toutes les règles précédemment apprises sont utilisées sur le corpus de test. Lorsqu'une règle avec une probabilité $p_1$ est déclenchée et suggère un nom $n_1$ pour une classe de locuteur $s_a$, le score pour l'hypothèse $s_a = n_1$ est incrémenté.
Lorsque deux règles sont déclenchées pour le même nom de locuteur, leurs probabilités sont combinées en utilisant la formule :
\begin{equation}\mathbf{}
  p_{1+2}=1 - (1-p_1)(1-p_2)
\label{eq:sl}
\end{equation}

Comme dans les précédents travaux de Canseco (2005), Tranter (2006) utilise des concepts pour généraliser les règles. Ces concepts sont issus des données d'apprentissage et ont été complétés avec d'autres sources (journaux écrits notamment). Ces concepts sont les suivants : GOODBYE, HELLO, OKAY, THANKS, LOCATION, PERSON, PERSON'S, SHOW, SHOW'S and TITLE. Ils regroupent ceux utilisés par Canseco, la concept [agree] devient OKAY, [greet] devient THANKS, ... En revanche là où Tranter différencie HELLO et GOODBYE, Canseco les regroupait au sein d'un même concept [comm]. De plus, Tranter n'utilise pas le concept [quest] et les verbes.

\subsection{Corpus}

Le corpus d'apprentissage utilisé contient environ 70 heures d'enregistrements de journaux d'informations en langue anglaise provenant du corpus Hub-4 de 1996/1997 (corpus utilisé en partie par Canseco). Ce corpus a été transcrit manuellement et les identités de chacun des locuteurs ont été renseignées (quand cela était possible). De plus, les erreurs de transcription correspondant à des locuteurs du document ont été corrigées. Le corpus de développement correspond à celui utilisé pour la campagne RT-04 \cite{NIST2004} et se compose de 12 documents enregistrés en février 2001 et de novembre à décembre 2003. Le corpus d'évaluation correspond aussi à celui de la campagne RT-04, soit 12 émissions enregistrées en décembre 2003.

\subsection{Performance}

Afin de tester le système mis en place, ces travaux examinent les locuteurs de la référence pour évaluer la difficulté de la tâche. En effet, 3 cas différents pour ces locuteurs sont possibles :

\begin{itemize}
  \item \textbf{D}isponible~: le nom complet du locuteur de la référence est connu et est présent à l'identique dans la transcription de l'émission.
  \item \textbf{I}ndisponible~: le nom complet du locuteur de la référence est connu mais n'est pas présent à l'identique dans la transcription de l'émission. Ceci peut être du à plusieurs choses~: des personnes dont les noms sont cités dans d'autres émissions, l'utilisation de synonymes demandant une connaissance du contexte (le président = Bill Clinton) ou encore l'utilisation de tournures demandant un traitement supplémentaire (John Smith et sa femme Judy = Judy Smith).
  \item \textbf{N}on-nommé~: le nom complet du locuteur de la référence n'est pas connu.
\end{itemize}

À la différence de Canseco, ces travaux ne prennent pas en compte les noms partiels, c'est à dire les cas où uniquement le prénom ou le nom correspond à l'identité de la référence. Le nom complet associé par le système d'INL à une classe de locuteur doit correspondre exactement au nom renseigné dans la transcription. Le tableau \ref{tab:results_tranter_perf} présente les résultats sur les différents corpus. 

\begin{table}[h]
  \begin{center}

    \small
    \begin{tabular}{|c||c|c|c||c|c|}
      \hline
      Données & D & I & N & Temps total (h) & Nombre d'occurrences \\ \hline
      Apprentissage & 79,3\% & 4,0\% & 16,8\% & 148,2\% & 6912 \\
      Développement & 78,2\% & 8,8\% & 13,0\% & 4,3\% & 195 \\
      Évaluation & 76,8\% & 12,9\% & 10,3\% & 4,2\% & 206 \\ \hline
      Évaluation (srap) & 47,3\% & 42,5\% & 10,3\% & 4,2\% & 138 \\
      \hline

      \hline
    \end{tabular}
  \end{center}
  \caption{\% de données Disponibles (D), Indisponibles (I) et Non-nommées (N). Cela fixe la limite haute des performances atteignables, puisque les noms complets I ne peuvent être récupérés à partir de la transcription. Les résultats en utilisant les transcriptions automatiques (srap) sont aussi donnés.}
  \label{tab:results_tranter_perf}
\end{table}

La quantité de noms complets disponibles à partir de la transcription reste aux alentours de 78\%. L'utilisation de transcriptions automatiques multiplie par 3 les noms complets non disponibles dans la transcription. Cela est du aux nombreuses erreurs de transcription des noms complets. 

\subsection{Métrique}

Dans ces travaux plusieurs cas possibles sont étudiés lors de la comparaison entre la référence et les hypothèses fournies par le système d'INL :

\begin{itemize}
  \item Correct ($C$)~: le système propose une identité correspondant à celle indiquée dans la référence ;
  \item Substitution ($S$)~: le système propose une identité différente de l'identité présente dans la référence ;
  \item Insertion ($I$)~: le système propose une identité alors que le locuteur n'est pas identifié dans la référence ;
  \item Suppression ($D$) : le système ne propose pas d'identité alors que le locuteur est identifié dans la référence ;
  \item Inconnu ($U$)~: le système ne propose pas d'identité et la référence ne contient pas d'identité.
\end{itemize}

Une mesure de Précision et de Rappel peut être définie à partir des 5 cas d'erreur~:
\begin{equation}
	\label{eq:PR}
	P = \frac{C}{C+S+I} \ \ ; \ \ R = \frac{C}{C+S+D}
\end{equation}

%Dans tous les précédents articles \cite{Tranter06,Esteve07,Chengyuan07} traitant de l'identification nommée du locuteur, les résultats sont présentés sous la forme de mesures de précision et de rappel.

Ces valeurs peuvent être complétées par un taux d'erreur $Err$ global. Ce taux s'inspire du calcul du $WER$ utilisé pour l'évaluation de la transcription. Il a l'avantage de mesurer la qualité des résultats du système d'identification nommée en une seule valeur, facilitant les comparaisons entre les systèmes par rapport aux mesures de précision et de rappel.
\begin{equation}
	\label{eq:PR}
	Err = \frac{S+I+D}{S+I+D+C+U} \ \ ;
\end{equation}

À noter que ces métriques sont utilisés pour mesurer les performances en terme de durée.

\subsection{Résultats}

La figure \ref{fig:resultats_tranter} donne les différents résultats en fonction du type de transcription utilisée (\emph{srap} : transcription automatique, \emph{aseg} : segmentation et classification automatiques) et de l'usage des concepts ou non. 
Ces résultats montrent que l'utilisation de catégories pour généraliser les mots présents dans les règles apporte un gain significatif, que ce soit sur des transcriptions manuelles ou des transcriptions automatiques. Sur des transcriptions réalisées manuellement et sur le corpus de développement, à 95 \% de précision le rappel est de 60 \%. Sur les données d'évaluation, le rappel chute à 38 \% pour la même précision. Le tableau \ref{table:resultats_tranter} donne le rappel maximum obtenu sur chaque type de transcription (automatiques ou manuelles), ainsi que le rappel à 95\% de précision.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Transcription & Segmentation & Rappel maximum & Rappel à 95 \% de Précision \\
      \hline
      ref & ref & 64 \% & 38 \% \\
      auto & ref & 44 \% & 38 \% \\
      auto & auto & 38 \% & 26 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Résumé des résultats sur le corpus de test. Sont présentés le meilleur rappel obtenu et le rappel à 95 \% de précision}
  \label{table:resultats_tranter}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/resultats_tranter.pdf}
\caption{Résultats sur le corpus d'évaluation avec ou sans (mots) l'utilisation de concepts. Les résultats sont donnés pour les transcriptions de référence (ref), les transcriptions utilisant un SRAP (srap) pour obtenir les mots et les transcriptions utilisant un système de segmentation automatique pour les tours de parole et les locuteurs (aseg) }
\label{fig:resultats_tranter}
\end{center}
\end{figure}

Le taux d'erreur (DER : Diarization Error Rate) du système de segmentation/classification utilisé est de 6,9 \%. Le taux d'erreur du système de transcription (WER : Word Error Rate) est quant à lui de 12,6 \%. L'utilisation de transcriptions automatiques n'affecte pas le rappel à 95 \% de précision mais le rappel maximum possible (en faisant baisser la précision) passe de 64 \% à 44 \%. L'utilisation de transcriptions et de segmentations automatiques fait chuter le rappel à 26 \% (pour 95 \% de précision) et fait aussi chuter le rappel maximum possible à 34 \%.
À noter que tous les résultats sont exprimés en terme du durée : plus les locuteurs s'exprimant de manière conséquente sont détectés, plus les résultats sont élevés : le nombre de locuteurs correctement nommés n'est pas pris en compte. Voir la section \ref{sec:metrique} pour plus de détails sur les différentes métriques d'évaluation.


La méthode à base de N-grammes permet d'automatiser l'apprentissage des règles que Canseco avait définies à la main. Une propagation relativement simple des scores est effectuée au sein des classes de locuteurs : les différents scores obtenus pour un locuteur $n_1$ et une classe de locuteur $s_a$ sont cumulés. Les tests réalisés montrent de bonnes performances sur des données manuelles, mais de beaucoup moins bonnes sur les données automatiques alors que les systèmes automatiques de transcription et de segmentation sont très performants.
L'étiquetage des noms complets est réalisé grâce à une liste de locuteurs et l'étiquetage des autres catégories est réalisé à partir des données de l'apprentissage étiquetées manuellement. Afin de se rapprocher d'une utilisation "réelle" de l'identification nommée il est possible d'utiliser un détecteur d'entités nommées pour automatiser cet étiquetage. De plus, la propagation des scores au niveau des classes de locuteur et les problèmes concernant l'attribution d'un seul nom complet à une classe de locuteur soulevés dans \ref{attributions_globales} n'ont été étudiés que succintement. En , une solution rudimentaire a été mise en place, qui consiste simplement à additionner les différents scores.




\section{Approche statistique : modèle d'entropie maximum}

Suite aux travaux du LIUM et de Cambridge, Microsoft s'est appuyé sur le modèle N-gramme et a ajouté des informations supplémentaires pour améliorer l'identification nommée du locuteur \cite{Chengyuan07}.

Ces travaux ont principalement ajouté des informations acoustiques pour améliorer l'identification nommée à base de N-grammes. Ces informations sont de deux sortes : 
\begin{itemize}
	\item l'utilisation de modèles acoustiques des voix des locuteurs cibles (lorsqu'il sont disponibles). Des modèles sont appris sur les échantillons de voix des locuteurs cibles. Ils seront ensuite utilisés pour essayer d'identifier ce locuteur acoustiquement pendant le test.
	\item l'utilisation du genre du locuteur. Lors de l'attribution d'un nom complet à un locuteur, une vérification est effectuée : le genre du prénom devrait correspondre au genre détecté par l'acoustique.
\end{itemize}
La position du nom complet dans l'enregistrement est aussi prise en compte. Posons $S_f$ le premier segment de la classe de locuteur anonyme $C$, $S_l$ le dernier segment et $n_C$ un nom complet étiqueté comme devant appartenir à la classe $C$. Plusieurs cas de figure sont étudiés, les plus pertinents sont : 
\begin{itemize}
	\item $n_l$ apparait avant $S_f$,
	\item $n_l$ apparait dans $S_f$,
	\item $n_l$ apparait avant $S_l$,
	\item $n_l$ apparait dans $S_l$.
\end{itemize}

Afin de prendre en compte le fait que plusieurs N-grammes peuvent correspondre à un même contexte lexical autour du nom complet détecté, Tranter utilise une combinaison heuristique présentée dans la formule \ref{eq:sl}. Ces travaux proposent de remplacer cette heuristique par un modèle d'entropie maximum \cite{BergerEntropie96}. Cette approche permet de représenter les différentes règles au sein d'un modèle statistique et d'y ajouter des connaissances supplémentaires.

Pour déterminer la probabilité d'attribuer un nom complet $s$ à une classe de locuteur $c$, un ensemble de règles (N-grammes) applicables $\mathcal{R}(s)$ est tout d'abord déterminé. La combinaison heuristique de Tranter peut être réécrite de cette manière :

\begin{equation}\mathbf{}
  P(s|c) \propto 1 - \prod_{r\in \mathcal{R}(s)}(1-p_r)
\label{eq:sl_new}
\end{equation}

Le modèle d'entropie maximum proposé s'exprime de la manière suivante :

\begin{equation}\mathbf{}
  P(s|c)=\frac{1}{Z_\lambda(c)} \exp(\sum_{k=1}^{F} \lambda_k f_k(s,c))
\label{eq:maxent}
\end{equation}

Les N-grammes sont utilisés comme attributs (\emph{features}) $f_k(s,c)$ du modèle d'entropie maximum. $\lambda_k$ représente les paramètres du modèle (multiplicateur de Lagrange) et $Z_\lambda(c)$ est une constante de normalisation qui permet de s'assurer que la somme des probabilités est égale à un. Les paramètres du modèle sont appris en utilisant la méthode dite de \emph{Generalized Iterative Scaling} afin d'optimiser la probabilité \emph{a posteriori} des paramètres $\lambda$ étant donné les données d'apprentissage avec une distribution Gaussienne \emph{a priori} \cite{Chen2000}.
\todo{S'assurer que la précédente phrase se traduit bien comme ça}

Les règles linguistiques et les concepts sont obtenus de la même manière que dans \ref{Tranter} : un modèle N-gramme est utilisé pour apprendre les règles et un dictionnaire de concept pour remplacer certains mots.

Le corpus utilisé correspond à celui utilisé par Tranter, avec un découpage différent. Le corpus Hub4-1997 (environ 83 heures) a été utilisé pour l'apprentissage tandis que le corpus Hub4-1996 (environ 85 heures) a été utilisé pour le test. Seules les transcriptions manuelles de ces corpus ont été utilisées. Comme dans les précédents travaux, une classe de locuteur ne peut être nommée que si le nom complet du locuteur apparaît dans la transcription. Sur le corpus de test, ceci implique un rappel maximal de 83\% (contre environ 78\% pour Tranter, cf tableau \ref{tab:results_tranter_perf}). À noter qu'à la différence de Tranter, les erreurs de transcription concernant les noms complets des locuteurs du document ne sont pas corrigées.

Les résultats présentés dans la figure \ref{fig:resultats_microsoft} montrent qu'en utilisant les mêmes informations, le modèle d'entropie maximum est plus performant que la simple combinaison heuristique utilisée par Tranter. En revanche, l'ajout des modèles de locuteur n'apporte aucun gain significatif. L'utilisation d'informations sur les genres et la position des noms complets, donnent les meilleurs résultats.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/resultats_microsoft.pdf}
\caption{Résultats sur le corpus d'évaluation. Le modèle d'entropie maximum (maxent) et le modèle N-gramme utilisent les mêmes règles lexicales. Les résultats avec l'ajout de la position (pos), des données acoustiques (ac) et l'utilisation du genre (g) sont aussi donnés pour le modèle d'entropie maximum.}
\label{fig:resultats_microsoft}
\end{center}
\end{figure}

Ces travaux sont une extension des travaux menés par Cambridge dans \cite{Tranter06}. Ils améliorent les performances obtenues par Cambridge grâce au modèle d'entropie maximum et l'utilisation d'informations supplémentaires comme la position des locuteurs dans l'enregistrement ou leur genre. En revanche, aucune expérience sur des données automatiques n'a été conduite. De plus, les entités nommées et les concepts ont été étiquetés manuellement.

\section{Bilan}

Les travaux sur l'identification nommée du locuteur utilisant la transcription enrichie du signal sont relativement récents puisque les premiers travaux menés par Canseco datent de 2004. Ils partagent tous la même hypothèse de base, à savoir qu'il est possible de déterminer, pour un nom de locuteur présent dans la transcription, s'il se rapporte au locuteur qui parle actuellement, à celui qui va parler juste après, ou à celui qui a parlé juste avant.

Deux principaux systèmes statistiques ont suivis les travaux de Canseco, à savoir celui à base de N-grammes de l'université de Cambridge et celui à base d'arbre de classification sémantique (SCT) de l'université du Mans. Les travaux de \cite{Esteve07} ont démontré la supériorité du système à base de SCT par rapport au système à base de N-grammes sur des transcriptions automatiques. Ces systèmes ne seront utilisables que sur des transcriptions automatiques, en effet l'intérêt de nommer automatiquement des locuteurs sur des transcriptions manuelles est nul. Il faudra donc s'intéresser plus particulièrement au système à base de SCT par la suite. Par rapport à ces systèmes précurseurs, Microsoft a ajouté la prise en compte du genre du locuteur et la prise en compte de la position du nom complet détecté dans l'enregistrement. Ces informations améliorant les performances, il faudra les prendre en compte.

Les travaux du LIUM ont été les seuls à utiliser un système automatique de détection des entités nommées. Dans le but d'automatiser le plus possible le processus, l'utilisation d'un tel système est indispensable. De plus, l'affectation des étiquettes \og précédent \fg{}, \og courant \fg{} et \og suivant \fg{} a beaucoup été étudiée dans la littérature. En revanche, le processus de propagation de ces étiquettes au niveau des classes de locuteur et la résolution des conflits n'a pas été étudiée jusqu'ici. Pour finir, beaucoup de travaux ont été conduits sur des transcriptions partiellement ou totalement manuel ; un travail sur des transcriptions entièrement automatiques est à réaliser.


\chapter{Système automatique d'INL basé sur l'utilisation d'un arbre de classification sémantique}
\minitoc
\newpage

\label{sec:sys_id_lium}

\todo{Présenter un graphe du système avec les différents modules à la fin de ce chapitre pour dire qu'on a tout réécrit et que maintenant ça s'architecture comme ça. Ca sera dans le coup plus facile de dire quelles briques on remplace dans le nouveau système et les futures XP}

Cette thèse fait suite aux travaux déjà menés au Laboratoire d'Informatique de l'Université du Maine (LIUM) sur l'INL \cite{MauclairOdyssey06} et \cite{Esteve07}. Le LIUM s'est appuyé sur les travaux de Canseco (cf. section \ref{sub:regles_manuelles}) pour mettre en place un système d'INL basé sur un arbre de classification sémantique pour les attributions locales \cite{Kuhn1995} et sur un détecteur automatique d'entités nommées pour identifier les noms complets dans la transcription. L'utilisation de ces deux systèmes automatiques et d'un processus de décision permet au système du LIUM de fonctionner de manière entièrement automatique pour identifier les locuteurs d'un document.

Nous commençons par décrire le système de détection des entités nommées, puis nous décrirons ensuite l'arbre de classification sémantique utilisé pour réaliser les attributions locales. Nous finirons par expliquer le processus d'attribution globale du LIUM. Outre le module de détection d'entités nommées, les autres modules ont fait l'objet d'une réécriture complète pour cette thèse.

%Le laboratoire d'informatique de l'université du Maine (LIUM) a présenté ses travaux sur l'identification nommée du locuteur dans \cite{MauclairOdyssey06} et \cite{Esteve07}.
%Les travaux du LIUM \cite{MauclairOdyssey06} ont été publiés en même temps que ceux de l'Université de Cambridge. Ils font aussi suite aux premiers travaux de Canseco. Le LIUM utilise un arbre de classification sémantique (SCT - Semantic Classification Tree \cite{Kuhn1995}) pour déterminer les règles linguistiques à utiliser pour attribuer l'étiquette \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} à un nom complet détecté dans la transcription. Ces règles linguistiques sont basées sur des expressions régulières construites autour des noms complets détectés. Ce système ayant servi de base aux travaux présentés ici, il est plus longuement présenté dans la section qui lui est consacrée \label{sec:sys_id_lium}.

\section{Détection des entités nommées}

Le système de détection des entités nommées (EN) a été mis en place afin de participer à une partie expérimentale de la campagne d'évaluation ESTER I \cite{Galliano05} sur la détection des EN. C'est un système à base de règles. Quelques règles ont été inférées à partir du corpus d'apprentissage d'ESTER I et d'autres ont été développées manuellement (pour, par exemple, réaliser une grammaire pour détecter les dates). De plus, des listes de prénoms, de patronymes, de villes, de pays, etc. ont été ajoutées à la base de connaissances du système.

La liste des EN choisie pour la campagne ESTER I se compose de 8 catégories principales qui sont : les personnes, les lieux, les organisations, les groupes socio-politiques, les montants, les informations temporelles, les produits et les aménagements. Pour son système d'INL, le LIUM n'a retenu que 5 de ces catégories : les personnes, les lieux, les organisations les groupes socio-politiques et les informations temporelles \cite{Esteve07}. Les résultats sur le corpus de développement de la campagne ESTER I sont présentés dans le tableau \ref{table:resultats_ne_lium} en terme de précision et de rappel. La tâche de détection des entités nommées n'était qu'expérimentale dans ESTER I, il n'y a donc pas eu d'évaluation finale.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      Type d'EN & Rappel (\%) & Précision (\%) \\
      \hline
      \hline
      personne & 98,7 \% & 92,6 \% \\
      \hline
      lieu & 83,9 \% & 87,9 \% \\
      \hline
      organisation & 86,1 \% & 84,5 \% \\
      \hline
      groupe socio-politique & 84,0 \% & 91,8 \% \\
      \hline
      information temporelle & 96,7 \% & 87,7 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Performances du système de détection des EN du LIUM sur le corpus de développement ESTER I}
  \label{table:resultats_ne_lium}
\end{table}

Afin de généraliser la transcription dans le cadre de l'INL, lorsqu'une EN est détectée, la suite de mots correspondant est remplacée par le type de l'EN dans la transcription. Les traitements effectués sur la transcription sont décrits dans la figure \ref{fig:pre_traitement}.

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/pre_traitement.pdf}
\caption{Pré-traitement su signal audio}
\label{fig:pre_traitement}
\end{center}
\end{figure}


\section{Attributions locales : arbre de classification sémantique}

Les arbres de classification sémantiques (SCT - Semantic Classification Tree) sont souvent utilisés dans le traitement du langage naturel. Par exemple, ils sont employés pour des systèmes de dialogue \cite{Kuhn1995}, pour l'estimation de modèles de langage hiérarchiques n-grammes \cite{Esteve2001}, ou pour la détection de noms propres inconnus \cite{Bechet2000}. Les SCTs sont des arbres de décision binaire s'appuyant sur des patrons linguistiques comme paramètre de décision pour classer les échantillons. Dans le cadre de l'identification nommée, ils sont utilisés pour attribuer les étiquettes \og{}précédent\fg{}, \og{}courant\fg{} et \og{}suivante\fg{} décrites dans la section \ref{ssec:attributions_locales}.

\subsection{Arbre de décision binaire}

L'attribution locale des étiquettes est la base de tout système d'INL. Là où les autres travaux utilisent des règles réalisées manuellement (cf. section \ref{sub:regles_manuelles}) ou des N-grammes (\ref{sub:regles_statistiques}), le LIUM utilise un type particulier d'arbre binaire, à savoir un arbre de classification sémantique.

Un arbre de décision binaire \cite{Breiman84,Cornejols02} est un type de classificateur qui va permettre d'affecter des classes à un échantillon grâce à une suite de décisions binaires de type oui/non. Les n\oe{}uds de l'arbre sont des tests auquel l'échantillon sera soumis, en fonction du résultat du test (oui ou non) l'échantillon sera ensuite testé sur la branche correspondante. Si cette branche ne contient qu'une feuille, alors elle correspond aux classes à attribuer. Si cette branche est un sous-arbre contenant d'autres tests, ils sont effectués jusqu'à atteindre une feuille de l'arbre.
Prenons l'exemple d'un arbre de décision binaire permettant de classer les phrases selon le fait qu'elles parlent ou non de l'identification nommée. Les deux classes possibles sont \emph{INL} et \emph{NOTINL}. Si une phrase contient les termes \og{}identification\fg{} et \og{}nommée\fg{} elle sera alors considérée comme appartenant à la classe \emph{INL}, sinon elle sera classée comme \emph{NOTINL}. L'arbre présenté dans la figure \ref{fig:arbre_binaire} modélise ce cas de figure.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/arbre_binaire.pdf}
\caption{Exemple d'arbre de décision binaire permettant de classer une phrase X}
\label{fig:arbre_binaire}
\end{center}
\end{figure}

%le nom détecté parmi les étiquettes previous, current, next et other. Les SCTs reposent sur l'utilisation d'expressions régulières. Afin d'être analysés par le SCT, des couples sont créés comprenant une occurrence de nom complet ainsi que son contexte lexical. Le but est de classer ces couples avec les 4 étiquettes previous, current, next et other (voir les feuilles de la figure 7.2) en utilisant les expressions régulières.

\subsection{Apprentissage}

Dans les SCTs, les n\oe{}uds de l'arbre sont des expressions régulières construites lors de la phase d'apprentissage. Pendant le processus de construction du SCT, chaque n\oe{}ud est associé à une expression régulière contenant des mots et des caractères spéciaux (\emph{<}, \emph{>} et \emph{+}). le < (respectivement >) se rapporte au commencement (respectivement la fin) d'une intervention d'un locuteur tandis que + se rapporte à n'importe quelle suite de mots. Par exemple, l'expression régulière \emph{< + de + >} correspond à chaque intervention contenant le mot \emph{de}, alors que \emph{< + en direct + de + >} correspond à chaque intervention contenant les mots \emph{en direct} et \emph{de} apparaissant dans cet ordre. La figure \ref{fig:sct} montre une petite partie d'un tel arbre de classification avec un exemple de probabilités attribuées à chaque étiquette.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/sct.pdf}
\caption{Exemple d'arbre de classification sémantique}
\label{fig:sct}
\end{center}
\end{figure}

Le processus de construction du SCT doit choisir pour chaque n\oe{}ud l'expression régulière qui réduit au minimum un critère d'impureté. Ici le critère utilisé est le critère de Gini (se référer à \cite{Breiman84} pour de plus amples détails). Un autre critère communément utilisé est l'entropie de Shannon.

Pour chaque niveau dans l'arbre, ce processus de construction ajoute un mot à l'expression régulière courante. Le critère d'impureté permet d'évaluer le degré de déterminisme associé à un n\oe{}ud : plus le critère d'impureté est bas, plus la classification est déterministe.

Le fonctionnement et l'apprentissage des SCTs sont décrits de manière exhaustive dans \cite{KuhnPHD}.


\subsection{Algorithme}

Une fois l'arbre construit, il est utilisé sur le contexte lexical de chaque entité nommée de type \og{}personnes\fg{} détectée. Ce contexte lexical est constitué des 5 mots à gauche et à droite de l'entité nommée. L'algorithme utilisé est décrit ci-après.

\subsection{Attributions locales}

\section{Attributions globales}
\section{Corpus et données}




%\section{Arbres de classification sémantique (SCT) et décision locales}
%\label{sec:sct}
%
%Comme l'Université de Cambridge, le LIUM a généralisé l'approche de Canseco en automatisant l'apprentissage des règles linguistiques (grâce à l'arbre de classification sémantique) et en automatisant le processus de décision consistant à attribuer un nom complet à une classe de locuteur anonyme.
%L'arbre de classification utilisé est un arbre de décision binaire construit à partir du corpus d'apprentissage. Chacun des noeuds de l'arbre représente une expression régulière. Lorsque le contexte lexical (vingt mots à gauche et vingt à droite) d'un nom complet correspond à une des expressions régulières de l'arbre, une probabilité que ce soit le locuteur \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} lui est attribué, en fonctions des données apprises sur le corpus d'apprentissage. La décision \og locale \fg{} ( \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} ) avec le plus grand score est reportée au sein de la classe de locuteur anonyme correspondant. Lorsque le même nom complet est attribué plusieurs fois à une classe de locuteur, les probabilités de l'arbre de classification sont cumulées pour obtenir son score. Finalement, pour chaque classe de locuteur, le nom complet avec le plus grand score est attribué. À noter que l'arbre de classification permet de prendre en compte des informations plus \og globales \fg{} que le contexte lexical seul. Le LIUM utilise ainsi la position du nom complet dans le tour de parole (au début de celui-ci, à la fin celui-ci ou dans un tour de parole très court) comme information supplémentaire lors de l'apprentissage et du test de l'arbre.
%
%Ils ont réimplémenté le système à base de N-grammes de \cite{Tranter06} et l'ont comparé à l'approche basée sur les SCT. Les évaluations ont été conduites sur le corpus d'évaluation de la campagne ESTER 1 avec une liste fermée de locuteurs provenant des corpus d'apprentissage, de développement et de test. Cette liste contient 1007 locuteurs. Les résultats ont montrés que le système à base de N-gramme et celui à base de SCT se comportait de manière similaire sur des transcriptions réalisées entièrement manuellement. En revanche, sur des transcriptions automatiques (en gardant la segmentation et la classification de référence), le système à base de SCT s'est avéré beaucoup plus robuste.

%\section{Système de décision global}
%
%Ces travaux sont les premiers à utiliser un système automatique de détection des entités nommées. Ils automatisent complètement le processus d'identification nommée, de l'apprentissage au test : toutes les étapes sont automatiques.
%En revanche, le système de décision est relativement sommaire : seule l'étiquette avec le score maximal est pris en compte, les probabilités des autres étiquettes pour un nom complet sont tout simplement ignorées. De plus la prise de décision au niveau de la classe de locuteur se réduit à attribuer le nom de locuteur ayant un score maximum : l'incertitude au sein d'une même classe (plusieurs locuteurs possibles avec des scores élevés) n'est pas prise en compte.
%
\section{Expériences et résultats}
\subsection{Évaluation et métriques}

\label{sec:metrique}

Un système d'identification nommée est évalué en comparant l'hypothèse générée par celui-ci à la référence distribuée avec le corpus. Cette comparaison met en évidence 5 cas d'erreur ou de succès possibles relatifs aux situations suivantes~:
\begin{itemize}
\item l'identité proposée est correcte ($C_1$)~: le système propose une identité correspondant à celle indiquée dans la référence ;
\item erreur de substitution ($S$)~: le système propose une identité différente de l'identité présente dans la référence ;
\item erreur de suppression ($D$) : le système ne propose pas d'identité alors que le locuteur est identifié dans la référence ;
\item erreur d'insertion ($I$)~: le système propose une identité alors que le locuteur n'est pas identifié dans la référence ;
\item il n'y a pas d'identité ($C_2$)~: le système ne propose pas d'identité et la référence ne contient pas d'identité.
\end{itemize}


Une mesure de Précision et de Rappel peut être définie à partir des 5 cas d'erreur~:
\begin{equation}
	\label{eq:PR}
	P = \frac{C_1}{C_1+S+I} \ \ ; \ \ R = \frac{C_1}{C_1+S+D}
\end{equation}

%Dans tous les précédents articles \cite{Tranter06,Esteve07,Chengyuan07} traitant de l'identification nommée du locuteur, les résultats sont présentés sous la forme de mesures de précision et de rappel.

Comme il a été proposé dans \cite{Tranter06}, ces valeurs peuvent être complétées par un taux d'erreur $Err$ global également calculé à partir de ces 5 erreurs. Ce taux s'inspire du calcul du WER utilisé pour l'évaluation de la transcription. Il a l'avantage de mesurer la qualité des résultats du système d'identification nommée en une seule valeur, facilitant les comparaisons entre les systèmes par rapport aux mesures de précision et de rappel.
\begin{equation}
	\label{eq:PR}
	Err = \frac{S+I+D}{S+I+D+C_2+C_1} \ \ ;
\end{equation}


Les erreurs peuvent être calculées en terme de durée ou en terme de nombre de locuteurs.
Pour une évaluation en durée, dans le cas où un locuteur parlant 90\% du temps est correctement nommé et que les six autres locuteurs parlant seulement 10\% du temps ne le sont pas, le système présentera un taux d'erreur de 10\%. 

Pour une évaluation en terme de nombre de locuteurs, dans le même cas de figure, le système aura un taux d'erreur de 87,5\%.

D'un point de vue applicatif, la métrique exprimée en durée est préférable si les locuteurs considérés comme importants correspondent aux locuteurs s'exprimant beaucoup. En revanche, si l'application cherche à nommer le plus possible de locuteurs, il est plus intéressant d'évaluer les performances en terme de nombre de locuteurs.


\subsection{Légende}

\textit{\textbf{Trans.}: Transcription \textbf{M}anuelle ou \textbf{A}utomatique.\newline
\textbf{Seg/Class.}: segmentation/classification manuelles ou automatiques.\newline
\textbf{R, P}: rappel et précision calculés en en durée.\newline
\textbf{ErrDur}: Taux d'erreur en durée.\newline
\textbf{ErrLoc} : Taux d'erreur en nombre de locuteurs.\newline}

\subsection{Résultats avec liste de locuteurs globale}

Le tableau \ref{tab:results_liumni} présente les résultats du système sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles ou totalement automatiques. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs d'ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste des locuteurs (le genre des locuteurs étant extrait des transcriptions de référence).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 87,36\% & 95,69\% & 11,67\% & 14,11\% \\ \hline
\textbf{A} & \textbf{A} & 29,31\% & 59,98\% & 64,49\% & -\% \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 85,99\% & 95,72\% & 12,88\% & 18,26\% \\ \hline
\textbf{A} & \textbf{A} & 29,48\% & 69,42\% & 64,46\% & -\% \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs}
\label{tab:results_liumni}
\end{table}

\subsection{Résultats avec liste de locuteurs par show}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 88,67\% & 97,31\% & 10,45\% & 12,45\% \\ \hline
\textbf{A} & \textbf{A} & 28,80\% & 69,65\% & 64,91\% & -\% \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 86,85\% & 96,56\% & 12,10\% & 17,01\% \\ \hline
\textbf{A} & \textbf{A} & 28,83\% & 73,77\% & 64,84\% & -\% \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs du show}
\label{tab:results_liumni_liste_show}
\end{table}


\subsection{Impact de la prise en compte du genre}

Le tableau \ref{tab:results_liumni_sansgenre} présente les mêmes résultats que le tableau \ref{tab:results_liumni} sans utiliser l'information sur le genre des locuteurs. La liste des locuteurs d'ESTER 1 (tous corpus confondus) est utilisée, mais sans prendre en compte le genre du locuteur.

Lorsque le genre n'est pas utilisé pour affiner les décisions, les performances du système se dégradent, que ce soit sur les transcriptions automatiques ou manuelles. Sur les transcriptions manuelles, le taux d'erreur augmente d'environ 3\% en absolu. Sur les transcriptions automatiques, le taux d'erreur augmente très peu sur les corpus de développement, mais augmente de 4\% sur le corpus de test. Le genre des locuteurs étant détecté automatiquement, il est donc soumis à des erreurs \todo{donner le taux d'erreur}. Mais même si cette détection comporte des erreurs, l'utiliser améliore quand même les performances.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 84,30\% & 90,60\% & 14,55\% & 17,84\% \\ \hline
\textbf{A} & \textbf{A} & 28,56\% & 54,08\% & 65,65\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 84,12\% & 93.40\% & 14,68\% & 20,33\% \\ \hline
\textbf{A} & \textbf{A} & 24,61\% & 58,82\% & 68,78\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs, sans prise en compte du genre}
\label{tab:results_liumni_sansgenre}
\end{table}


\subsection{Système ouvert sans liste de locuteurs et de leur genre}

\ref{tab:results_liumni_sansliste} \ref{tab:results_liumni_genre_prenom}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 67,23\% & 93,12\% & 30,04\% & 28,63\% \\ \hline
\textbf{A} & \textbf{A} & 20,26\% & 76,84\% & 72,73\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 71,46\% & 95,03\% & 26,11\% & 27,80\% \\ \hline
\textbf{A} & \textbf{A} & 20,60\% & 64,03\% & 72,32\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système de base avec une transcription enrichie manuelle ou automatique sans liste de locuteurs et sans genre}
\label{tab:results_liumni_sansliste}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 70,97\% & 97,89\% & 26,55\% & 22,82\% \\ \hline
\textbf{A} & \textbf{A} & 20,91\% & 80,64\% & 72,10\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 74,58\% & 97,85\% & 23,23\% & 24,90\% \\ \hline
\textbf{A} & \textbf{A} & 23,77\% & 74,53\% & 69,40\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique sans liste de locuteurs, et avec prise en compte du genre par le prénom}
\label{tab:results_liumni_genre_prenom}
\end{table}

\section{Bilan}
      Les travaux ont beaucoup porté sur la manière d'étiqueter les suivant/précédent/courant/autre => Un papier montre que les SCT > N-grammes. Aller plus loin : améliorer le processus de décision qui vient après ces décisions locales
      Utiliser une analyse conjointe du signal sonore et du texte pour améliorer les décisions "aveugles" (renforcer les décisions avec le genre des locuteurs)

Les noms complets des locuteurs sont présents dans la transcription, et ils est possible de les affecter à tour de parole contigu

\chapter{LIUM\_NI : Un système d'INL par analyse conjointe du signal et de sa transcription}
\minitoc
\newpage

\section{Analyse des erreurs}


Toutes les mauvaises décisions prises par le système ont été étiquetées manuellement et comparées avec les différents scores générés par l'arbre. Sont comptées comme erreurs toutes les décisions non prises (erreur de suppression), ou toutes les décisions prises à tort (erreur de substitution). Cette étude a permis de mettre en évidence deux grandes catégories d'erreurs~: les erreurs de détection d'entités nommées et les erreurs de décision de l'arbre. D'autres types d'erreurs ont aussi été constatés mais elles sont moins significatives. 
	
%Dans les 18 fichiers analysés correspondant au test ESTER (10 heures d'enregistrement de journaux d'informations), 86 erreurs ont été détectées. Ces erreurs peuvent être de deux types, soit un mauvais nom de locuteur attribué, soit aucun nom attribué alors que c'était possible.

\subsection{Formalisme}

Dans l'étude qui suit, un même formalisme sera utilisé dans chacun des exemples. Les entités nommées détectées comme telles par Nemesis seront entourées de crochets, les anthroponymes seront en plus mis en italique (ex : \textit{[Jacques Chirac]}). ``(...)'' signifiera que la phrase a été coupée pour plus de lisibilité. Les exemples seront de ce type :\\\\
\texttt{
LOCU1 (Jacques Chirac) : (...) fin d'un tour de parole\\
LOCU2 (inconnu) : [Entité nommée] fin de phrase\\
LOCU2 (inconnu) : \textit{[Anthroponyme]}\\
LOCU3 (Bernard Thomasson): début du tour de parole suivant (...)\\\\
}

Chaque tour de parole sera identifié par une étiquette anonyme \texttt{LOCUX} suivie entre parenthèse de la vraie identité du locuteur tirée de la transcription de référence. Il pourra y avoir plusieurs segments à l'intérieur du même tour de parole, c'est le cas du \texttt{LOCU2} ici.

\subsection{Entités nommées}

Ce qui est ici appelé ``erreur'' est relatif à ce qu'attend l'arbre. \'Etiqueter \textit{[président Jacques Chirac]} comme anthroponyme est correct du point de vue linguistique et n'est pas une erreur commise par Nemesis, mais n'est pas conforme à ce qui est attendu par le système d'identification nommée. En effet, le système d'identification nommée ne marche qu'avec les couples noms~/prénoms et n'acceptera pas \textit{[président Jacques Chirac]}. Il faut donc bien garder à l'esprit que ce qui est appelé ici erreur, est ce qui met le système d'identification nommée en échec.\\

%\subsection{Anaphores et autres erreurs}

%Dans quelques cas, même un humain n'aurait pas été capable d'identifier la personne qui parle à partir du texte. En effet, il arrive que l'information ne soit pas du tout présente dans le texte. C'est le cas pour Bernard Thomasson dans 20041008\_1800\_1830\_INFO\_DGA, il n'est jamais cité dans la transcription. Dans les enregistrements de la Radio RTM, la journaliste Samira lamrani n'est jamais appelée par son nom/prénom. Seul son prénom est utilisé. Même si le système arrive à attribuer Samira au locuteur, le système attend uniquement des nom/prénoms, le locuteur est donc mis en inconnu.

%\subsection{Nemesis}


\textbf{\'Etiquetage erroné}\\

Nemesis rencontre des problèmes pour l'étiquetage des radios type France Inter et France Info. En effet, France étant un prénom, il étiquette France Inter et France Info comme étant des anthroponymes. L'ajout de France Info et de France Inter dans ses lexiques de médias ne change rien. La règle qui est déclenchée prioritairement par Nemesis est donc celle qui considère France comme un prénom. Par exemple :\\\\
\texttt{
LOCU1 (Frank Noblesse) : (...) sans pouvoir baisser sa vitesse\\
LOCU2 (inconnu) : \textit{[France Info]} actualités\\
LOCU2 (inconnu) : \textit{[Franck Noblesse]}\\
LOCU1 (\textbf{Frank Noblesse}) : dix huit heures quatre l' otage (...)\\\\
}

Dans cet exemple, \texttt{France Info} et \texttt{Franck Noblesse} sont étiquetés tous les deux comme anthroponymes.\texttt{ France Info} est évalué comme étant \texttt{LOCU1} (le locuteur précédent le tour de parole où il a été détecté) à 42\%. \texttt{Franck Noblesse} comme étant aussi \texttt{LOCU1} (cette fois ci le suivant) à 46\%. Un peu plus loin dans le fichier \texttt{France Info} est encore attribué à \texttt{LOCU1} :\\\\
\texttt{
LOCU3 (Nicolas Hénin) : (...) très hostile au sud de [Bagdad]\\
LOCU3 (Nicolas Hénin) : [Amman] \textit{[Nicolas Hénin]} \textit{[France Info]}\\
LOCU1 (Frank Noblesse) : \textit{[Christian Chesnot]} et \textit{[Georges Malbrunot]} otages en (...)\\\\
}

Ici \texttt{France Info} est encore détecté comme étant \texttt{LOCU1} (le locuteur suivant) à 92\%. Lorsque le système prend une décision, il se rend compte que \texttt{LOCU1} a été attribué deux fois à \texttt{France Info} avec un total de 1,34 points (0,42 + 0,92) alors qu'il n'a été attribué qu'une fois à \texttt{Franck Noblesse} avec un total de 0,46 points. Il choisit ainsi d'attribuer \texttt{France Info} comme identité pour LOCU1 au lieu de Franck Noblesse.


Lors de l'évaluation, le système ne prend pas en compte l'identité attribuée au locuteur puisque \texttt{France Info} n'est pas connu comme locuteur correct dans la transcription de référence, ceci fait donc chuter le rappel. C'est le seul cas d'étiquetage erroné posant vraiment problème dans le corpus étudié.\\

\textbf{\'Etiquetage partiel}\\

Dans quelques rares cas (3 sur les 86 erreurs), Nemesis n'étiquette que partiellement les identités de locuteur. Ceci peu être du à un prénom étranger comme \texttt{Anissa} (\texttt{Anissa \textit{[El Jabri]}} partiellement étiqueté) ou à prénom peu commun comme \texttt{Eymard} (\texttt{Eymard du \textit{[Chatenet]}} partiellement étiqueté). Un autre cas a été relevé, \texttt{\textit{[Frédéric]} Rivière} est en effet mal étiqueté puisque seul son prénom l'est.\\

%En revanche, il est aussi assez fréquent (15 occurrences sur les 18 fichiers) que Némésis étiquette mal le nom de locuteur, soit parce que c'est un nom étranger, soit parce qu'il lui arrive d'étiqueter la fonction de la personne. Quelques exemples ci dessous.
%CJ pourquoi ci-dessus fais tu une transition vers la partie suivante ? En lisant, on est un peu déstabilisé, on se demande où on est.
%CJ avant toute chose, il y a ambiguité sur le mot nom. On parle de nom du locuteur qui est en fait son prénom et son nom (de famille). Pourquoi ne pas appeler ce nom de famille le patronyme. ce serait plus clair.
%CJ Je ne comprends pas => Nemesis étiquette mal le nom du locuteur parce que c'est un patronyme étranger. Pourquoi ce n'est pas dans la catégorie "etiquetage erroné" ?
%CJ Ce n'est pas parce que Nemesis inclus le rôle de la personne dans l'entité nommée, qu'il commet une erreur d'étiquetage (voir explication thèse Fourour chap 4 p 84 de pourquoi il prend "large")
%VJ Oui je sais, c'est ce que j'essaie d'expliquer en dessous, je vais faire en sorte d'être plus clair.
\textbf{\'Etiquetage inadéquat}\\

Certaines étiquettes sont correctes du point de vue linguistique mais ne correspondent pas à ce qui est attendu par le système d'identification.
 Par exemple, \texttt{\textit{[président du Fetia Api]}} est bien un anthroponyme et est détecté comme tel par Nemesis. En revanche, lors de l'évaluation, \texttt{\textit{[président du Fetia Api]}} ne sera pas connu de la référence (dans laquelle il n'y a que des couples prénoms~/patronymes) et sera donc considéré comme inconnu. Ce genre d'erreur provient donc d'une inéquation entre les sorties de Nemesis et les entrées du système d'identification nommée.


Nemesis essaye tant que possible d'étendre l'entité nommée au maximum, ce qui donne des entités nommées comme celle-là : \texttt{\textit{[ministre de l'intérieur Jean Pierre Chevènement]}}. C'est en effet un anthroponyme et une entité nommée valide, mais qui ne correspond pas aux attentes du système d'identification nommée.

Un dernier cas d'étiquetage inadéquat relève quant à lui d'un problème lié à Nemesis. Ainsi, il est difficile pour Nemesis de délimiter le contexte droit d'une entité nommée, c'est à dire savoir où elle s'arrête, quelles autres entités nommées inclure, etc. Dans le cas d'entités nommées mixtes ou le problème de sur-composition se pose (entité nommée contenant une autre entité nommée), Nemesis choisi de ne ``retenir que la forme la plus complète constituant une entité nommée'' (cf \cite{Fourour04}, 4.1.2). Deux problèmes de ce type ont été relevées dans notre corpus : \texttt{\textit{[Joël Collado de Météo France]}} et \texttt{\textit{[Emmanuel Butstraten de BASF Agro]}}. En effet, le système d'identification nommée n'est évalué qu'avec des couples prénoms/patronymes, la sur-composition lui pose donc problème.\\
%CJ tu peux être plus explicite sur le problème (thèse Fourour chap 2  paragraphe 2.1.2.1 p 28 et chap 4 paragraphe 4.1.2 p 85 (l'identification). Dans tes 2 exemples, c'est un problème de sur-composition pour le contexte droit.

%Un ensemble de mots ne contenant pas de nom et prénom est détecté comme une personne. Cette détection fait chuter le rappel en attribuant ce \textit{faux locuteur} à un tour de parole. Par exemple, Nemesis étiquette l'entité nommée ``[président du Fetia Api]'' comme personne qui est ensuite attribuée à un tour de parole. ``[président du Fetia Api]'' est bien une personne, mais cette entité nommée n'est pas un locuteur au sens de l'identification. Il est identifié par sa fonction au lieu de son nom et prénom.

\textbf{Utilisation de Némésis dans un mauvais contexte}\\

Les entrées qui sont données à Nemesis sont des transcriptions utilisées pour la reconnaissance automatique de la parole. Ces transcriptions sont allégées de toute ponctuation, ce pourquoi Nemesis n'a pas été prévu. Nemesis étant un outil tiré du TALN, il se base sur la ponctuation pour détecter les entités nommées. Utiliser Nemesis avec de telles transcriptions l'amène à accoler des entités nommées qui sont normalement séparées par une ponctuation. Voici quelques exemples d'entités nommées agglutinées qui auraient du être étiquetées séparément (la ponctuation manquante a été rajoutée entre parenthèses): \texttt{\textit{[Bernard Thomasson (.) Claude Thélot]}}, \texttt{\textit{[Friponil (.) Frank Aletru]}}, \texttt{\textit{[France Info (,) Tendance Junior (,) Aurélie Kieffer (.)]}}.


\subsection{Erreurs de l'arbre de décision}

\textbf{Décision incertaine}\\

Dans beaucoup de cas, l'arbre de classification n'est pas capable de décider si le locuteur est le suivant, le précédent ou le courant. Ces locuteurs se retrouvent donc étiquetés comme \textit{autre} alors que dans la majorité des cas ils parlent avant ou après le tour de parole où ils ont été étiquetés.

Ceci est du à un locuteur qui n'est pas clairement annoncé, ce qui ne permet pas à l'arbre de savoir si l'entité nommée concerne le locuteur suivant. C'est le cas dans l'exemple ci-dessous.\\\\
\texttt{
LOCU1 (Valérie Crova) : (...) qui a toujours entretenu selon \textit{[Jean Christophe Bouisson]} des rapports tendus avec le gouvernement.\\
LOCU2 (Jean Christophe Bouisson) : On ne peut pas penser (...)\\
}
\\Dans cet exemple ``[Jean Christophe Bouisson]'' est étiqueté comme désignant un \textit{autre} tour de parole alors qu'il se rapporte au \textit{suivant} (LOCU2).\\\\

Un des cas relevé plusieurs fois est celui ou la personne annoncée est suivie de la fonction quelle occupe.\\\\
\texttt{
LOCU1 (Yves Izard) : (...) \textit{[Marie George Buffet]} la secrétaire nationale du parti communiste français\\
LOCU2 (Marie George Buffet) : le Premier ministre pense d'après (...)\\
}\\
On se retrouve ici dans un cas presque identique au premier, à cette différence prêt que la fonction ``la secrétaire nationale du parti communiste français'' pourrait être catégorisée et donc réduite à un unique mot FONCTION par exemple. Un ajout dans ce sens lors de la détection d'entités nommées serait intéressant pour généraliser au plus possible les mots lors de l'apprentissage de l'arbre.\\

\textbf{Décision erronnée}\\

Les erreurs de substitution sont toujours dues à des locuteurs qui ont été étiquetés à tort comme suivant, précédent ou courant. Comme décrit dans \ref{metrique}, ces erreurs n'affectent donc que les performances en terme de rappel.

Un cas qui revient plusieurs fois est celui où un locuteur annonce le prochain interviewé et le prochain intervieweur dans la même phrase. Dans ce cas, le système accorde toujours plus d'importance au locuteur cité en dernier, qui n'est généralement pas le locuteur suivant, mais celui juste après (le journaliste généralement).\\\\

\texttt{
LOCU1 (Fabrice Drouelle) : écoutez (...) \textit{[pers Jean Michel Hibon]} au micro de \textit{[pers Jérôme Susini]}\\
LOCU2 (Jean-Michel Hibon) : mon logement de fonction (...)
}\\\\
Dans ce cas LOCU2 est étiqueté comme étant Jérôme Susini et non Jean-Michel Hibon. En effet, selon l'arbre la probabilité que Jérôme Susini désigne le tour de parole suivant est plus grande que celle concernant Jean Michel Hibon.\\\\

%\noindent \textbf{20041013\_1700\_1800\_INFO\_DGA} :\\
%Bernard\_Thomasson 1144.075 1149.122 \textbf{[pers Jean Jacques Vanier]} sera à partir de demain sur la scène de L' Européen à [gsp Paris] pour trois mois \\
%Bernard\_Thomasson 1149.122 1153.543 il présente son nouveau spectacle à part ça la vie est belle à \textbf{[pers Bernard Stéphane]}\\
%\textbf{Jean-Jacques\_Vanier} 1153.543 1159.549 c' est l' histoire de quatre amis qui ont décidé de garder leur amitié au delà de la vie et au delà de la mort quoi \\\\

%Ici c'est Bernard Stéphane qui est attribué à Jean-Jacques Vanier.\\
%Sur les 12 erreurs affectant la précision que j'ai pu détecter, 10 étaient de ce type. Généralement, lorsque c'est la personne interrogée et non le journaliste qui parle en premier, le système commet une erreur.

% \subsection{Mauvaise décision de l'arbre}
% 
% La performance en terme de rappel du système est principalement affectée par les personnes qui sont signalées comme n'étant ni avant ni après (elles sont étiquetées en autre). Sur les 58 non attributions, 16 sont dues à ce type de problème. On observe généralement le problème lorsque la personne n'est pas mentionnée juste avant de parler, mais une ou deux phrases avant. Voici quelques cas représentatifs :\\\\
% 
% \noindent \textbf{20041011\_1300\_1400\_INTER\_DGA} :\\
% 
% \noindent delphine simon 1127.884 1134.114 Vanessa Descoureaux vient de joindre \textbf{Marie Sophie Desaulle} la présidente de l' association des paralysés de de France\\
% delphine simon 1134.114 1135.522 on écoute sa réaction\\
% \textbf{marie sophie desaulle} 1135.522 1138.677 ben Christopher Reeve permettait effectivement que\\\\
% 
% \noindent \textbf{20041013\_1700\_1800\_INFO\_DGA} :\\
% 
% \noindent yves izard 725.272 729.093 \textbf{Marie George Buffet} la secrétaire nationale du parti communiste français
% \textbf{marie george buffet} 729.093 734.142 le Premier ministre pense d' après les informations ils sont en vie mais tout cela bien sûr reste fragile\\\\
% 
% Il arrive aussi à l'arbre d'étiqueter un locuteur comme étant le prochain, et non l'actuel (7 occurrences). Ce type d'erreur génère aussi de mauvaises attributions de nom, 2 cas sur les 7. Les deux erreurs d'attribution de nom :\\\\
% 
% 
% \noindent \textbf{20041027\_1230\_1300\_RFI\_ELDA} :\\
% 
% \noindent eduardo febro 481.682 484.570 \textbf{Eduardo Febro} Miami radio France internationale\\
% \textbf{françois bernard} 484.570 491.834 République démocratique du Congo les ministres des affaires étrangères du Rwanda\\
% 
% François Bernard est étiqueté comme étant Eduardo Febro. A noter qu'Eduardo febro ayant déjà été étiqueté correctement plus haut, on ne devrait pas pouvoir attribuer deux fois le même nom à deux locuteurs différents.\\
% 
% \noindent \textbf{20041222\_1300\_1320\_RTM\_ELDA} :\\
% 
% \noindent samir bour 777.168 780.378 \textbf{Samir Bour} depuis Tanger pour RTM Chaîne Internationale\\
% \textbf{aziza ziani} 783.356 785.518 et puis cette inf activité royale\\\\
% Aziza ziani est ici étiquetée comme étant samir bour.

\subsection{Autres erreurs}
D'autres erreurs plus indépendantes du système ont été relevées~: certaines personnes ne sont citées que partiellement (seul leur prénom est cité dans la transcription), d'autres ont été mal orthographiées par le transcripteur ou encore certaines ne sont pas du tout citées ou annoncées dans la transcription. Sur les 11 heures du corpus, ce dernier cas ne concerne que 3 personnes. Cette constatation permet de valider l'hypothèse de départ~: les noms des locuteurs sont présents dans la transcription.

\subsection{Répartition des erreurs}

Le tableau \ref{tab:repart_erreurs} montre la répartition et l'importance des différents types d'erreurs sur le corpus de test. Les erreurs provenant de l'arbre de classification (A1 et A2) sont clairement dominantes avec plus de 72\% des erreurs totales alors que les erreurs dues à Nemesis (N1, N2 et N3) ne représentent qu'un peu moins de 19\% des erreurs. Pour l'arbre de classification, les erreurs A1 et A2 ont le même ordre de grandeur (environ 36\% en moyenne). En revanche, en ce qui concerne Nemesis les erreurs de type N1 sont plus fréquentes que celles de type N2 et N3 (environ 2 fois plus). Les étiquetages de France Info et de France Inter comme anthroponymes ne sont pas comptabilisés dans ces résultats. Leur proportion (79 étiquettes) relativement importante aurait faussé la répartition des autres erreurs. Nous considérons en effet que ce souci lié à Nemesis est marginal et sera facilement corrigé.

{\footnotesize{
\begin{figure}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Erreurs} & \textbf{Fréquence} & \textbf{Pourcentage Total} \\ \hline
\hline
\textbf{(N1) \'Etiquetage partiel ou inadéquat} & 7 & 8,2\% \\ \hline
\textbf{(N2) \'Etiquetage erroné} & 4 & 4,6\% \\ \hline \hline
\textbf{(N3) \'Mauvais contexte} & 5 & 5,8\% \\ \hline \hline
%\textit{total} & \textit{16} & \textit{18,6\%} \\ \hline \hline
\textbf{(A1) Décision incertaine} & 32 & 37,2\% \\ \hline
\textbf{(A2) Décision erronée} & 30 & 34,9\% \\ \hline \hline 
%\textit{total} & \textit{62} & \textit{72,1\%} \\ \hline \hline
\textbf{(E) Autre} & 8 & 9,3\%\\ \hline \hline
\textbf{Total} & 86 & 100\% \\ \hline
\end{tabular}
\end{center}

\caption{Répartition des erreurs sur le corpus de test.\newline
%------------\newline
{\textit{N1, N2, N3~: erreurs issues de la détection d'entités nommées.\newline
A1, A2~: erreurs issues de l'arbre de classification.\newline
E~: autres erreurs.\newline}}
%------------
}
\label{tab:repart_erreurs}

\end{figure}
}}

% \subsection{Autres erreurs}
% 
% Dans deux des cas relevés, la transcription du nom est fausse. La personne ayant transcrit manuellement le nom s'est trompé dans l'orthographe, le système ne trouve donc pas de concordance entre les noms trouvés.\\\\
% 
% \noindent \textbf{20041027\_1230\_1300\_RFI\_ELDA} :\\
% 
% \noindent  françois bernard 340.634 341.727 \textbf{Patrick Gouy}\\
% \textbf{patrice gouy} 341.727 345.982 les deux candidats ont ouvert au Mexique où vit un million d' expatriés\\\\
% 
% Il arrive aussi (3 occurrences) que le locuteur ne soit pas annoncé mais soit juste mentionné quelque part dans le texte, souvent plus loin, en faisant référence à ce qu'il a dit. C'est par exemple le cas de Jean Pierre Raffarin dans 20041012\_1800\_1830\_INFO\_DGA.\\
% Il peut aussi arriver que les erreurs soient reportée globalement alors que l'on hésitait entre deux personnes. Dans 20041007\_0800\_0900\_INTER\_DGA, pour un locuteur nous avons un score de 0.47 pour dominique de villepin et de 0.46 pour alain ray, nous choisissons dominique de villepin (ce qui est faux) et nous reportons l'erreur sur tout le fichier.

\newpage
\subsection{Conclusion}

Nous avons présenté notre système d'identification de locuteur ayant comme principales caractéristiques de s'appuyer sur les données de transcription, d'effectuer la reconnaissance des entités nommées grâce à un outil issu du traitement automatique des langues et de réaliser l'apprentissage et la prise de décision à l'aide d'un arbre de classification sémantique. Nous avons d'abord analysé les résultats obtenus en terme de précision et de rappel et nous avons obtenu des résultats prometteurs. Nous avons ensuite mené une étude exhaustive concernant les erreurs commises par le système, erreurs qui se situent principalement au niveau du système de reconnaissance d'entités nommées Nemesis et de l'arbre de classification. 

Au niveau de Nemesis, il faudrait premièrement calibrer ses sorties pour les adapter aux entrées attendues de notre système d'identification. En effet, Nemesis a été conçu pour détecter des entités nommées simples et complexes (par exemple ``président de la république Jacques Chirac'' est une entité nommée étiquetée comme anthroponyme par Nemesis) et le système d'identification exploite des entités nommées sous une forme simple de type ``prénom/patronyme'', ce qui génère des erreurs.  Deuxièmement, l'utilisation de Nemesis peut aussi être optimisée par un ajout de ponctuation au niveau de la transcription qui lui permettrait de mieux détecter les entités nommées (notamment quand une entité se trouve à la fin d'un segment et l'autre au début du suivant, il étiquette les deux entités nommées ensemble).

%Au niveau de Nemesis, la prise en compte des fonctions des personnes et la modification de certaines règles de décision permettrait d'améliorer l'étiquetage. Mais aussi, un ajout de ponctuation au niveau de la transcription
% (virgule après chaque fin de segment) 
%lui permettrait de mieux détecter les entités nommées.
% (notamment quand une entité se trouve à la fin d'un segment et l'autre au début du suivant, il étiquette les deux entités nommées ensemble).
%SM c'est pas déjà fait !

Au niveau de l'arbre de classification, il serait intéressant de prendre en compte le numéro d'ordre du nom de locuteur détecté dans le tour de parole lors de l'apprentissage de l'arbre et du test. Ceci permettrait dans certains cas de faire abstraction des mots pouvant suivre le dernier nom de locuteur du tour de parole. Dans un souci d'augmenter les données d'apprentissage fournies à l'arbre, catégoriser ou lemmatiser le plus de mots possibles semblerait être une piste intéressante. La détection des fonctions des personnes par Nemesis irait dans ce sens.

Pour finir, comme c'est déjà le cas pour certains modèles de langage en reconnaissance de la parole, utiliser les informations fournies par un étiqueteur grammatical pour apprendre et tester l'arbre de classification sera à étudier.

\section{Meilleure détection des entités nommées}

\subsection{Némésis, un outil prévu pour le TAL}

\subsection{Némésis et LIA\_NE :  expériences et résultats}

\section{Système de décision}

\subsection{Problématique}

\subsection{Décisions locales : le classifieur}

\subsection{Décisions globales : Théorie des croyances}

\section{Applications et connaissances a priori}
liste de locuteurs, genre des locuteurs


\section{Segmentation et classification automatique}

Comme décrit dans \ref{ssec:ral}, la tâche de segmentation et de classification en locuteur consiste à annoter des régions du signal audio. Ces annotations peuvent être de plusieurs types : des étiquettes représentant les différents locuteurs, le genre des locuteurs, le type de canal (radio, studio) ou encore le type d'environnement sonore comme le bruit, la musique, \dots Dans la tâche de segmentation, les étiquettes représentant les locuteurs sont anonymes. Le système du LIUM est entièrement décrit dans \cite{liumspkdiarization}, ce qui suit en donne un court aperçu.

\subsection{Segmentation BIC}
Le système utilisé au LIUM (LIUM\_SpkDiarization) utilise une segmentation basée sur GLR (Generalized Likelihood Ratio) et BIC \cite{Siegler97automaticsegmentation} (Bayesian Information Criterion). Une première passe permet de détecter les points de rupture du signal constituant les frontières des segments. Ces points de rupture sont détectés en utilisant la mesure de vraisemblance GLR calculée (grâce à des GMM à matrice de covariance pleine) sur une fenêtre de 5 secondes tout le long du signal. Lorsque la mesure atteint un maximum dans cette fenêtre de 5 secondes, le système considère que c'est un point de rupture. Une seconde passe est ensuite effectuée afin de fusionner les segments correspondant au même locuteur, du début à la fin de l'enregistrement. La mesure utilisée pour le regroupement est la distance BIC utilisant des matrices de covariance pleines. 

\subsection{Classification BIC}

L'algorithme utilisé pour regrouper les segments au sein de classes se base sur une classification hiérarchique. Au départ, chaque classe est constituée d'un unique segment, chaque classe étant modélisée avec une GMM à matrice de covariance pleine. La mesure utilisée pour regrouper les classes entre elles, et pour stopper le regroupement est la mesure BIC. Les classes les plus proches sont regroupées tant que la distance BIC entre ces classes est inférieure à 0.

\subsection{Détection du genre et du canal}

La détection du genre est réalisée grâce à des GMM (avec une diagonale à 128 composantes) pour chacun des quatre combinaisons possibles entre le genre (homme/femme) et le canal (studio/téléphone). Chaque classe est étiquetée en fonction de la GMM qui maximise la vraisemblance avec la classe en question. Chaque modèle a été préalablement appris en utilisant le corpus d'apprentissage de la campagne ESTER.

\section{Expériences et résultats}



\subsection{Légende}

\textit{\textbf{Trans.}: Transcription \textbf{M}anuelle ou \textbf{A}utomatique.\newline
\textbf{Seg/Class.}: segmentation/classification manuelles ou automatiques.\newline
\textbf{R, P}: rappel et précision calculés en en durée.\newline
\textbf{ErrDur}: Taux d'erreur en durée.\newline
\textbf{ErrLoc} : Taux d'erreur en nombre de locuteurs.\newline}

\subsection{Corpus et données}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Corpus & Courant & Suivant & Précédent & Autre \\
      \hline
      Train & 3,9 & 16,6 & 19,2 & 32,5 \\
      Dev & 8,3 & 17,6 & 9,3 & 45,2 \\
      Test & 26,7 & 17,1 & 12.3 & 39,3 \\      
      \hline
    \end{tabular}
  \end{center}

  \caption{Taux d'erreur des étiquettes courantes / suivantes / précédentes des locuteurs faisant partie de la liste de locuteur fermée ESTER1}
  \label{table:etiquettes_sct}
\end{table}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Corpus & Courant & Suivant & Précédent & Autre \\
      \hline
      Train & 17,3 & 46,3 & 51,9 & 6,8 \\
      Dev & 42,1 & 46 & 40 & 13,2 \\
      Test & 52,1 & 42,4 & 42 & 10,7 \\      
      \hline
    \end{tabular}
  \end{center}

  \caption{Taux d'erreur des étiquettes courantes / suivantes / précédentes des locuteurs en liste ouverte}
  \label{table:etiquettes_sct}
\end{table}

% Résultats sans filtrage de liste de locuteurs sur du manuel (même pas sur le genre) & LIA_NE
%
% Train ESTER 1 (Train spkid)
% Next : 934/2019 = 46.2605250123824 %
% Previous : 188/362 = 51.9337016574586 %
% Current : 36/208 = 17.3076923076923 %
% Other : 666/9766 = 6.81957812819988 %

% Test ESTER 1 (Dev spkid)
%
% Next : 139/302 = 46.0264900662252 %
% Previous : 26/65 = 40 %
% Current : 8/19 = 42.1052631578947 %
% Other : 168/1274 = 13.1868131868132 %

% Test ESTER 2 sans africa (Test spkid)
%
% Next : 132/311 = 42.443729903537 %
% Previous : 37/88 = 42.0454545454545 %
% Current : 12/23 = 52.1739130434783 %
% Other : 132/1238 = 10.6623586429725 %



% Résultats avec filtrage via liste de locuteurs sur du manuel (mais pas avec la prise en compte du genre) & LIA_NE
%
% Train ESTER 1 (Train spkid)
%
% Next : 211/1273 = 16.5750196386489 %
% Previous : 41/213 = 19.2488262910798 %
% Current : 7/179 = 3.91061452513966 %
% Other : 647/1988 = 32.5452716297787 %


% TEST ESTER 1 (Dev spkid)
%
% Next : 34/193 = 17.6165803108808 %
% Previous : 4/43 = 9.30232558139535 %
% Current : 1/12 = 8.33333333333333 %
% Other : 161/356 = 45.2247191011236 % 

% Test ESTER 2 sans africa (Test spkid)
%
% Next : 36/211 = 17.0616113744076 %
% Previous : 7/57 = 12.280701754386 %
% Current : 4/15 = 26.6666666666667 %
% Other : 126/321 = 39.2523364485981 %

\subsection{Utilisation d'une liste de locuteurs globale et de leur genre}

Le tableau \ref{tab:results_fctcroyance} présente les résultats du système utilisant les fonctions de croyances. Les expériences sont menées sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles ou totalement automatiques. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs d'ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste des locuteurs (le genre des locuteurs étant extrait des transcriptions de référence).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 86,16\% & 94,38\% & 12,75\% & 15,35\% \\ \hline
\textbf{A} & \textbf{A} & 21,45\% & 49,96\% & 71,59\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 85,49\% & 95,01\% & 13,34\% & 18,67\% \\ \hline
\textbf{A} & \textbf{A} & 25,63\% & 61,20\% & 67,95\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs et la détection du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Utilisation d'une liste de locuteurs par show et de leur genre}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 87,01\% & 95,75\% & 11,95\% & 14,11\% \\ \hline
\textbf{A} & \textbf{A} & 20,95\% & 61,53\% & 72,01\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 86,35\% & 96,10\% & 12,56\% & 17,43\% \\ \hline
\textbf{A} & \textbf{A} & 24,97\% & 64,90\% & 68,33\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs spécifique au show et la détection du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Influence de la prise en compte du genre}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 80,19\% & 89,33\% & 18,32\% & 19,09\% \\ \hline
\textbf{A} & \textbf{A} & 20,70\% & 45,15\% & 72,81\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 82,24\% & 91,16\% & 16,45\% & 21,58\% \\ \hline
\textbf{A} & \textbf{A} & 24,61\% & 58,82\% & 68,78\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs et sans prise en compte du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Système ouvert sans liste de locuteurs et sans genre}

\ref{tab:results_fctcroyance_sansliste}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 65,74\% & 91,96\% & 31,63\% & 29,46\% \\ \hline
\textbf{A} & \textbf{A} & 19,53\% & 70,52\% & 73,39\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 69,85\% & 94,10\% & 27,79\% & 28,22\% \\ \hline
\textbf{A} & \textbf{A} & 20,59\% & 61,42\% & 72,31\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique sans liste de locuteurs et sans genre}
\label{tab:results_fctcroyance_sansliste}
\end{table}

\section{Limites de l'approche}
très dépendant du corpus pour l'automatique (car on mesure en durée), un ou deux locuteurs détectés en plus ou en moins peuvent beaucoup faire varier les résultats alors que ça ne représente qu'une infime partie du nombre des locuteurs à détecter



% \section{Commandes}
% 
% Un mot entre guillemets : \quotes{mot}.
% 
% \todo{écrire cette thèse}
% 
% \tocite{une référence}
% 
% \textit{et caetera} : a, b, c\etc
% 
% \section{Références natbib}
% 
% \citet{exemple1}
% 
% \citep{exemple1}
% 
% \citep[avant][après]{exemple1}
% 
% \citet{exemple2}
% 
% \citep{exemple2}
% 
% \citep[avant][après]{exemple2}
% 
% \section{Les maths}
% 
% \begin{align}
% x^2-y^2&=(x+y)\times(x-y)\label{equation:trivia}\\
% \max_{i<5}f(i)&=\argmin_{j<18} g(j)\nonumber
% \end{align}
% 
% Voilà l'équation~\ref{equation:trivia}.
% 
% \section{Figures et tables}
% 
% Les figures sont centrées, comme on peut le voir dans la figure~\ref{figure:logo_lia}.
% 
% \cfigurex{logos/logo_uapv}{Titre court pour la table des illustrations.}{\label{figure:logo_lia}Caption de la figure\etc c'est le logo du LIA.}{.5}
% 
% Les tables ont un trait en haut et en bas, comme le démontre la table~\ref{table:stats}.
% 
% \ctablexx{lr}{Titre court pour la liste des tables.}{Caption de la table\etc quelques statistiques.}{table:stats}{
% \textbf{Aliment} & \textbf{Quantité} \\
% \hline
% Chips & 198 \\
% Poulet & 1 \\
% }

\postdocument
\end{document}
