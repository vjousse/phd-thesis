\documentclass[12pt]{article}
\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{array}


% or use the graphicx package for more complicated commands
% \usepackage{graphicx}
\usepackage[pdftex]{graphicx}
% or use the epsfig package if you prefer to use the old commands


\author{Vincent JOUSSE}
\title{Identification nommée du locuteur : exploitation conjointe du signal sonore et de sa transcription}

\begin{document}
	\maketitle
  \tableofcontents
  \newpage
  \listoffigures
  \newpage
  \listoftables
  \newpage

\section{Introduction}


\newpage
\section{Systèmes automatiques : fondements}

Depuis les débuts de l'informatique, l'Homme a toujours essayé de l'utiliser pour automatiser des tâches humaines. Que ce soit de la simple calculatrice à la reconnaissance de la parole, l'informatique est de nos jours utilisée dans tous les domaines. Mais pour pouvoir réaliser ces tâches, les programmes informatiques doivent avoir une certaine connaissance des actions à effectuer pour pouvoir les réaliser.

Il existe deux principales façons d'insufler ces connaissances à un programme. La première est de tout simplement lui donner les règles qui vont lui permettre de réaliser les tâches qu'il a à accomplir. Dans l'exemple de la calculatrice, il suffit de lui fournir les différentes formules arithmétiques pour qu'elle puisse fonctionner. Ce type de procédé marche bien lorsqu'il existe des règles formelles pour réaliser une tâche. Mais ces règles ne sont pas toujours disponibles ou peuvent varier fortement en fonction du contexte, c'est par exemple le cas en reconnaissance de la parole.

La reconnaissance de la parole fait intervenir plusieurs domaines complexes à gérer pour un système automatique. Il faut tout d'abord transformer le signal audio en paramètres exploitables par un programme informatique. Il faut ensuite transformer ce signal audio en phonèmes, puis les phonèmes en mots et les mots en phrases. Ces étapes dépendent de beaucoup de paramètres comme les conditions d'enregistrement, la langue utilisée dans le document, l'accent des personnes qui parlent ou leur façon d'utiliser la langue (néologismes, mauvaises constructions grammaticales). En plus de variabilité de ces paramètres, il est impossible de définir des règles formelles pour encadrer chacun d'eux.

Prenons l'exemple d'un enregistrement dans lequel les locuteurs parlent français. Même s'il existe une grammaire et un orthographe bien définis pour la langue française écrite, ce n'est pas le cas à l'oral. En effet les contractions (cheval sera prononcé ch'val), les faux départs et les répétitions sont légions. En plus de ces différences entre le français écrit et le français parlé, les locuteurs peuvent très bien faire des fautes de grammaire ou employer des mots qui ne font pas parti du dictionnaire courant. Toutes ces variations dans l'utilisation de la langue française rendent difficile l'utilisation de règles formelles pour reconnaître les mots prononcés. C'est pour pallier à ces problèmes que les systèmes de reconnaissance automatique de la parole utilisent des techniques d'apprentissage automatique qui vont permettre au système d'apprendre automatiquement de nouvelles connaissances en fonction des données qui lui seront fournies.

\subsection{Apprentissage automatique}
\subsubsection{Généralités}

L'apprentissage automatique (qui est un sous-domaine de l'Intelligence Artificielle) a pour but de permettre à une machine, un programme, de s'adapter et d'évoluer grâce aux données qui lui sont fournies, de manière à remplir des tâches complexes qu'il serait difficile, voir impossible, de remplir avec des méthodes plus traditionnelles. Un des défis de l'apprentissage automatique est d'extraire les données pertinentes présentes dans les corpus d'apprentissage afin d'en tirer des généralités applicables à d'autres données. L'aprentissage automatique repose sur la notion de classification. La classification va viser à étiqueter chaque donnée en l'associant à une classe. Ces classes peuvent être connues à l'avance ou déterminées automatiquement par le système. C'est ce qui va différencier l'apprentissage supervisé de l'apprentissage non supervisé. 

\subsubsection{Apprentissage supervisé}

L'apprentissage supervisé va permettre d'apprendre des règles à partir de données d'apprentissages servant d'exemple. Ces données sont généralement validées et étiquetées par des êtres humains. C'est à partir de ces exemples déjà classés que le système va apprendre des règles essayant de reproduire la classification à partir des données. Par exemple, les arbres de décision sont des outils utilisant un apprentissage supervisé, puisqu'ils vont permettrent de ranger des données de test dans des classes prédéfinies. Ils auront été au préalablement appris grâce à des données d'apprentissage étiquetées au préalable avec les bonnes classes.

\subsubsection{Apprentissage non supervisé}

L'apprentissage non supervisé (aussi appelé classification automatique) consiste à trier un groupe hétérogène de données en regroupant les données homogènes au sein d'une même classe, et les données hétérogènes au sein d'autres classes. À la différence de l'apprentissage supervisé, il n'y a pas de \og sortie attendue \fg{}. C'est à dire que c'est au système d'établir ses propres classes, au lieu de devoir classer les données dans des classes prédéfinies comme pour l'apprentissage supervisé. En reconnaissance automatique de la parole, et plus précisément dans le domaine de la reconnaissance du locuteur, les modèles à mélange de gaussiennes sont un bon exemple de système d'apprentissage non supervisé.

\subsection{Reconnaissance automatique du locuteur}
\subsubsection{Identification automatique du locuteur}
\subsubsection{Vérification automatique du locuteur}
\subsubsection{Suivi de locuteur}
\subsubsection{Segmentation en locuteur}

\label{ssub:decoupage_regroupement}

Sauf cas particuliers, les enregistrements sonores sont généralement composés de plusieurs intervenants (ou locuteurs). Chaque prise de parole d'un locuteur est appelée \og tour de parole \fg. Ces tours de paroles commencent quand un locuteur prend la parole et s'arrêtent lorsqu'un autre locuteur prend la parole (ou lorsqu'un jingle ou une publicité coupe l'intervention). Ces locuteurs peuvent n'intervenir qu'une fois dans le fichier, auquel cas ils n'auront qu'un tour de parole. Mais, comme les journalistes ou les présentateurs, beaucoup de locuteurs sont présents tout au long du fichier et prennent la parole à plusieurs endroits dans l'enregistrement. Ces différents tours de paroles appartiennent au même locuteur, on dit qu'ils sont regroupés au sein de la même \og classe de locuteur \fg{}. La figure \ref{fig:decoupage_tours_classes} donne un exemple de découpage en tours de parole et de regroupement en classes de locuteurs.


\begin{figure}

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/decoupage_tours_classes}

\caption{Découpage en tour de parole et en classes de locuteurs}
\label{fig:decoupage_tours_classes}
\end{center}

\end{figure}

On peut noter sur cette figure, qu'en plus de regrouper les tours de parole par classe de locuteur (classe du Locuteur 1, classe du Locuteur 2), le genre du locuteur est aussi mentionné. Cette information vient enrichir la transcription et pourra être utile par la suite. Les classes de locuteur sont pour l'instant anonymes. En effet, la transcription ne nous permet pas de dire que c'est \og Monsieur untel \fg{} qui parle, elle permet juste de dire que c'est le même locuteur qui parle à deux endroits différents. La solution idéale serait de pouvoir nommer les classes de locuteurs par le prénom et le patronyme du locuteur concerné (couple que nous appellerons par la suite nom complet), c'est ce que les techniques d'identification nommée du locuteur cherchent à réaliser.

\subsection{Transcription automatique de la parole}

\subsubsection{Principes}

Le signal de parole que traitent les SRAP est un signal complexe à étudier. En effet, ce signal possède de nombreux paramètres qui peuvent énormément varier. Ce signal est évidemment différent d'un locuteur à l'autre mais peut aussi l'être pour le même locuteur en fonction des conditions d'enregistrement ou de l'état de santé de la personne. Quoiqu'il en soit, une même personne prononcera rarement le même mot de la même façon : cela rend les traitements automatiques compliqués. En effet, pour pouvoir traiter automatiquement ces données il faut être capable d'en extraire des informations pertinentes pour les systèmes automatiques, la difficulté réside à pouvoir extraire ces informations quelques soient les conditions d'enregistrement, l'émotivité, l'accent ou encore l'âge du locuteur. En plus de ces difficultés d'extraction des caractéristiques acoustiques, s'ajoutent la difficulté de segmentation du signal en mot. En effet pour un SRAP, la parole est un flux continu qu'il ne peut interpréter tel quel comme une suite de mots. La transcription que le SRAP produit est une transcription \emph{verbatim} du document sonore, une unité de cette transcription est appelée mot. Pour résoudre ces problèmes, la majorité des SRAP actuels utilisent une approche probabiliste.

L'objectif d'un système de RAP probabiliste est d'associer une séquence de mots $\hat{W} = w_1 w_2 ...wk$ (avec $w_i$ qui est un mot de cette séquence) à une séquence d'observations acoustiques $X$. Le système recherche la séquence de mots qui maximise la probabilité \emph{a posteriori} $P(W|X)$, où $P(W|X)$ est la probabilité d'émission de $W$ sachant $X$ . On obtient, après application de la règle de Bayes :

\begin{equation}
  \label{eq:bayes}
  \hat{W} = \underset{W}{arg\ max}\ P(W|X)\ =\ \underset{W}{arg\ max}\ \frac{P(W)P(X|W)}{P(X)}
\end{equation}

Comme la séquence d'observations acoustiques $X$ est fixée, $P(X)$ peut être considérée comme une valeur constante inutile dans l'équation \ref{eq:bayes}. On a donc :

\begin{equation}
  \label{eq:bayes_simplifie}
  \hat{W} =\ \underset{W}{arg\ max}\ P(W)P(X|W)
\end{equation}

Deux types de modèles probabilistes sont utilisés pour la recherche de la séquence de mots la plus probable : des modèles acoustiques qui fournissent la valeur de $P(X 
|W)$, et un modèle de langage qui fournit la valeur de $P(W)$. $P(X|W)$ peut se concevoir comme la probabilité d'observer $X$ lorsque $W$ est prononcée, alors que $P(W)$ se réfère à la probabilité que $W$ soit prononcée dans un langage donné. La difficulté pour obtenir un système de RAP performant est de définir les modèles les plus pertinents possibles pour le calcul de $P(W)$ et $P(X|W)$ (voir figure \ref{fig:principes_srap}).

\subsubsection{Transcription enrichie}
\label{sub:transcription_enrichie}

Transcrire un signal audio consiste d'une part à retranscrire les mots qui ont été prononcés mais aussi à enrichir cette transcription avec différentes informations éventuellement disponibles comme le début et la fin de chaque intervention, le nom des différents locuteurs ou encore leur genre. Ces transcriptions peuvent être réalisées par un humain (elles seront appelées transcriptions manuelles) ou de manière entièrement automatique (elles seront appelés transcriptions automatiques) : en fonction de cela, les informations disponibles dans la transcription peuvent varier, c'est notamment le cas du nom des locuteurs. En effet, un humain pourra essayer, en fonction du contexte, de nommer les locuteurs de l'enregistrement à l'inverse d'un système automatique qui utilisera des étiquettes anonymes en guise de nom de locuteur (comme locu1, locu2, ...). C'est précisément à cet aspect de la transcription enrichie (à savoir le nommage des locuteurs) que les travaux sur l'identification nommée du locuteur s'intéressent. Il convient tout d'abord d'expliquer plus précisément les différents aspects d'une transcription enrichie.



% \subsubsection{Transcription et étiquetage des entités nommées}
% 
%   La transcription du signal audio consiste tout simplement à écrire les mots qui ont été prononcés par les locuteurs. Que cette transcription soit réalisée par un humain ou par un système automatique de reconnaissance de la parole (RAP), elle consiste à remplir les différents tours de parole avec les mots correspondants. QUand les transcriptions sont réalisées par des systèmes de RAP, elles sont beaucoup moins riches que celles produites par un humain. En effet, en plus des erreurs de transcription, elles ne contiennent généralement pas de ponctuation. Ce manque peut être comblé par des post-traitements qui essaient de remettre la ponctuation dans la transcription. 
% 
% Un autre type de post-traitement utile pour l'identification nommée du locuteur est la détection des entités nommées. En effet, pour pouvoir attribuer un nom complet à un locuteur à partir de la transcription, il va d'abord falloir être capable de détecter ce nom complet dans la transcription. Pour ce faire, il existe des systèmes de détection des entités nommées qui vont être capable de détecter, en plus des noms complets, différentes entités nommées comme les lieux, les organisations ou encore les radios. La figure \ref{fig:etapes_transcription_enrichie} montre toutes les étapes pour obtenir une transcription enrichie en entités nommées.

\begin{figure}

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/segmentation-classification}

\caption{Les étapes de réalisation d'une transcription enrichie}
\label{fig:etapes_transcription_enrichie}
\end{center}

\end{figure}


% \subsection{Les systèmes automatiques : reconnaissance de la parole et détection des entités nommées}
% 
% À l'heure où l'informatique et Internet sont partout, les quantités de données numériques ne cessent de croître. Ces grandes collections de données sont difficilement indexables manuellement, il faut donc, pour faciliter la recherche et l'accès à l'information, qu'elles soient traitées de manière automatique. Les enregistrements audio sont traités à l'aide de systèmes de reconnaissance automatique de la parole (SRAP), ces systèmes sont constitués de plusieurs composantes comment le montre la figure \ref{fig:principes_srap}.
% 
% \begin{figure}
% 
% \begin{center}
% \includegraphics[width=0.9\columnwidth]{img/sys_rap}
% 
% \caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
% \label{fig:principes_srap}
% \end{center}
% 
% \end{figure}
% 
% \subsubsection{Principes généraux} % (fold)
% \label{ssub:principes_generaux}



% subsubsection principes_généraux (end)

% \subsubsection{Segmentation et classification} % (fold)
% \label{ssub:les_systemes_de_segmentation_et_de_classification_en_locuteurs}
% 
% Afin de traiter de manière efficace un signal complexe comme le signal audio, les SRAP ont besoin de segmenter le signal en parties homogènes appelées segments. Ces segments se doivent d'être cohérents, un même segment doit avoir les mêmes conditions d'enregistrement, doit être prononcé par le même locuteur, ... Ces segments sont caractérisés par des conditions acoustiques spécifiques comme la présence de parole, la nature de la parole (téléphonique ou enregistrement studio), la présence de musique, le genre du locuteur ou encore son identité ou une étiquette anonyme. Comme décrit dans \ref{ssub:decoupage_regroupement}, l'identification nommée utilise les tours de parole. Un tour de parole n'est en fait qu'une suite de segments dont l'identité ou l'étiquette sont identiques.
% 
% 
% Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.
% 
% 
% Les méthodes qui donnent les meilleurs résultats sur les journaux radiophoniques utilisent un regroupement BIC suivi d'un regroupement de type CLR [1, 2]. C'est ce type de système \ref{article sylvain} qui sera utilisé pour les travaux du présent papier.
% 
% Ces méthodes sont évaluées selon une métrique nommée DER (Diarization Error Rate). TODO : expliquer et donner la formule

% subsubsection les_systèmes_de_segmentation_et_de_classification_en_locuteurs (end)

% \subsubsection{Transcription automatique de la parole} % (fold)
% \label{ssub:transcription_automatique_de_la_parole}
% 
% Les systèmes de transcription automatique de la parole sont évalués selon une métrique nommée WER (Word Error Rate). TODO : expliquer et donner la formule

% subsubsection transcription_automatique_de_la_parole (end)


% \subsubsection{Détection des entités nommées} % (fold)
% \label{ssub:detection_des_entites_nommees}

% subsubsection détection_des_entités_nommées (end)

\subsection{Détection des entités nommées}

\newpage
\section{L'identification nommée du locuteur}

L'identification nommée du locuteur consiste à attribuer un prénom et un patronyme (appelé \og nom complet \fg{}) à chaque locuteur d'un document audio. Contrairement à la reconnaissance de la parole, l'identification nommées de locuteurs dans de larges corpus audio est un domaine de recherche relativement récent. Jusqu'en 2005, l'identification de locuteurs servait à suivre des locuteurs anonymes à différents endroits des enregistrements. Ce sont les travaux du LIMSI et plus particulièrement de Leonardo Canseco-Rodriguez \cite{CansecoRodriguez2005}, qui ont été les premiers à s'intéresser à l'identification nommée du locuteur utilisant la transcription (réalisée manuellement) du signal audio.

\subsection{Méthode purement acoustique}
\label{sub:suivi_locuteur}

L'identification nommée du locuteur a d'abord été réalisée avec des méthodes purement acoustiques \cite{??}. Ces méthodes sont des méthodes dites de \og suivi du locuteur \fg. En effet, on apprend un modèle acoustique à partir des échantillons de voix d'une personne, et c'est ce modèle qui sera ensuite utilisé pour identifier (ou plutôt suivre) la personne dans un autre enregistrement. Plus la quantité de données disponibles pour apprendre le modèle sera grande, plus l'identification du locuteur sera performante.
Même si ces modèles sont performants \cite{??}, ils souffrent de plusieurs inconvénients. Tout d'abord, il est absolument nécessaire de connaître les personnes que l'on cherche à identifier. En effet, puisqu'il faut commencer par apprendre un modèle acoustique et l'associer au nom du locuteur concerné, il n'est pas possible d'identifier des personnes que l'on ne connaît pas au préalable. De plus, lorsque l'on peut apprendre ce modèle acoustique, il faut posséder une quantité de données suffisante pour pouvoir l'apprendre : plusieurs minutes sont un minimum. Pour finir, il faut que les conditions acoustiques des données que l'on possède soient similaires à celles sur lesquelles il va falloir travailler : des données trop éloignées dans le temps (et donc avec une voix qui peut avoir changée) ou dans des conditions d'enregistrement différentes (studio ou téléphone par exemple) dégraderont les performances d'identification.

Ces méthodes entièrement basées sur l'acoustique imposent donc de disposer de modèles acoustiques pour chaque locuteur d'un enregistrement que l'on cherche à nommer : ces connaissances a priori sont souvent compliquées à obtenir et le principe même de ces méthodes empêche toute identification d'un locuteur non connu à l'avance. Ce sont ces limitations qui ont motivé les travaux consistant à utiliser la transcription du signal audio pour identifier les locuteurs d'un enregistrement. Dans les journaux d'information, les locuteurs ont pour habitude de s'annoncer, Canseco-Rodriguez \cite{CansecoRodriguez2005,CansecoRodriguez2006} a donc eu l'idée d'utiliser ces annonces pour essayer de nommer les locuteurs d'un enregistrement à partir du contenu de la transcription. Ces transcriptions peuvent être réalisées par un humain ou par des systèmes entièrement automatiques. Elles sont au coeur des méthodes d'identification nommées présentées dans ce papier.


\subsection{Concepts}

L'idée directrice des travaux reposant sur la transcription d'un enregistrement est que le nom complet d'un locuteur d'un enregistrement est présent dans la transcription de cet enregistrement. C'est généralement le cas dans les journaux radiophoniques où la majorité des locuteurs s'annoncent : il est donc possible d'utiliser ces informations pour nommer les locuteurs du document. C'est d'ailleurs pour cette raison que tous les travaux du domaine concernent uniquement des enregistrements de journaux radiophoniques. Pour pouvoir exploiter les noms de locuteurs cités dans la transcription il va tout d'abord falloir les détecter puis être capable de les attribuer aux bons tours de parole.

Les méthodes d'identification nommée du locuteur à partir de la transcription sont multiples mais partagent toutes un principe commun. Du pré-traitement de la transcription, en passant par des décisions locales puis globales, les outils utilisés divergent mais le principe reste le même.

\subsubsection{Préparation de la transcription} % (fold)
\label{ssub:enrichissement_de_la_transcription}

Bien que l'identification nommée se base sur des transcriptions, ces transcriptions doivent être enrichies (cf \ref{sub:transcription_enrichie}) et préparées avant de pouvoir les exploiter. En effet, dans le cadre de l'identification nommée, certains mots ont besoin d'être regroupés en catégories et remplacés dans la transcription. C'est le cas pour une bonne partie des entités nommées (lieux, radios, noms complets) qui ont été détectées dans la transcription. Ce pré-traitement a pour but de généraliser au maximum les règles qui seront utilisées par la suite. C'était  \og Maude Bayeu en direct de Paris\fg{} donnera par exemple \og C'était PERSONNE en direct de LIEU \fg{}. La figure \ref{fig:pre_traitement} illustre les pré-traitements nécessaires afin d'obtenir une transcription exploitable par un système d'identification nommée du locuteur.

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/pre_traitement.pdf}
\caption{Pré-traitement su signal audio}
\label{fig:pre_traitement}
\end{center}
\end{figure}

% subsubsection enrichissement_de_la_transcription (end)

\subsubsection{Étiquetage des nom complets et propagation} % (fold)
\label{ssub:etiquetage_des_nom_complets}

Lorsque la transcription est enrichie avec l'étiquetage des entités nommées qu'elle contient, il est possible de déterminer l'emplacement des différents nom complets cités dans l'enregistrement. La difficulté va être de réussir à déterminer si ces noms détectés sont des locuteurs de l'enregistrement ou non, et quand ils le sont à quelle partie de l'enregistrement ils se rapportent. Pour ce faire, les méthodes d'identification nommée attribuent des étiquettes à chaque nom complet détecté, ces étiquettes sont : \og précédent \fg{}, \og courant \fg{}, \og suivant \fg{} ou \og autre \fg{}. Ces étiquettes vont permettent de déterminer si le nom complet détecté dans la transcription désigne le locuteur s'exprimant dans le tour de parole précédent, dans le tour courant, dans le suivant ou dans aucun des tours contigus. Le dernier cas appelé \og autre \fg{} met en évidence une des limites de l'approche : que le nom complet détecté concerne un locuteur de l'enregistrement ou pas, ce locuteur doit parler dans les tours de parole contigus au tour de parole où son nom a été détecté. La figure \ref{fig:segment} décrit les différents cas de figures possibles.


\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/segment.pdf}
\caption{Principe de base des systèmes d'identification nommée basés sur la transcription}
\label{fig:segment}
\end{center}
\end{figure}

À cette étape, les noms complets détectés sont propagés aux tours de parole contigus. On appellera cette propagation \og décision locale \fg{} par la suite.

% subsubsection Étiquetage_des_nom_complets (end)

\subsubsection{Attribution d'un nom complet à une classe anonyme} % (fold)
\label{ssub:attribution_d_un_nom_complet_a_une_classe_anonyme}

Attribuer un complet à une classe de locuteur anonyme revient à répercuter les décisions locales au sein du fichier tout entier, c'est à dire passer des tours de paroles aux classes de locuteur, avec tous les problèmes que cela implique. En effet, comme le montre la figure \ref{fig:decisions_locales_conflit}, il est tout à fait possible que plusieurs noms complets différents soient en conflit au sein d'un même tour de parole. Dans l'exemple, deux noms complets sont possibles pour le locuteur numéro 2 : Maude Bayeu et Pierre Moscovici.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/decision_multiple.pdf}
\caption{Exemple de décisions locales conflictuelles}
\label{fig:decisions_locales_conflit}
\end{center}
\end{figure}

Chaque tour de parole étant regroupé au sein d'une classe de locuteur (celle que l'on cherche à nommer), ces conflits vont se répercuter au sein de la classe de locuteur elle même. La figure \ref{fig:processus_entier} résume tous ces différents problèmes.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/entire_process.pdf}
\caption{Vue globale du processus attribution d'un complet à une classe de locuteur anonyme}
\label{fig:processus_entier}
\end{center}
\end{figure}

Même si les façons de réaliser les différentes étapes différent entre les méthodes d'identification nommée, les principes qui viennent d'être décrits sont communs à toutes. Le traitement d'un fichier audio avec un système d'identification nommée se résume donc aux étapes qui sont décrites dans la figure \ref{fig:fonctionnement_general}.

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/fonctionnement_general.pdf}
\caption{Aperçu global d'un processus d'identification nommée du locuteur}
\label{fig:fonctionnement_general}
\end{center}
\end{figure}


% subsubsection attribution_d_un_nom_complet_à_une_classe_anonyme (end)

\subsubsection{Métriques et évaluation}

\label{sec:metrique}

Un système d'identification nommée est évalué en comparant l'hypothèse générée par celui-ci à la référence distribuée avec le corpus. Cette comparaison met en évidence 5 cas d'erreur ou de succès possibles relatifs aux situations suivantes~:
\begin{itemize}
\item l'identité proposée est correcte ($C_1$)~: le système propose une identité correspondant à celle indiquée dans la référence ;
\item erreur de substitution ($S$)~: le système propose une identité différente de l'identité présente dans la référence ;
\item erreur de suppression ($D$) : le système ne propose pas d'identité alors que le locuteur est identifié dans la référence ;
\item erreur d'insertion ($I$)~: le système propose une identité alors que le locuteur n'est pas identifié dans la référence ;
\item il n'y a pas d'identité ($C_2$)~: le système ne propose pas d'identité et la référence ne contient pas d'identité.
\end{itemize}


Une mesure de Précision et de Rappel peut être définie à partir des 5 cas d'erreur~:
\begin{equation}
	\label{eq:PR}
	P = \frac{C_1}{C_1+S+I} \ \ ; \ \ R = \frac{C_1}{C_1+S+D}
\end{equation}

%Dans tous les précédents articles \cite{Tranter06,Esteve07,Chengyuan07} traitant de l'identification nommée du locuteur, les résultats sont présentés sous la forme de mesures de précision et de rappel.

Comme il a été proposé dans \cite{Tranter06}, ces valeurs peuvent être complétées par un taux d'erreur $Err$ global également calculé à partir de ces 5 erreurs. Ce taux s'inspire du calcul du WER utilisé pour l'évaluation de la transcription. Il a l'avantage de mesurer la qualité des résultats du système d'identification nommée en une seule valeur, facilitant les comparaisons entre les systèmes par rapport aux mesures de précision et de rappel.
\begin{equation}
	\label{eq:PR}
	Err = \frac{S+I+D}{S+I+D+C_2+C_1} \ \ ;
\end{equation}


Les erreurs peuvent être calculées en terme de durée ou en terme de nombre de locuteurs.
Pour une évaluation en durée, dans le cas où un locuteur parlant 90\% du temps est correctement nommé et que les six autres locuteurs parlant seulement 10\% du temps ne le sont pas, le système présentera un taux d'erreur de 10\%. 

Pour une évaluation en terme de nombre de locuteurs, dans le même cas de figure, le système aura un taux d'erreur de 87,5\%.

D'un point de vue applicatif, la métrique exprimée en durée est préférable si les locuteurs considérés comme importants correspondent aux locuteurs s'exprimant beaucoup. En revanche, si l'application cherche à nommer le plus possible de locuteurs, il est plus intéressant d'évaluer les performances en terme de nombre de locuteurs.

\subsection{Règles symboliques}
\label{sub:regles_manuelles}

 Les auteurs ont été les premiers à montrer que le couple prénom/patronyme d'un locuteur apparaissant dans un contexte lexical donné permettaient d'identifier de manière précise l'identité des locuteurs s'exprimant dans les tours de parole proches. Leur méthode repose sur l'utilisation de règles affectant les étiquettes \textit{\og tour courant \fg}, \textit{\og tour précédent \fg}, \textit{\og tour suivant \fg } aux noms complets détectés. Ces étiquettes ont été reprises dans l'ensemble des travaux réalisés sur l'identification nommée depuis. Les règles utilisées ont été définies manuellement après analyse d'un corpus de langue anglaise. Douze règles sont utilisées pour désigner le locuteur courant, 34 pour le suivant et 6 pour le précédent.


Dans le but de généraliser les règles, les mots pouvant être regroupés au sein d'une même classe sémantique, sont remplacés par cette classe dans la transcription. Les différentes classes sémantiques retenues sont :
\begin{itemize}
	\item les noms complets (prénom et patronyme) de locuteurs ([name]),
	\item le nom de l'émission ([show]),
	\item les toponymes comme les villes, pays et monuments [(location]),
	\item les professions ([title]),
	\item les remerciements ([greet]),
	\item les mots faisant office d'accord avec l'autre ([agree]),
	\item mots gérant la communication (bonjour, au revoir : [comm])
	\item et les questions ([quest]).
\end{itemize}

Ces dictionnaires de classes ont été réalisés en extrayant les informations des transcriptions et en les complétant avec des listes de noms et des journaux en ligne. Ils permettent de généraliser les règles utilisées.

À partir du corpus de développement de 150 heures d'enregistrements de radio et de télévision anglais, les règles les plus utiles pour déterminer l'identité des locuteurs ont été extraites. Elles sont décrites dans le tableau \ref{table:canseco_regles}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      3162 & [title] [name] \\
      848 & I am [name] \\
      673 & [show]'s [name] \\
      382 & [agree] [name] \\
      293 & [name] [show] [location] \\
      186 & [show]'s [name] reports \\
      176 & [thanks] [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Règles les plus utiles pour déterminer l'identité d'un locuteur sur le corpus de développement}
  \label{table:canseco_regles}    
\end{table}

En plus de ces règles, d'autres règles comprenant un caractère joker (*) ont été utilisées. Ce caractère va permettre de remplacer n'importe quel mot, de manière à généraliser encore un peu plus les règles. Des exemples de règles avec joker sont données dans le tableau \ref{table:canseco_joker}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      458 & with [comm] us * [name] \\
      109 & joining * [name] \\
      108 & [name] * joins \\
      45 & with * [comm] me \\
      24 & [comm-agreement] * [name] reporting \\
      24 & we are joined * by [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Exemple de règles utilisant des joker (*) sur le corpus de développement}
  \label{table:canseco_joker}    
\end{table}

L'évaluation a été réalisée sur un corpus de test en anglais, où a été étudié le comportement de chaque règle pour les noms de locuteurs présents dans l'enregistrement. Le taux d'erreur des règles est de 11\%. Les règles permettent donc d'attribuer correctement un nom de locuteur au tour de parole précédent, suivant ou courant.


Ces travaux ont prouvé la pertinence de l'utilisation du contexte lexical pour nommer les locuteurs d'un enregistrement. En revanche, ils nécessitent un traitement manuel du corpus~: les règles et les dictionnaires de concepts sont réalisés par un humain. Le temps de mise en place de telles règles peut être long suivant la quantité de corpus à analyser et demande une expertise du domaine pour pouvoir être réalisée. Dans les travaux décrits dans \cite{CansecoRodriguez2005,CansecoRodriguez2006}, un corpus en langue anglaise de 150 heures a été étudié.
De plus, le passage d'un corpus à un autre est fastidieux : il faut réécrire le jeu de règles pour l'utiliser sur des documents d'une autre langue ou provenant d'autres types d'émission.
Pour finir, seuls les tours de paroles contigus au tour de parole dans lequel le nom de locuteur a été détecté peuvent être nommés. Aucune propagation au sein du fichier entier n'a été étudiée.
Ces travaux permettent de valider l'idée d'utiliser des informations linguistiques contenues dans les transcriptions manuelles (comme les noms complets des locuteurs et les mots les entourant) pour nommer les locuteurs d'un document audio.

\subsection{Règles statistiques : N-grammes}

\label{sub:regles_statistiques}

\subsubsection{Principes}

\subsubsection{Modèles N-grammes pour l'identification nommée du locuteur}

Suite aux travaux de Canseco (2005), le laboratoire de l'Université de Cambridge s'est intéressé à l'identification nommée du locuteur \cite{Tranter06}. Les travaux de Cambridge consistent à automatiser l'apprentissage des règles linguistiques et à mesurer l'impact de leur système d'identification nommée sur des données provenant de systèmes automatiques (classification en locuteur et transcription automatiques).

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/n_gram_tranter.pdf}
\caption{Exemple de n-grammes}
\label{fig:tranter_n_gram}
\end{center}
\end{figure}

Comme le montre la figure \ref{fig:tranter_n_gram}, les règles linguistiques sont représentées par des N-grammes allant de 2 à 5 mots. Ces N-grammes sont obtenus via un corpus d'apprentissage dans lequel la liste des personnes (leurs noms complets) intervenant dans les enregistrements est connue à l'avance. Les occurrences de ces noms complets sont détectées grâce à cette liste et les N-grammes sont appris en utilisant le contexte du nom complet détecté (une fenêtre de cinq mots incluant le nom complet est utilisée). Chaque N-gramme apparaissant plus d'un certain nombre de fois (cinq dans le cas de ces travaux) est considéré comme une règle de prédiction et est donc retenu comme règle linguistique.
Toutes les règles précédemment apprises sont utilisées sur le corpus de test. Lorsqu'une règle avec une probabilité $p_1$ est déclenchée et suggère un nom $n_1$ pour une classe de locuteur $s_a$, le score pour l'hypothèse $s_a = n_1$ est incrémenté.
Lorsque deux règles sont déclenchées pour le même nom de locuteur, leurs probabilités sont combinées en utilisant la formule :
\begin{equation}\mathbf{}
  p_{1+2}=1 - (1-p_1)(1-p_2)
\label{eq:sl}
\end{equation}

Comme dans les précédents travaux de Canseco (2005), Tranter (2006) utilise des classes sémantiques pour généraliser les règles. Ces classes sont issues des données d'apprentissage et ont été complétées avec d'autres sources (journaux écrits notamment). Ces classes sont les suivantes : GOODBYE, HELLO, OKAY, THANKS, LOCATION, PERSON, PERSON'S, SHOW, SHOW'S and TITLE. Ces classes regroupent celles utilisées par Canseco, la classe [agree] devient OKAY, [greet] devient THANKS, ... En revanche là où Tranter différencie HELLO et GOODBYE, Canseco les regroupait au sein d'une même classe [comm]. De plus, Tranter n'utilise pas la classe [quest].

Les résultats montrent que l'utilisation de catégories pour généraliser les mots présents dans les règles apporte un gain significatif. Sur des transcriptions réalisées manuellement et sur le corpus de développement, à 95 \% de précision le rappel est de 60 \%. Sur les données d'évaluation, le rappel chute à 38 \% pour la même précision.
Le système a été testé sur des données issues de systèmes automatiques, les résultats sont présentés dans le tableau \ref{table:resultats_tranter} et la figure \ref{fig:resultats_tranter}. 

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Trans. & Seg. & Rap. maximum & Rap. à 95 \% Pré. \\
      \hline
      ref & ref & 64 \% & 38 \% \\
      auto & ref & 44 \% & 38 \% \\
      auto & auto & 38 \% & 26 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Résumé des résultats sur le corpus de test. Sont présentés le meilleur rappel obtenu et le rappel à 95 \% de précision}
  \label{table:resultats_tranter}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/resultats_tranter.pdf}
\caption{Résultats sur le corpus d'évaluation avec (cat) ou sans (word) classes sémantiques. Les résultats sont donnés pour les transcriptions de référence (ref), les transcriptions utilisant un SRAP (asr) pour obtenir les mots et les transcriptions utilisant un système de segmentation automatique pour les tours de parole et les locuteurs (aseg) }
\label{fig:resultats_tranter}
\end{center}
\end{figure}

Le taux d'erreur (DER : Diarization Error Rate) du système de segmentation/classification utilisé est de 6,9 \%. Le taux d'erreur du système de transcription (WER : Word Error Rate) est quant à lui de 12,6 \%. L'utilisation de transcriptions automatiques n'affecte pas le rappel à 95 \% de précision mais le rappel maximum possible (en faisant baisser la précision) passe de 64 \% à 44 \%. L'utilisation de transcriptions et de segmentations automatiques fait chuter le rappel à 26 \% (pour 95 \% de précision) et fait aussi chuter le rappel maximum possible à 34 \%.
À noter que tous les résultats sont exprimés en terme du durée : plus les locuteurs parlant beaucoup sont détectés, plus les résultats sont élevés : le nombre de locuteurs correctement nommés n'est pas pris en compte. Voir la section \ref{sec:metrique} pour plus de détails sur les différentes métriques d'évaluation.



La méthode à base de N-grammes permet d'automatiser l'apprentissage des règles que Canseco avait définies à la main. De plus, une propagation relativement simple des scores est effectuée au sein des classes de locuteurs : les différents scores obtenus pour un locuteur $n_1$ et une classe de locuteur $s_a$ sont cumulés. Les tests réalisés montrent de bonnes performances sur des données manuelles, mais de beaucoup moins bonnes sur les données automatiques alors que les systèmes automatiques de transcription et de segmentation sont performants.
De plus, l'étiquetage des noms complets est réalisé grâce à une liste de locuteurs et l'étiquetage des autres catégories est réalisé à partir des données de l'apprentissage étiquetées manuellement.
Afin de se rapprocher d'une utilisation "réelle" de l'identification nommée il serait donc bon d'utiliser un détecteur d'entités nommées pour réaliser cet étiquetage. De plus, la propagation des scores au niveau des classes de locuteur et les problèmes concernant l'attribution d'un seul nom complet à une classe de locuteur soulevés dans \ref{ssub:attribution_d_un_nom_complet_a_une_classe_anonyme} n'ont pas vraiment été étudiés, une simple somme des différents scores étant effectuée.


\subsection{Règles statistiques : Arbre de décision}

Le laboratoire d'informatique de l'université du Maine (LIUM) a présenté ses travaux sur l'identification nommée du locuteur dans \cite{MauclairOdyssey06} et \cite{Esteve07}.
Les travaux du LIUM \cite{MauclairOdyssey06} ont été publiés en même temps que ceux de l'Université de Cambridge. Ils font aussi suite aux premiers travaux de Canseco. Le LIUM utilise un arbre de classification sémantique (SCT - Semantic Classification Tree \cite{Kuhn1995}) pour déterminer les règles linguistiques à utiliser pour attribuer l'étiquette \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} à un nom complet détecté dans la transcription. Ces règles linguistiques sont basées sur des expressions régulières construites autour des noms complets détectés. Ce système ayant servi de base aux travaux présentés ici, il est plus longuement présenté dans la section qui lui est consacrée \label{sec:sys_id_lium}.

\subsection{Modèle d'entropie maximum}

\subsubsection{Principes}

\subsubsection{Modèle d'entropie maximum pour l'identification nommée du locuteur}

Suite aux travaux du LIUM et de Cambridge, Microsoft s'est appuyé sur le modèle N-gramme et a ajouté des informations supplémentaires pour améliorer l'identification nommée du locuteur \cite{Chengyuan07}.

Ces travaux ont principalement ajouté des informations acoustiques pour améliorer l'identification nommée à base de N-grammes. Ces informations sont de deux sortes : 
\begin{itemize}
	\item l'utilisation de modèles acoustiques des voix des locuteurs cibles (lorsqu'il sont disponibles). Des modèles sont appris sur les échantillons de voix des locuteurs cibles. Ils seront ensuite utilisés pour essayer d'identifier ce locuteur acoustiquement pendant le test.
	\item l'utilisation du genre du locuteur. Lors de l'attribution d'un nom complet à un locuteur, une vérification est effectuée : le genre du prénom devrait correspondre au genre détecté par l'acoustique.
\end{itemize}
La position du nom complet dans l'enregistrement est aussi prise en compte. Posons $S_f$ le premier segment de la classe de locuteur anonyme $C$, $S_l$ le dernier segment et $n_C$ un nom complet étiqueté comme devant appartenir à la classe $C$. Plusieurs cas de figure sont étudiés, les plus pertinents sont : 
\begin{itemize}
	\item $n_l$ apparait avant $S_f$,
	\item $n_l$ apparait dans $S_f$,
	\item $n_l$ apparait avant $S_l$,
	\item $n_l$ apparait dans $S_l$.
\end{itemize}

Pour combiner toutes ces informations, Microsoft a choisi d'utiliser un modèle d'entropie maximum conditionnelle. Les règles linguistiques et les catégories sont obtenues de la même manière que dans \cite{Tranter06} : un modèle N-gramme pour apprendre les règles et un dictionnaire pour les catégories.

Les résultats montrent qu'en utilisation les mêmes informations que dans \cite{Tranter06} le modèle d'entropie maximum est plus performant. À l'inverse de l'information sur les genres et la position des noms complets, l'ajout des modèles de locuteur n'apporte en revanche aucun gain significatif. Les expériences ont été menées sur des transcriptions entièrement manuelles.

Ces travaux sont une extension des travaux menés par Cambridge dans \cite{Tranter06}. Ils améliorent les performances obtenues par Cambridge grâce au modèle d'entropie. Ils sont les premiers à introduire l'utilisation du genre du locuteur. En revanche, aucune expérience sur des données automatiques n'a été conduite. De plus,   les entités nommées et les catégories ont été étiquetées manuellement.

\subsection{Conclusion}

Les travaux sur l'identification nommée du locuteur utilisant la transcription enrichie du signal sont relativement récents puisque les premiers travaux menés par Canseco datent de 2004. Ils partagent tous la même hypothèse de base, à savoir qu'il est possible de déterminer, pour un nom de locuteur présent dans la transcription, s'il se rapporte au locuteur qui parle actuellement, à celui qui va parler juste après, ou à celui qui a parlé juste avant.

Deux principaux systèmes statistiques ont suivis les travaux de Canseco, à savoir celui à base de N-grammes de l'université de Cambridge et celui à base d'arbre de classification sémantique (SCT) de l'université du Mans. Les travaux de \cite{Esteve07} ont démontré la supériorité du système à base de SCT par rapport au système à base de N-grammes sur des transcriptions automatiques. Ces systèmes ne seront utilisables que sur des transcriptions automatiques, en effet l'intérêt de nommer automatiquement des locuteurs sur des transcriptions manuelles est nul. Il faudra donc s'intéresser plus particulièrement au système à base de SCT par la suite. Par rapport à ces systèmes précurseurs, Microsoft a ajouté la prise en compte du genre du locuteur et la prise en compte de la position du nom complet détecté dans l'enregistrement. Ces informations améliorant les performances, il faudra les prendre en compte.

Les travaux du LIUM ont été les seuls à utiliser un système automatique de détection des entités nommées. Dans le but d'automatiser le plus possible le processus, l'utilisation d'un tel système est indispensable. De plus, l'affectation des étiquettes \og précédent \fg{}, \og courant \fg{} et \og suivant \fg{} a beaucoup été étudiée dans la littérature. En revanche, le processus de propagation de ces étiquettes au niveau des classes de locuteur et la résolution des conflits n'a pas été étudiée jusqu'ici. Pour finir, beaucoup de travaux ont été conduits sur des transcriptions partiellement ou totalement manuel ; un travail sur des transcriptions entièrement automatiques est à réaliser.

\newpage
\section{Le système d'identification nommée du Laboratoire d'Informatique de l'Université du Maine (LIUM\_NI)}
\label{sec:sys_id_lium}

\subsection{Corpus et données}

\subsection{Entités nommées}

Dans \cite{Esteve07} ils utilisent pour la première fois un détecteur d'entités nommées pour détecter les noms complets mais aussi les catégories servant à généraliser l'apprentissage. Le système est celui qui a été utilisé pour la campagne d'évaluation ESTER 1, les entités nommées utilisées ont été tirées de celles demandées pour ESTER, à savoir : personne, lieu, organisation, groupe/organisation politique et temps. 

\subsection{Arbres de classification sémantique (SCT) et décision locales}

Comme l'Université de Cambridge, le LIUM a généralisé l'approche de Canseco en automatisant l'apprentissage des règles linguistiques (grâce à l'arbre de classification sémantique) et en automatisant le processus de décision consistant à attribuer un nom complet à une classe de locuteur anonyme.
L'arbre de classification utilisé est un arbre de décision binaire construit à partir du corpus d'apprentissage. Chacun des noeuds de l'arbre représente une expression régulière. Lorsque le contexte lexical (vingt mots à gauche et vingt à droite) d'un nom complet correspond à une des expressions régulières de l'arbre, une probabilité que ce soit le locuteur \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} lui est attribué, en fonctions des données apprises sur le corpus d'apprentissage. La décision \og locale \fg{} ( \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} ) avec le plus grand score est reportée au sein de la classe de locuteur anonyme correspondant. Lorsque le même nom complet est attribué plusieurs fois à une classe de locuteur, les probabilités de l'arbre de classification sont cumulées pour obtenir son score. Finalement, pour chaque classe de locuteur, le nom complet avec le plus grand score est attribué. À noter que l'arbre de classification permet de prendre en compte des informations plus \og globales \fg{} que le contexte lexical seul. Le LIUM utilise ainsi la position du nom complet dans le tour de parole (au début de celui-ci, à la fin celui-ci ou dans un tour de parole très court) comme information supplémentaire lors de l'apprentissage et du test de l'arbre.

Ils ont réimplémenté le système à base de N-grammes de \cite{Tranter06} et l'ont comparé à l'approche basée sur les SCT. Les évaluations ont été conduites sur le corpus d'évaluation de la campagne ESTER 1 avec une liste fermée de locuteurs provenant des corpus d'apprentissage, de développement et de test. Cette liste contient 1007 locuteurs. Les résultats ont montrés que le système à base de N-gramme et celui à base de SCT se comportait de manière similaire sur des transcriptions réalisées entièrement manuellement. En revanche, sur des transcriptions automatiques (en gardant la segmentation et la classification de référence), le système à base de SCT s'est avéré beaucoup plus robuste.

\subsection{Système de décision global}

Ces travaux sont les premiers à utiliser un système automatique de détection des entités nommées. Ils automatisent complètement le processus d'identification nommée, de l'apprentissage au test : toutes les étapes sont automatiques.
En revanche, le système de décision est relativement sommaire : seule l'étiquette avec le score maximal est pris en compte, les probabilités des autres étiquettes pour un nom complet sont tout simplement ignorées. De plus la prise de décision au niveau de la classe de locuteur se réduit à attribuer le nom de locuteur ayant un score maximum : l'incertitude au sein d'une même classe (plusieurs locuteurs possibles avec des scores élevés) n'est pas prise en compte.

\subsection{Expériences et résultats}

\subsection{Positionnement et hypothèses}
      Les travaux ont beaucoup porté sur la manière d'étiqueter les suivant/précédent/courant/autre => Un papier montre que les SCT > N-grammes. Aller plus loin : améliorer le processus de décision qui vient après ces décisions locales
      Utiliser une analyse conjointe du signal sonore et du texte pour améliorer les décisions "aveugles" (renforcer les décisions avec le genre des locuteurs)

Les noms complets des locuteurs sont présents dans la transcription, et ils est possible de les affecter à tour de parole contigu


\newpage
\section{LIUM\_NI : Un nouveau système de décision (titre à trouver ?)}

\subsection{Analyse des erreurs}


Toutes les mauvaises décisions prises par le système ont été étiquetées manuellement et comparées avec les différents scores générés par l'arbre. Sont comptées comme erreurs toutes les décisions non prises (erreur de suppression), ou toutes les décisions prises à tort (erreur de substitution). Cette étude a permis de mettre en évidence deux grandes catégories d'erreurs~: les erreurs de détection d'entités nommées et les erreurs de décision de l'arbre. D'autres types d'erreurs ont aussi été constatés mais elles sont moins significatives. 
	
%Dans les 18 fichiers analysés correspondant au test ESTER (10 heures d'enregistrement de journaux d'informations), 86 erreurs ont été détectées. Ces erreurs peuvent être de deux types, soit un mauvais nom de locuteur attribué, soit aucun nom attribué alors que c'était possible.

\subsubsection{Formalisme}

Dans l'étude qui suit, un même formalisme sera utilisé dans chacun des exemples. Les entités nommées détectées comme telles par Nemesis seront entourées de crochets, les anthroponymes seront en plus mis en italique (ex : \textit{[Jacques Chirac]}). ``(...)'' signifiera que la phrase a été coupée pour plus de lisibilité. Les exemples seront de ce type :\\\\
\texttt{
LOCU1 (Jacques Chirac) : (...) fin d'un tour de parole\\
LOCU2 (inconnu) : [Entité nommée] fin de phrase\\
LOCU2 (inconnu) : \textit{[Anthroponyme]}\\
LOCU3 (Bernard Thomasson): début du tour de parole suivant (...)\\\\
}

Chaque tour de parole sera identifié par une étiquette anonyme \texttt{LOCUX} suivie entre parenthèse de la vraie identité du locuteur tirée de la transcription de référence. Il pourra y avoir plusieurs segments à l'intérieur du même tour de parole, c'est le cas du \texttt{LOCU2} ici.

\subsubsection{Entités nommées}

Ce qui est ici appelé ``erreur'' est relatif à ce qu'attend l'arbre. \'Etiqueter \textit{[président Jacques Chirac]} comme anthroponyme est correct du point de vue linguistique et n'est pas une erreur commise par Nemesis, mais n'est pas conforme à ce qui est attendu par le système d'identification nommée. En effet, le système d'identification nommée ne marche qu'avec les couples noms~/prénoms et n'acceptera pas \textit{[président Jacques Chirac]}. Il faut donc bien garder à l'esprit que ce qui est appelé ici erreur, est ce qui met le système d'identification nommée en échec.\\

%\subsection{Anaphores et autres erreurs}

%Dans quelques cas, même un humain n'aurait pas été capable d'identifier la personne qui parle à partir du texte. En effet, il arrive que l'information ne soit pas du tout présente dans le texte. C'est le cas pour Bernard Thomasson dans 20041008\_1800\_1830\_INFO\_DGA, il n'est jamais cité dans la transcription. Dans les enregistrements de la Radio RTM, la journaliste Samira lamrani n'est jamais appelée par son nom/prénom. Seul son prénom est utilisé. Même si le système arrive à attribuer Samira au locuteur, le système attend uniquement des nom/prénoms, le locuteur est donc mis en inconnu.

%\subsection{Nemesis}


\textbf{\'Etiquetage erroné}\\

Nemesis rencontre des problèmes pour l'étiquetage des radios type France Inter et France Info. En effet, France étant un prénom, il étiquette France Inter et France Info comme étant des anthroponymes. L'ajout de France Info et de France Inter dans ses lexiques de médias ne change rien. La règle qui est déclenchée prioritairement par Nemesis est donc celle qui considère France comme un prénom. Par exemple :\\\\
\texttt{
LOCU1 (Frank Noblesse) : (...) sans pouvoir baisser sa vitesse\\
LOCU2 (inconnu) : \textit{[France Info]} actualités\\
LOCU2 (inconnu) : \textit{[Franck Noblesse]}\\
LOCU1 (\textbf{Frank Noblesse}) : dix huit heures quatre l' otage (...)\\\\
}

Dans cet exemple, \texttt{France Info} et \texttt{Franck Noblesse} sont étiquetés tous les deux comme anthroponymes.\texttt{ France Info} est évalué comme étant \texttt{LOCU1} (le locuteur précédent le tour de parole où il a été détecté) à 42\%. \texttt{Franck Noblesse} comme étant aussi \texttt{LOCU1} (cette fois ci le suivant) à 46\%. Un peu plus loin dans le fichier \texttt{France Info} est encore attribué à \texttt{LOCU1} :\\\\
\texttt{
LOCU3 (Nicolas Hénin) : (...) très hostile au sud de [Bagdad]\\
LOCU3 (Nicolas Hénin) : [Amman] \textit{[Nicolas Hénin]} \textit{[France Info]}\\
LOCU1 (Frank Noblesse) : \textit{[Christian Chesnot]} et \textit{[Georges Malbrunot]} otages en (...)\\\\
}

Ici \texttt{France Info} est encore détecté comme étant \texttt{LOCU1} (le locuteur suivant) à 92\%. Lorsque le système prend une décision, il se rend compte que \texttt{LOCU1} a été attribué deux fois à \texttt{France Info} avec un total de 1,34 points (0,42 + 0,92) alors qu'il n'a été attribué qu'une fois à \texttt{Franck Noblesse} avec un total de 0,46 points. Il choisit ainsi d'attribuer \texttt{France Info} comme identité pour LOCU1 au lieu de Franck Noblesse.


Lors de l'évaluation, le système ne prend pas en compte l'identité attribuée au locuteur puisque \texttt{France Info} n'est pas connu comme locuteur correct dans la transcription de référence, ceci fait donc chuter le rappel. C'est le seul cas d'étiquetage erroné posant vraiment problème dans le corpus étudié.\\

\textbf{\'Etiquetage partiel}\\

Dans quelques rares cas (3 sur les 86 erreurs), Nemesis n'étiquette que partiellement les identités de locuteur. Ceci peu être du à un prénom étranger comme \texttt{Anissa} (\texttt{Anissa \textit{[El Jabri]}} partiellement étiqueté) ou à prénom peu commun comme \texttt{Eymard} (\texttt{Eymard du \textit{[Chatenet]}} partiellement étiqueté). Un autre cas a été relevé, \texttt{\textit{[Frédéric]} Rivière} est en effet mal étiqueté puisque seul son prénom l'est.\\

%En revanche, il est aussi assez fréquent (15 occurrences sur les 18 fichiers) que Némésis étiquette mal le nom de locuteur, soit parce que c'est un nom étranger, soit parce qu'il lui arrive d'étiqueter la fonction de la personne. Quelques exemples ci dessous.
%CJ pourquoi ci-dessus fais tu une transition vers la partie suivante ? En lisant, on est un peu déstabilisé, on se demande où on est.
%CJ avant toute chose, il y a ambiguité sur le mot nom. On parle de nom du locuteur qui est en fait son prénom et son nom (de famille). Pourquoi ne pas appeler ce nom de famille le patronyme. ce serait plus clair.
%CJ Je ne comprends pas => Nemesis étiquette mal le nom du locuteur parce que c'est un patronyme étranger. Pourquoi ce n'est pas dans la catégorie "etiquetage erroné" ?
%CJ Ce n'est pas parce que Nemesis inclus le rôle de la personne dans l'entité nommée, qu'il commet une erreur d'étiquetage (voir explication thèse Fourour chap 4 p 84 de pourquoi il prend "large")
%VJ Oui je sais, c'est ce que j'essaie d'expliquer en dessous, je vais faire en sorte d'être plus clair.
\textbf{\'Etiquetage inadéquat}\\

Certaines étiquettes sont correctes du point de vue linguistique mais ne correspondent pas à ce qui est attendu par le système d'identification.
 Par exemple, \texttt{\textit{[président du Fetia Api]}} est bien un anthroponyme et est détecté comme tel par Nemesis. En revanche, lors de l'évaluation, \texttt{\textit{[président du Fetia Api]}} ne sera pas connu de la référence (dans laquelle il n'y a que des couples prénoms~/patronymes) et sera donc considéré comme inconnu. Ce genre d'erreur provient donc d'une inéquation entre les sorties de Nemesis et les entrées du système d'identification nommée.


Nemesis essaye tant que possible d'étendre l'entité nommée au maximum, ce qui donne des entités nommées comme celle-là : \texttt{\textit{[ministre de l'intérieur Jean Pierre Chevènement]}}. C'est en effet un anthroponyme et une entité nommée valide, mais qui ne correspond pas aux attentes du système d'identification nommée.

Un dernier cas d'étiquetage inadéquat relève quant à lui d'un problème lié à Nemesis. Ainsi, il est difficile pour Nemesis de délimiter le contexte droit d'une entité nommée, c'est à dire savoir où elle s'arrête, quelles autres entités nommées inclure, etc. Dans le cas d'entités nommées mixtes ou le problème de sur-composition se pose (entité nommée contenant une autre entité nommée), Nemesis choisi de ne ``retenir que la forme la plus complète constituant une entité nommée'' (cf \cite{Fourour04}, 4.1.2). Deux problèmes de ce type ont été relevées dans notre corpus : \texttt{\textit{[Joël Collado de Météo France]}} et \texttt{\textit{[Emmanuel Butstraten de BASF Agro]}}. En effet, le système d'identification nommée n'est évalué qu'avec des couples prénoms/patronymes, la sur-composition lui pose donc problème.\\
%CJ tu peux être plus explicite sur le problème (thèse Fourour chap 2  paragraphe 2.1.2.1 p 28 et chap 4 paragraphe 4.1.2 p 85 (l'identification). Dans tes 2 exemples, c'est un problème de sur-composition pour le contexte droit.

%Un ensemble de mots ne contenant pas de nom et prénom est détecté comme une personne. Cette détection fait chuter le rappel en attribuant ce \textit{faux locuteur} à un tour de parole. Par exemple, Nemesis étiquette l'entité nommée ``[président du Fetia Api]'' comme personne qui est ensuite attribuée à un tour de parole. ``[président du Fetia Api]'' est bien une personne, mais cette entité nommée n'est pas un locuteur au sens de l'identification. Il est identifié par sa fonction au lieu de son nom et prénom.

\textbf{Utilisation de Némésis dans un mauvais contexte}\\

Les entrées qui sont données à Nemesis sont des transcriptions utilisées pour la reconnaissance automatique de la parole. Ces transcriptions sont allégées de toute ponctuation, ce pourquoi Nemesis n'a pas été prévu. Nemesis étant un outil tiré du TALN, il se base sur la ponctuation pour détecter les entités nommées. Utiliser Nemesis avec de telles transcriptions l'amène à accoler des entités nommées qui sont normalement séparées par une ponctuation. Voici quelques exemples d'entités nommées agglutinées qui auraient du être étiquetées séparément (la ponctuation manquante a été rajoutée entre parenthèses): \texttt{\textit{[Bernard Thomasson (.) Claude Thélot]}}, \texttt{\textit{[Friponil (.) Frank Aletru]}}, \texttt{\textit{[France Info (,) Tendance Junior (,) Aurélie Kieffer (.)]}}.


\subsubsection{Erreurs de l'arbre de décision}

\textbf{Décision incertaine}\\

Dans beaucoup de cas, l'arbre de classification n'est pas capable de décider si le locuteur est le suivant, le précédent ou le courant. Ces locuteurs se retrouvent donc étiquetés comme \textit{autre} alors que dans la majorité des cas ils parlent avant ou après le tour de parole où ils ont été étiquetés.

Ceci est du à un locuteur qui n'est pas clairement annoncé, ce qui ne permet pas à l'arbre de savoir si l'entité nommée concerne le locuteur suivant. C'est le cas dans l'exemple ci-dessous.\\\\
\texttt{
LOCU1 (Valérie Crova) : (...) qui a toujours entretenu selon \textit{[Jean Christophe Bouisson]} des rapports tendus avec le gouvernement.\\
LOCU2 (Jean Christophe Bouisson) : On ne peut pas penser (...)\\
}
\\Dans cet exemple ``[Jean Christophe Bouisson]'' est étiqueté comme désignant un \textit{autre} tour de parole alors qu'il se rapporte au \textit{suivant} (LOCU2).\\\\

Un des cas relevé plusieurs fois est celui ou la personne annoncée est suivie de la fonction quelle occupe.\\\\
\texttt{
LOCU1 (Yves Izard) : (...) \textit{[Marie George Buffet]} la secrétaire nationale du parti communiste français\\
LOCU2 (Marie George Buffet) : le Premier ministre pense d'après (...)\\
}\\
On se retrouve ici dans un cas presque identique au premier, à cette différence prêt que la fonction ``la secrétaire nationale du parti communiste français'' pourrait être catégorisée et donc réduite à un unique mot FONCTION par exemple. Un ajout dans ce sens lors de la détection d'entités nommées serait intéressant pour généraliser au plus possible les mots lors de l'apprentissage de l'arbre.\\

\textbf{Décision erronnée}\\

Les erreurs de substitution sont toujours dues à des locuteurs qui ont été étiquetés à tort comme suivant, précédent ou courant. Comme décrit dans \ref{metrique}, ces erreurs n'affectent donc que les performances en terme de rappel.

Un cas qui revient plusieurs fois est celui où un locuteur annonce le prochain interviewé et le prochain intervieweur dans la même phrase. Dans ce cas, le système accorde toujours plus d'importance au locuteur cité en dernier, qui n'est généralement pas le locuteur suivant, mais celui juste après (le journaliste généralement).\\\\

\texttt{
LOCU1 (Fabrice Drouelle) : écoutez (...) \textit{[pers Jean Michel Hibon]} au micro de \textit{[pers Jérôme Susini]}\\
LOCU2 (Jean-Michel Hibon) : mon logement de fonction (...)
}\\\\
Dans ce cas LOCU2 est étiqueté comme étant Jérôme Susini et non Jean-Michel Hibon. En effet, selon l'arbre la probabilité que Jérôme Susini désigne le tour de parole suivant est plus grande que celle concernant Jean Michel Hibon.\\\\

%\noindent \textbf{20041013\_1700\_1800\_INFO\_DGA} :\\
%Bernard\_Thomasson 1144.075 1149.122 \textbf{[pers Jean Jacques Vanier]} sera à partir de demain sur la scène de L' Européen à [gsp Paris] pour trois mois \\
%Bernard\_Thomasson 1149.122 1153.543 il présente son nouveau spectacle à part ça la vie est belle à \textbf{[pers Bernard Stéphane]}\\
%\textbf{Jean-Jacques\_Vanier} 1153.543 1159.549 c' est l' histoire de quatre amis qui ont décidé de garder leur amitié au delà de la vie et au delà de la mort quoi \\\\

%Ici c'est Bernard Stéphane qui est attribué à Jean-Jacques Vanier.\\
%Sur les 12 erreurs affectant la précision que j'ai pu détecter, 10 étaient de ce type. Généralement, lorsque c'est la personne interrogée et non le journaliste qui parle en premier, le système commet une erreur.

% \subsection{Mauvaise décision de l'arbre}
% 
% La performance en terme de rappel du système est principalement affectée par les personnes qui sont signalées comme n'étant ni avant ni après (elles sont étiquetées en autre). Sur les 58 non attributions, 16 sont dues à ce type de problème. On observe généralement le problème lorsque la personne n'est pas mentionnée juste avant de parler, mais une ou deux phrases avant. Voici quelques cas représentatifs :\\\\
% 
% \noindent \textbf{20041011\_1300\_1400\_INTER\_DGA} :\\
% 
% \noindent delphine simon 1127.884 1134.114 Vanessa Descoureaux vient de joindre \textbf{Marie Sophie Desaulle} la présidente de l' association des paralysés de de France\\
% delphine simon 1134.114 1135.522 on écoute sa réaction\\
% \textbf{marie sophie desaulle} 1135.522 1138.677 ben Christopher Reeve permettait effectivement que\\\\
% 
% \noindent \textbf{20041013\_1700\_1800\_INFO\_DGA} :\\
% 
% \noindent yves izard 725.272 729.093 \textbf{Marie George Buffet} la secrétaire nationale du parti communiste français
% \textbf{marie george buffet} 729.093 734.142 le Premier ministre pense d' après les informations ils sont en vie mais tout cela bien sûr reste fragile\\\\
% 
% Il arrive aussi à l'arbre d'étiqueter un locuteur comme étant le prochain, et non l'actuel (7 occurrences). Ce type d'erreur génère aussi de mauvaises attributions de nom, 2 cas sur les 7. Les deux erreurs d'attribution de nom :\\\\
% 
% 
% \noindent \textbf{20041027\_1230\_1300\_RFI\_ELDA} :\\
% 
% \noindent eduardo febro 481.682 484.570 \textbf{Eduardo Febro} Miami radio France internationale\\
% \textbf{françois bernard} 484.570 491.834 République démocratique du Congo les ministres des affaires étrangères du Rwanda\\
% 
% François Bernard est étiqueté comme étant Eduardo Febro. A noter qu'Eduardo febro ayant déjà été étiqueté correctement plus haut, on ne devrait pas pouvoir attribuer deux fois le même nom à deux locuteurs différents.\\
% 
% \noindent \textbf{20041222\_1300\_1320\_RTM\_ELDA} :\\
% 
% \noindent samir bour 777.168 780.378 \textbf{Samir Bour} depuis Tanger pour RTM Chaîne Internationale\\
% \textbf{aziza ziani} 783.356 785.518 et puis cette inf activité royale\\\\
% Aziza ziani est ici étiquetée comme étant samir bour.

\subsubsection{Autres erreurs}
D'autres erreurs plus indépendantes du système ont été relevées~: certaines personnes ne sont citées que partiellement (seul leur prénom est cité dans la transcription), d'autres ont été mal orthographiées par le transcripteur ou encore certaines ne sont pas du tout citées ou annoncées dans la transcription. Sur les 11 heures du corpus, ce dernier cas ne concerne que 3 personnes. Cette constatation permet de valider l'hypothèse de départ~: les noms des locuteurs sont présents dans la transcription.

\subsubsection{Répartition des erreurs}

Le tableau \ref{tab:repart_erreurs} montre la répartition et l'importance des différents types d'erreurs sur le corpus de test. Les erreurs provenant de l'arbre de classification (A1 et A2) sont clairement dominantes avec plus de 72\% des erreurs totales alors que les erreurs dues à Nemesis (N1, N2 et N3) ne représentent qu'un peu moins de 19\% des erreurs. Pour l'arbre de classification, les erreurs A1 et A2 ont le même ordre de grandeur (environ 36\% en moyenne). En revanche, en ce qui concerne Nemesis les erreurs de type N1 sont plus fréquentes que celles de type N2 et N3 (environ 2 fois plus). Les étiquetages de France Info et de France Inter comme anthroponymes ne sont pas comptabilisés dans ces résultats. Leur proportion (79 étiquettes) relativement importante aurait faussé la répartition des autres erreurs. Nous considérons en effet que ce souci lié à Nemesis est marginal et sera facilement corrigé.

{\footnotesize{
\begin{figure}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Erreurs} & \textbf{Fréquence} & \textbf{Pourcentage Total} \\ \hline
\hline
\textbf{(N1) \'Etiquetage partiel ou inadéquat} & 7 & 8,2\% \\ \hline
\textbf{(N2) \'Etiquetage erroné} & 4 & 4,6\% \\ \hline \hline
\textbf{(N3) \'Mauvais contexte} & 5 & 5,8\% \\ \hline \hline
%\textit{total} & \textit{16} & \textit{18,6\%} \\ \hline \hline
\textbf{(A1) Décision incertaine} & 32 & 37,2\% \\ \hline
\textbf{(A2) Décision erronée} & 30 & 34,9\% \\ \hline \hline 
%\textit{total} & \textit{62} & \textit{72,1\%} \\ \hline \hline
\textbf{(E) Autre} & 8 & 9,3\%\\ \hline \hline
\textbf{Total} & 86 & 100\% \\ \hline
\end{tabular}
\end{center}

\caption{Répartition des erreurs sur le corpus de test.\newline
%------------\newline
{\textit{N1, N2, N3~: erreurs issues de la détection d'entités nommées.\newline
A1, A2~: erreurs issues de l'arbre de classification.\newline
E~: autres erreurs.\newline}}
%------------
}
\label{tab:repart_erreurs}

\end{figure}
}}

% \subsection{Autres erreurs}
% 
% Dans deux des cas relevés, la transcription du nom est fausse. La personne ayant transcrit manuellement le nom s'est trompé dans l'orthographe, le système ne trouve donc pas de concordance entre les noms trouvés.\\\\
% 
% \noindent \textbf{20041027\_1230\_1300\_RFI\_ELDA} :\\
% 
% \noindent  françois bernard 340.634 341.727 \textbf{Patrick Gouy}\\
% \textbf{patrice gouy} 341.727 345.982 les deux candidats ont ouvert au Mexique où vit un million d' expatriés\\\\
% 
% Il arrive aussi (3 occurrences) que le locuteur ne soit pas annoncé mais soit juste mentionné quelque part dans le texte, souvent plus loin, en faisant référence à ce qu'il a dit. C'est par exemple le cas de Jean Pierre Raffarin dans 20041012\_1800\_1830\_INFO\_DGA.\\
% Il peut aussi arriver que les erreurs soient reportée globalement alors que l'on hésitait entre deux personnes. Dans 20041007\_0800\_0900\_INTER\_DGA, pour un locuteur nous avons un score de 0.47 pour dominique de villepin et de 0.46 pour alain ray, nous choisissons dominique de villepin (ce qui est faux) et nous reportons l'erreur sur tout le fichier.

\newpage
\subsubsection{Conclusion}

Nous avons présenté notre système d'identification de locuteur ayant comme principales caractéristiques de s'appuyer sur les données de transcription, d'effectuer la reconnaissance des entités nommées grâce à un outil issu du traitement automatique des langues et de réaliser l'apprentissage et la prise de décision à l'aide d'un arbre de classification sémantique. Nous avons d'abord analysé les résultats obtenus en terme de précision et de rappel et nous avons obtenu des résultats prometteurs. Nous avons ensuite mené une étude exhaustive concernant les erreurs commises par le système, erreurs qui se situent principalement au niveau du système de reconnaissance d'entités nommées Nemesis et de l'arbre de classification. 

Au niveau de Nemesis, il faudrait premièrement calibrer ses sorties pour les adapter aux entrées attendues de notre système d'identification. En effet, Nemesis a été conçu pour détecter des entités nommées simples et complexes (par exemple ``président de la république Jacques Chirac'' est une entité nommée étiquetée comme anthroponyme par Nemesis) et le système d'identification exploite des entités nommées sous une forme simple de type ``prénom/patronyme'', ce qui génère des erreurs.  Deuxièmement, l'utilisation de Nemesis peut aussi être optimisée par un ajout de ponctuation au niveau de la transcription qui lui permettrait de mieux détecter les entités nommées (notamment quand une entité se trouve à la fin d'un segment et l'autre au début du suivant, il étiquette les deux entités nommées ensemble).

%Au niveau de Nemesis, la prise en compte des fonctions des personnes et la modification de certaines règles de décision permettrait d'améliorer l'étiquetage. Mais aussi, un ajout de ponctuation au niveau de la transcription
% (virgule après chaque fin de segment) 
%lui permettrait de mieux détecter les entités nommées.
% (notamment quand une entité se trouve à la fin d'un segment et l'autre au début du suivant, il étiquette les deux entités nommées ensemble).
%SM c'est pas déjà fait !

Au niveau de l'arbre de classification, il serait intéressant de prendre en compte le numéro d'ordre du nom de locuteur détecté dans le tour de parole lors de l'apprentissage de l'arbre et du test. Ceci permettrait dans certains cas de faire abstraction des mots pouvant suivre le dernier nom de locuteur du tour de parole. Dans un souci d'augmenter les données d'apprentissage fournies à l'arbre, catégoriser ou lemmatiser le plus de mots possibles semblerait être une piste intéressante. La détection des fonctions des personnes par Nemesis irait dans ce sens.

Pour finir, comme c'est déjà le cas pour certains modèles de langage en reconnaissance de la parole, utiliser les informations fournies par un étiqueteur grammatical pour apprendre et tester l'arbre de classification sera à étudier.

\subsection{Meilleure détection des entités nommées}

\subsubsection{Némésis, un outil prévu pour le TAL}

\subsubsection{Némésis et LIA\_NE :  expériences et résultats}

\subsection{Système de décision}

\subsubsection{Problématique}

\subsubsection{Décisions locales : le classifieur}

\subsubsection{Décisions globales : Théorie des croyances}

\subsection{Applications et connaissances a priori}
liste de locuteurs, genre des locuteurs

\subsection{Limites de l'approche}
très dépendant du corpus pour l'automatique (car on mesure en durée), un ou deux locuteurs détectés en plus ou en moins peuvent beaucoup faire varier les résultats alors que ça ne représente qu'une infime partie du nombre des locuteurs à détecter

\newpage
\section{Conclusion}

\newpage
\bibliography{biblio}
%\bibliographystyle{frcomplet}
\bibliographystyle{alpha}

\end{document}
