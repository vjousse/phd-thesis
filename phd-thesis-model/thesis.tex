
\input{headers}

\begin{document}
%\layout

\dominitoc

\begin{titlepage}

\input{cover}

%\chapter*{Remerciements}

%\chapter*{Résumé}

\tableofcontents

\postheader

\chapter{Introduction}
\minitoc

\chapter{Reconnaissance automatique la parole}
\minitoc
\newpage

La reconnaissance automatique de la parole (RAP) a pour but de transcrire sous forme textuelle un signal audio. La RAP est un problème complexe, c'est pourquoi seuls des sous problèmes ont été résolus à ce jour. Des contraintes sont imposées afin de réduire la complexité du problème en ciblant des applications bien particulières. Un système idéal serait capable de transcrire n'importe quel locuteur (qu'il le connaisse ou nom), dans n'importe quel environnement sonore (studio, téléphone, dans la rue, etc) en utilisant un registre de langage et une locution spontanés. Malheureusement, un tel système n'existe pas et des contraintes sont encore nécessaires. Ces contraintes peuvent être la mise à contribution du locuteur pour apprendre sa voix afin de mieux la reconnaître, ou encore la réduction des phrases que le système va être capable de reconnaître.

Les systèmes de RAP peuvent êtres classés selon plusieurs critères.

\begin{itemize}
  \item \textbf{Le mode d'élocution} : cela peut être des syllabes ou des mots isolés, des mots connectés entre eux mais avec des pauses artificielles ou alors une parole quasi naturelle, appelée parole continue. 
    
    À noter que pour la parole continue, la distinction est faite entre la parole dite \og préparée \fg{} et la parole dite \og spontanée \fg{} \cite{Bazillon08}. La parole \og préparée \fg{} s'apparente à l'élocution qu'aura un journaliste lorsqu'il présente les informations : les répétitions, les faux départs et autres hésitations du langage parlé y sont peu présentes. La parole \og spontanée \fg{} concerne quant à elle le type de parole utilisée lors des conversations entre plusieurs personnes : les répétitions, hésitations et autres irrégularités du langage parlé y sont très présentes.

  \item \textbf{La taille du vocabulaire} : Tous les systèmes ont besoin d'un vocabulaire plus ou moins important. La taille d'un vocabulaire peut être inférieure à 1000 mots pour les systèmes à petit vocabulaire comme les serveurs vocaux mais peut dépasser les 60000 mots pour les systèmes de dictée vocale ou de transcription de journaux d'information.
  \item \textbf{La dépendance (ou non) vis-à-vis des locuteurs} : les serveurs vocaux doivent par exemple être indépendants de la personne qui va l'utiliser, tandis que les systèmes de dictée vocale peuvent se permettre d'être dépendant du locuteur.
  \item \textbf{L'environnement} protégé ou non : les serveurs vocaux vont devoir être capable de fonctionner dans un environnement bruité (on ne peut connaître à l'avance quel sera l'environnement sonore du locuteur qui interrogera le serveur), en revanche les systèmes de dictée vocale fonctionneront en environnement plus protégé.
\end{itemize}

Les applications de la RAP sont multiples et peuvent aller de la simple commande vocale (reconnaissance de mots isolés) à la transcription complète d'une émission de radio. Nous donnons par la suite un aperçu des quatre grands types de systèmes qui existent en reconnaissance de la parole.

\section{Différents types de systèmes}


\subsection{Commandes vocales}

Les systèmes à commandes vocales sont des systèmes que l'on trouve abondamment dans les systèmes embarqués. Ils vont permettre une interaction entre l'utilisateur et la machine grâce à des commandes vocales. Ces commandes sont de simples mots isolés que l'utilisateur doit prononcer pour interagir avec le système. Les cas d'utilisation sont nombreux, et il serait impossible de tous les citer, en voici un bref aperçu :

\begin{itemize}
  \item Jeux vidéo : la console portable DS de Nintendo embarque un micro pour permettre l'interaction homme/machine grâce à de simples mots.
  \item Téléphones portables : beaucoup de téléphones récents permettent de naviguer dans l'interface grâce à la voix.
  \item Aide aux handicapés : de tels systèmes peuvent pallier à un organe défaillant ou peuvent être utilisés dans des systèmes destinés à la ré\-éducation d'enfants sourds.
\end{itemize}

\label{lexique}

Comme tous les systèmes de RAP, ces systèmes se basent sur un vocabulaire (aussi appelé lexique) préalablement constitué. Ce lexique va définir la liste des mots que le système va être capable de reconnaître. Aucun autre mot que ceux contenus dans le lexique ne pourra être reconnu par le système.
Bien qu'il soit techniquement possible d'avoir un vocabulaire assez important, il est généralement limité à une centaine de mots. En effet, les personnes se servant de ces systèmes ne sont de toute façon pas capable de mémoriser plus d'une centaine de commandes. À noter que ces mots doivent être contrastés au niveau phonétique, de manière à réduire les risques d'ambiguïté lors de la reconnaissance.

Bien que d'autres méthodes aient été employées, comme la comparaison de références par programmation dynamique \cite{Tubach}, les systèmes utilisant des méthodes statistiques sont majoritairement utilisés depuis les années 1980 \cite{Rabiner89}. Certains de ces système utilisent des réseaux neuronaux, mais la plus part ont comme base les Modèles de Markov Cachés \cite{Gagnoulet89} (cf. \ref{ssec:trans_auto}). 

\subsection{Systèmes de compréhension}

Les systèmes de compréhension vont permettre un dialogue contraint avec une machine. L'utilisateur va devoir prononcer une suite de mots clefs que le système est capable de reconnaître. En plus du système de reconnaissance vocale, un dispositif de compréhension des phrases doit être utilisé afin d'interpréter les phrases et de réagir en conséquence. Le vocabulaire est restreint à quelques centaines de mots, comme dans les systèmes à commandes vocales. Afin de faciliter la gestion du dialogue par le système, les phrases acceptables se limitent à des schémas grammaticaux simplifiés. 

Bien que peu naturelles, ces restrictions syntaxiques et sémantiques sont acceptables si l'application est bien ciblée sur un domaine particulier. Les applications choisies pour ce type de systèmes le sont d'ailleurs en conséquence. Elles se limitent généralement à l'interrogation d'une base de données, de standards téléphoniques automatisés pour des renseignements météo ou des réservations de places.

Ces systèmes ont connu un important essor à la fin des années 1970 avec le lancement du projet DARPA aux États-Unis en 1977. Plusieurs laboratoires participent au projet DARPA comment CMU, MIT, BBN, SRI ou encore Bell Labs. Par exemple, le système SPHINX développé au CMU utilise un vocabulaire d'environ mille mots (issus du corpus \og Ressource Management Data Base \fg{}); ce système a notamment le mérite de permettre aux locuteurs une parole continue et ne nécessite pas d'apprentissage préalable du locuteur à reconnaître. Le taux de reconnaissance sur les mots est excellent, il est de l'ordre de 96\% \cite{Lee1990}.

En France, la recherche s'est orientée vers des systèmes capables de gommer les erreurs de reconnaissance. En effet, même avec des erreurs de décodage du signal sonore, il est souvent possible de comprendre le sens d'une phrase. Ces erreurs de \og bas niveau \fg{} pourraient être récupérées grâce à des systèmes de plus haut niveau. C'est dans cette optique qu'ont été conçus les systèmes KEAL au CNET, Esole au LIMSI \cite{Tubach}, Myrtille au CRIN \cite{Pierrel82} et Arial au CERFIA \cite{Perennou82}.

\subsection{Systèmes de dictée automatique}

Les systèmes de dictée automatique ont pour but de transcrire un texte dicté par un locuteur de la façon la plus fidèle possible. Comme pourrait le faire une secrétaire, le texte transcrit doit respecter les règles orthographiques et grammaticales propres à la langue concernée. La compréhension du texte à transcrire n'est pas requise, et c'est d'ailleurs ce qui amène les erreurs les plus perturbantes pour l'utilisateur. Les systèmes de dictée automatique ne sont pas capable de discriminer les différents sens d'un même mot.

Ce type de système se trouve à la charnière entre l'oral et l'écrit. En effet, même si l'interaction avec le système se fait grâce à la voix, le registre de langue utilisé sera plus proche du registre écrit. Ces systèmes sont souvent utilisés pour transcrire des rapports, des comptes rendus ou rédiger des lettres. Quoiqu'il en soit, la personne est consciente qu'elle s'adresse à un ordinateur et adaptera donc sa locution en conséquence. De plus, les tournures agrammaticales ou les hésitations seront beaucoup moins nombreuses que dans un dialogue spontané entre deux personnes. Dans une conversation, les hésitations, les faux départs, les répétitions ou les tournures incorrectes sont légions. La complexité est donc moindre que si il fallait retranscrire un dialogue tel quel.

En revanche, l'exercice même de la dictée sous-entend l'utilisation de plusieurs dizaines voire plusieurs centaines de milliers de formes fléchies. Les mots sont pris en contexte, et régis par une syntaxe aussi libre que la grammaire de la langue naturelle le permet. Le vocabulaire doit lui aussi être conséquent et est constitué de plusieurs milliers de mots (de 5~000 à plus de 60~000 mots). Avec ce type de système, une séance de dictée ressemblera plus à un médecin qui dicte un compte-rendu d'opération à sa secrétaire qu'à une maîtresse d'école qui fait faire un exercice de français à ses élèves. L'utilisateur n'est donc pas comme l'enseignant, même si il a bien en tête ce qu'il doit dire à la machine, il improvise quand même un minimum. Il doit donc pouvoir se tromper, corriger des mots ou remanier une tournure.

Pour obtenir de bonnes performances en temps réel, ces systèmes à grand vocabulaire sont fortement dépendants du locuteurs. Une phase d'apprentissage est requise afin de permettre au logiciel d'apprendre des modèles spécifiques à la voix de la personne qui l'utilise. Plus cette phase d'apprentissage est importante moins le système commet d'erreurs.

Historiquement, c'est l'équipe de recherche IBM dirigée par F. Jelinek qui est la première à avoir développé un système grand vocabulaire (\emph{Tangora} 5~000 mots en 1985, 20~000 en 1987) pour la dictée vocale. Par la suite, l'ensemble des grands systèmes développés se sont inspirés du système \emph{Tangora}. Avec un taux de réussite supérieur à 95\% pour un vocabulaire de 20 000 mots, \emph{Tangora} est un système multi-lingue existant notamment pour l'anglais \cite{Averbuch87}, l'italien \cite{Alto87}, le français \cite{CerfDanon90} ou encore l'allemand \cite{Wothke89}.


\subsection{Systèmes de transcription de journaux d'informations}
\todo{trouver un autre titre ?}

\label{ssec:trans_broadcast}

Au milieu des années 90 sont apparues des campagnes d'évaluation ayant pour but de tester les performances des systèmes sur des corpus lus. Par exemple, les données disponibles pour la campagne américaine Hub-3 \cite{Pallet96} sont des extraits de journaux comme le \emph{Wall Street Journal} ou le \emph{New York Times}. Ces extraits sont lus et enregistrés dans différentes conditions de manière à constituer des corpus audio. Le langage utilisé est donc celui utilisé pour de l'écrit. De ce fait, ce type de campagne évalue les performances de systèmes s'apparentant à la dictée vocale. Les performances deviennent rapidement excellentes, le système d'IBM affiche des taux d'erreur de l'ordre de 7\% \cite{Bahl95}. En France, la première campagne lancée par l'AUPELF-UREF consistait à transcrire des textes lus du journal \emph{Le Monde} en utilisant le corpus BREF \cite{Lamel91}. Comme pour la campagne Hub-3, les résultats sont très bons puisque le système du LIMSI affiche des taux d'erreurs de l'ordre de 11\% \cite{Gauvain94speaker-independentcontinuous}.

Les campagnes d'évaluation se sont adaptées et de nouveaux corpus constitués d'enregistrements de journaux d'informations radiophoniques ont été constitués. Même si ces corpus contiennent beaucoup de parole qui a été préparée et écrite à l'avance par les journalistes, elle est néanmoins convertie à l'oral. À la différence des corpus précédents qui n'étaient constitués que de corpus écrits ensuite lus tel quel. De plus, ces corpus contiennent aussi de la parole plus spontanée, puisque beaucoup d'émissions font intervenir des invités ou interrogent des personnes dans la rue. À noter que ces corpus sont aussi fortement multi-locuteurs. Avec ces nouvelles campagnes d'évaluations, les systèmes sont confrontés à des corpus qui se rapprochent de plus en plus de dialogues complètement spontanés entre plusieurs personnes (même si comme nous l'avons fait remarquer, une partie de la parole est encore relativement préparée).

Aux États-Unis, le DARPA puis le NIST ont organisés des campagnes d'évaluations des systèmes de transcription d'émissions \cite{Graff96the1996} à partir de 1996. En France, c'est la campagne ESTER \cite{Gravier04,Galliano06} organisée par l'AFCP, la DGA et ELRA qui a permis de réaliser les premières évaluations des systèmes de reconnaissance Français sur des journaux d'informations radiophoniques. Une deuxième campagne d'évaluation Française nommée ESTER II \cite{Ester2} a ensuite été menée de 2008 à 2009 de manière à mesurer les progrès des différents systèmes. Cette campagne s'est aussi caractérisée par l'évaluation des systèmes de détection des Entités Nommées.

La métrique utilisée  par les différentes campagnes pour évaluer les transcriptions est le taux d'erreur mot (WER -- Word Error Rate). Pour calculer ce taux d'erreur, la meilleure hypothèse de transcription du système est alignée avec les transcriptions réalisées manuellement (transcriptions dites de référence). Trois types d'erreurs sont alors possibles. Lorsqu'un mot apparaît dans l'hypothèse de reconnaissance alors qu'il n'y a aucun mot correspondant dans la référence, il s'agit d'une insertion. Si, au contraire, il y avait un mot dans la référence et que ce mot n'apparaît pas dans l'hypothèse, nous parlons de suppression. Quand un mot de la référence est remplacé par un autre mot lors du décodage, c'est une substitution. Le taux d'erreur est ensuite déterminé par la formule \ref{eq:wer}. 

\begin{equation}\mathbf{}
  WER=\frac{\mathrm{nb.~insertions + nb.~suppressions + nb.~substitutions}}{\mathrm{nombre~de~mots~de~la~r\acute ef\acute erence}}
\label{eq:wer}
\end{equation}  

Lors de la première campagne ESTER\cite{Galliano05}, le meilleur système (celui du LIMSI) atteignait un taux d'erreur mot de 11,9\%. L'écart entre les différents systèmes était cependant relativement important, puisque le deuxième système, celui du LIUM, affichait un taux d'erreur mot de 23,6\%. Lors de la campagne ESTER II \cite{Ester2}, le premier système était toujours le LIMSI avec un taux d'erreur de 12,1\% . En revanche l'écart avec le système du LIUM s'est réduit puisque son taux d'erreur n'était plus que de 17,8\%.

\section{Transcription enrichie}

\label{sec:trans_enrichie}

Avec ces nouvelles campagnes est apparue la notion de transcription enrichie. En effet, les corpus étant constitués d'enregistrements de journaux radiophoniques, d'autres informations que la simple transcription en mots peuvent être extraites des corpus. Par rapport aux corpus de dictée vocale, les documents sont multi-locuteurs. Au sein d'un même enregistrement, des dizaines de locuteurs différents peuvent parler tout au long du fichier. De plus, ces locuteurs peuvent s'exprimer depuis un téléphone ou en studio. Toutes ces informations permettent d'étayer, d'enrichir la transcription. De nouvelles tâches sont donc apparues dans les campagnes permettant d'évaluer l'enrichissement des transcriptions, comme la reconnaissance du locuteur ou encore la détection des entités nommées.

La figure \ref{fig:etapes_transcription_enrichie} décrit les différentes composantes des transcriptions réalisées pour les campagnes de type ESTER. Lors de la réalisation de ces transcriptions, il est possible de distinguer deux grandes parties que nous détaillons ci-dessous. 

\subsection{Segmentation et classification}
La première partie consiste à découper le fichier de manière à obtenir des portions homogènes, c'est à dire concernant le même locuteur et les mêmes conditions d'enregistrement. Ce premier niveau de découpage est communément appelé découpage en segments. Contrairement à ce que l'on pourrait penser, les segments ne représentent généralement pas des phrases, mais plutôt des groupes de souffle : ils commencent et s'arrêtent lorsqu'un silence significatif est détecté. Ce premier découpage est effectué pour des raisons techniques, et notamment pour faciliter la tâche des systèmes de transcription automatique (cf. \ref{ssec:trans_auto}), qui doivent décoder segment par segment pour éviter des problèmes d'explosion combinatoire.

Tous les segments contigus appartenant au même locuteur définissent un tour de parole de ce locuteur. Un même locuteur qui parle à plusieurs endroits d'un enregistrement aura donc plusieurs tours de parole au sein de cet enregistrement. En reconnaissance du locuteur (cf. \ref{ssub:decoupage_regroupement}), on dit que tous les segments (et donc les tours de parole) d'un même locuteur sont regroupés au sein d'une même classe. À chaque locuteur du document correspond donc une classe de locuteur.

Au niveau purement acoustique, il est aussi possible de détecter le genre du locuteur qui s'exprime \cite{liumspkdiarization}. Cette information peut être ajoutée à la transcription enrichie, comme c'est le cas dans la figure \ref{fig:etapes_transcription_enrichie}.

À noter que pour apprendre et évaluer les systèmes de segmentation et de classification automatiques, des données de références doivent être disponibles. Ces données sont réalisées manuellement par un humain écoutant les enregistrements audios. Il découpe alors le fichier en segments et tours de paroles, puis renseigne le nom des différents locuteurs au sein du fichier. 
\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/systeme_transcription_description}

\caption{Les étapes de réalisation d'une transcription enrichie}
\label{fig:etapes_transcription_enrichie}
\end{center}

\end{figure}


\subsection{Transcription et entités nommées}

La deuxième partie consiste tout simplement à écrire les paroles prononcées par les différents locuteurs afin d'obtenir la transcription en mots du signal. Lorsque cette transcription est réalisée automatiquement, la ponctuation ainsi que les majuscules ne sont pas requises. En revanche, des techniques existent pour rajouter la ponctuation et les majuscules aux transcriptions afin de se rapprocher, tant que faire se peut, d'une transcription réalisée manuellement \cite{Beeferman98cyberpunc,Huang2002}.

Les transcriptions, qui plus est les transcriptions de journaux radiophoniques, contiennent un grand nombre d'entités nommées (noms de personnes, lieux, radios, etc). Des systèmes de détection des entités nommées permettent de les repérer dans les transcriptions (cf. \ref{ssec:EN}). Ce sont des informations supplémentaires qui vont pouvoir être utilisées en recherche d'informations, ou plus précisément dans le cadre des présents travaux, pour identifier les locuteurs d'un document.


%\section{Architecture détaillée}

%Les différentes campagnes d'évaluations des systèmes de RAP actuelles met\-tent l'accent sur la réalisation d'une transcription enrichie (cf. \ref{ssec:trans_broadcast}) via l'extraction d'informations sur les locuteurs ou encore la détection des entités nommées. Lors de la réalisation d'une transcription enrichie, plusieurs systèmes entrent en jeu. Les systèmes de transcription automatique de la parole évidemment, mais aussi des systèmes de détection d'entités nommées et de reconnaissance automatique du locuteur. Afin de situer le contexte des travaux présentés dans cette thèse, ces différents domaines de recherche sont présentés ci-dessous.

\section{Transcription automatique de la parole}
\label{ssec:trans_auto}

La transcription automatique de la parole est au coeur du processus de production d'une transcription enrichie : c'est elle qui va fournir le texte correspondant aux paroles prononcées.


Les systèmes de transcription automatique de la parole utilisés actuellement sont des systèmes fondés sur des méthodes statistiques. Ces fondements ont été élaborés par Jelinek et son équipe chez IBM \cite{Jelinek1976}, au milieu des années 1970. La figure \ref{fig:principes_srap} donne un aperçu du fonctionnement actuel d'un système de transcription automatique de la parole. 
 
\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/sys_rap}

\caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
\label{fig:principes_srap}
\end{center}

\end{figure}

\subsection{Principes généraux} % (fold)
\label{ssub:principes_generaux}


L'objectif d'un système de RAP probabiliste est d'associer une séquence de mots $\hat{W} = w_1 w_2 ...wk$ (avec $w_i$ qui est un mot de cette séquence) à une séquence d'observations acoustiques $X$. Le système recherche la séquence de mots qui maximise la probabilité \emph{a posteriori} $P(W|X)$, où $P(W|X)$ est la probabilité d'émission de $W$ sachant $X$ . On obtient, après application de la règle de Bayes :

\begin{equation}
  \label{eq:bayes}
  \hat{W} = \underset{W}{arg\ max}\ P(W|X)\ =\ \underset{W}{arg\ max}\ \frac{P(W)P(X|W)}{P(X)}
\end{equation}

Comme la séquence d'observations acoustiques $X$ est fixée, $P(X)$ peut être considérée comme une valeur constante inutile dans l'équation \ref{eq:bayes}. On a donc :

\begin{equation}
  \label{eq:bayes_simplifie}
  \hat{W} =\ \underset{W}{arg\ max}\ P(W)P(X|W)
\end{equation}

Deux types de modèles probabilistes sont utilisés pour la recherche de la séquence de mots la plus probable : des modèles acoustiques qui fournissent la valeur de $P(X |W)$, et un modèle de langage qui fournit la valeur de $P(W)$. $P(X|W)$ peut se concevoir comme la probabilité d'observer $X$ lorsque $W$ est prononcée, alors que $P(W)$ se réfère à la probabilité que $W$ soit prononcée dans un langage donné. La difficulté pour obtenir un système de RAP performant est de définir les modèles les plus pertinents possibles pour le calcul de $P(W)$ et $P(X|W)$ (voir figure \ref{fig:principes_srap}).

\subsection{Modèles acoustiques}

Il est impossible pour un système de transcription d'exploiter le signal de la parole directement. En effet il contient bon nombre d'informations qui ne sont pas utiles au décodage de la parole (mais qui le seront pour la reconnaissance du locuteur, cf \ref{ssub:decoupage_regroupement}) et qui peuvent le parasiter : des informations sur le locuteur, sur les conditions d'enregistrement, etc. Le signal de la parole étant très variable et redondant, il nécessite de toute façon des traitements spécifiques avant de pouvoir être exploité. Ces traitements (appelés paramétrisation) vont consister à extraire du signal les paramètres qui sont dépendants de la parole prononcée.

Plusieurs techniques de paramétrisation existent, les plus utilisées sont :
\begin{itemize}
  \item PLP (\emph{Perceptual Linear Prediction}) : domaine spectral \cite{Hermansky91}
  \item LPCC (\emph{Linear Prediction Cepstral Coefficient}) : domaine temporel \cite{Markel1982}
  \item MFCC (\emph{Mel Frequency Cepstral Coefficients}) : domaine cepstral \cite{Bridle74,Mermelstein76}
\end{itemize}

Le signal de la parole est modélisable par un ensemble d'unités acoustiques, qui peuvent être considérées comme des sons élémentaires de la langue. Classiquement, l'unité choisie est le phonème. Le phonème est la plus petite unité discrète ou distinctive d'un mot, auquel elle peut faire perdre son sens par substitution. Par exemple, \emph{"chou"} et \emph{"pou"} se distinguent par leurs phonèmes initiaux. Dans le cadre des systèmes de transcription de la parole actuels, ces unités acoustiques sont modélisées par des Modèles de Markov Cachés (MMC) \cite{Rabiner89}. 

L'apprentissage de ces modèles acoustiques s'effectue grâce a des données \emph{a priori} annotées. Cet apprentissage permet d'obtenir une modélisation du message de la parole. Différentes techniques d'apprentissage et d'adaptation sont utilisées, parmi les plus connues figurent notamment :

\begin{itemize}
  \item l'apprentissage par maximum de vraisemblance (\emph{Maximum Likelihood : ML}), 
  \item l'algorithme EM \cite{Dempster77maximumlikelihood} (\emph{Expectation/Maximisation}), 
  \item l'estimation par information mutuelle maximale \cite{Valtech97} (\emph{Maximum Mutual Information Estimation : MMIE}), 
  \item l'adaptation par maximum \emph{a posteriori} \cite{Gauvain94maximuma} (\emph{Maximum \emph{a posteriori} probability : MAP}), 
  \item et l'adaptation par régression linéaire \cite{Gales98maximumlikelihood} (\emph{Maximum Likelihood Linear Regression : MLLR}).
\end{itemize}

\subsection{Modèles de langage}

Les systèmes de transcription ont besoin de contraintes linguistiques propres à la langue à décoder. Ces contraintes vont êtres utilisées par le système pour guider le décodage dans la sélection d'hypothèses acoustiques concurrentielles. Ces contraintes sont introduites par les modèles de langage dans les systèmes de transcription. Ces modèles sont des modèles probabilistes, et la probabilité d'apparition de la séquence de mots $W$ s'exprime de la façon suivante :

\begin{equation}
P(W^k_1) = P(w_1) \prod_{i=2}^{k} P(w_i|h_i)
\label{eq4}
\end{equation}

Où $h_i$ correspond à l'historique du mot $w_i$. On a donc $h_i = w_1, ..., w_{i-1}$.

Les modèles utilisés en RAP sont des modèles de langage de type \textit{n-gramme} \cite{Jelinek1976}. Bien qu'ancien, ce type de modèle constitue toujours l'état de l'art.

Les modèles de ce type correspondent à une modélisation stochastique du langage, où l'historique d'un mot est représenté par les $n-1$ mots qui le précèdent :

\begin{equation}
P(W^k_1) = P(w_1) \prod_{i=2}^{n-1} P(w_i|w_1, ..., w_{i-1}) \prod_{i=n}^{k} P(w_i|w_{i-n+1}, ..., w_{i-1})
\label{eq5}
\end{equation}

Les modèles couramment utilisés dans les SRAP sont généralement des modèles d'ordre 3 ou 4 ($n=3$ ou $n=4$). Nous parlons de modèle \textit{trigramme} ou \textit{quadrigramme} (pour $n=1$ \textit{unigramme}, pour $n=2$ \textit{bigramme}, ...).
Dans le cas d'un modèle quadrigramme, l'équation \ref{eq5} s'écrit :

\begin{equation}
P(W^k_1) = P(w_1) P(w_2|w_1) P(w_3|w_1,w_2) \prod_{i=4}^{k} P(w_i|w_{i-3}w_{i-2}w_{i-1})
\label{eq6}
\end{equation}

Les modèles de langage $n-grammes$ ont la particularité d'être assez souples, puisqu'ils permettent de modéliser des phrases grammaticalement incorrectes. En revanche, ils ne s'interdisent pas non plus de produire des phrases dénuées de tout sens.
En effet, ces modèles de langage sont capables d'attribuer une probabilité à des mots inconnus. Le système est donc capable de probabiliser des phrases qu'il n'a jamais observé dans son corpus d'apprentissage, tout en privilégiant les séquences de mots les plus fréquemment observées. 

\section{Détection des entités nommées}
\label{ssec:EN}

%Avec l'avènement d'Internet, la quantité de données textuelles à disposition augmente de jour en jour de façon exponentielle. Comme c'est le cas pour les données audio, ces données sont, à cause de leur quantité, impossible à indexer efficacement par un humain. L'extraction d'informations et le traitement de ces données est pourtant capitale pour bon nombre de scientifiques et d'industriels. 

Les transcriptions produites par les SRAP contiennent un grand nombre d'informations qui sont très coûteuses à indexer manuellement (l'émission, le sujet traité, les personnes concernées, etc.). Ces informations peuvent être extraites automatiquement pour venir enrichir la transcription grâce à des techniques issues du Traitement Automatique des Langues Naturelles (TALN). Le TALN est un domaine de recherche qui s'intéresse au traitement de corpus de texte. Son domaine de recherche est vaste mais on peut notamment citer plusieurs domaines d'application dans lequel le TALN intervient : 

\begin{itemize}
  \item aide à la rédaction (corrections, résumés, génération automatique de textes),
  \item traduction,
  \item recherche documentaire (questions/réponses, extraction d'informations),
  \item dialogue homme-machine,
  \item traitement de la parole.
\end{itemize}

Dans le but d'enrichir une transcription enrichie, une des étapes est généralement de détecter les entités nommées. Ces entités nommées vont permettre d'extraire un certain nombre d'information des transcriptions, comme les noms de personne, les lieux, les noms d'organisations, etc.

\subsection{Catégorisation}

Par la richesse des informations qu'elles contiennent, les entités nommées (EN) sont des éléments très importants pour les systèmes d'extraction d'information. Bien qu'il n'existe pas de définition précise de ce qu'est une EN, elle est souvent apparentée aux \og noms propres \fg{}, mais pas seulement. Dans les années 1980, les campagnes d'évaluation MUC ont permis de définir ce qu'était une tâche de reconnaissance des entités nommées. Pour MUC-6 \cite{MUC6,MUC6results}, les EN sont les noms propres, les acronymes et éventuellement d'autres mots qui rentrent dans les catégories suivantes :

\begin{itemize}
  \item \emph{Organisation} : regroupe les entreprises, les institutions gouvernementales et les autres organisations ;
  \item \emph{Person} : regroupe les noms de personnes ou de familles ;
  \item \emph{Location} : regroupe les noms de lieux politiquement ou géographiquement définis (villes, pays, régions, etc.).
\end{itemize}

Il existe des typologies des EN beaucoup plus complètes que celle des campagnes MUC comme celle de Paik et al. \cite{Paik96} ou de Coates-Stephens \cite{Coates92}. Par exemple, celle de Coates-Stephens définit 8 catégories d'EN :
\begin{itemize}
  \item les noms de personnes ;
  \item les noms de lieux ;
  \item les noms d'organisations ;
  \item les noms d'origines (noms d'habitants de pays, de villes, de régions, etc.) ;
  \item les noms de législations (\emph{loi Évin}), d'indices boursiers (\emph{Nikkei, CAC 40, Dow Jones}) ;
  \item les noms d'événements (guerres, révolutions, catastrophes, salons, JO, etc.) ;
  \item les noms d'objets.
  
\end{itemize}

\subsection{Les différents types de systèmes}

Selon Poibeau \cite{Poibeau2002} et Sekine et Eriguchi \cite{Sekine2000}, les systèmes de détection des EN peuvent êtres classés en trois grands types :\\

\textbf{Les systèmes fondés sur des règles écrites \og à la main \fg{}}\\
Dans ces systèmes, le concepteur doit élaborer un ensemble de règles qui seront ensuite utilisées pour détecter les entités nommées. Historiquement, cette technique fût la première utilisée dans le domaine. Bien que depuis la campagne d'évaluation MUC-6, l'apprentissage automatique ait fait son apparition dans ce domaine, les systèmes à base de règles écrites \og à la main \fg{} restent encore très utilisés aujourd'hui.\\

\textbf{Les systèmes à base d'apprentissage automatique}\\
Ces systèmes utilisent des techniques d'apprentissage automatique pour apprendre un modèle capable d'étiqueter des entités nommées à partir d'un corpus d'apprentissage. Ces travaux sont largement inspirés des techniques utilisées en reconnaissance automatique de la parole, et utilisent diverses techniques d'apprentissage : modèles de Markov cachés, modèle d'entropie maximum, arbres de décision, etc. Le système (LIA\_NE) développé par Béchet au Laboratoire d'Informatique d'Avignon (LIA) pour ESTER2 \cite{Ester2} fait partie de ce type de systèmes.\\ 

\textbf{Les systèmes mixtes}\\
Ces systèmes utilisent généralement des lexiques initiaux. Parmi ces systèmes, Poibeau \cite{Poibeau2002} distingue deux approches. La première consiste à apprendre automatiquement des règles, puis à utiliser un expert pour les réviser. Dans la seconde, un ensemble de règles de base est constitué par le concepteur, puis étendu (semi-)automatiquement par inférence, afin d'obtenir une meilleur couverture. C'est le cas de Nemesis, le système développé par Fourour \cite{Fourour04} au Laboratoire d'Informatique de Nantes Atlantique (LINA).\\



\section{Reconnaissance automatique du locuteur}
\label{sec:ral}

La Reconnaissance Automatique du Locuteur (RAL) regroupe un ensemble de méthodes capables d'extraire les caractéristiques vocales propres à chaque individu à partir d'un document audio. La RAL joue un rôle important dans l'enrichissement d'une transcription, puisqu'elle va permettre d'apporter des informations sur les différentes locuteurs du document.
  

\subsection{Caractéristiques et variabilité}
\label{ssec:carac_variabilite}

Toutes les techniques de RAL se basent sur l'extraction des caractéristiques (\emph{features}) du signal audio \cite{Atal76,Doddington85,hollien_acoustics_of_crime90,oshaughnessy86,Sambur75}. Ces caractéristiques doivent être robustes par rapport aux conditions d'enregistrement et fournir le plus de renseignements possibles concernant l'identité du locuteur.

Une question importante est celle de la variabilité des caractéristiques mesurées. On parle de variabilité intra-locuteur lorsque l'on s'intéresse à la variation des paramètres mesurés pour un même locuteur, et de variabilité inter-locuteur si on considère des locuteurs différents \cite{Doddington85,hollien_acoustics_of_crime90,Rosenberg91}. Pour la reconnaissance du locuteur, on cherche à extraire des caractéristiques du signal de parole qui présentent une forte variabilité inter-locuteur (pour pouvoir différencier les locuteurs entre eux) et une faible variabilité intra-locuteur (pour garantir la robustesse du système).

\subsection{Applications}

Des serveurs vocaux aux terminaux mobiles en passant par les dispositifs de sécurité, la RAL est utilisée dans de nombreux domaines. Certains serveurs vocaux l'utilisent notamment pour détecter le genre du locuteur qui s'exprime. Par exemple, grâce à la plateforme MISTRAL, la société Calistel \cite{MeignierMistral} propose de router les appels provenant de locuteurs masculins vers des opératrices féminines et inversement, afin d'obtenir un taux de satisfaction des appels plus important. Mais les applications de la RAL les plus connues sont certainement celles qui tournent autour de la biométrie. 

Dans les applications biométriques, un système de RAL va essayer de reconnaître, grâce à sa voix, un locuteur qui cherche à s'identifier au prêt d'un terminal. Même si ces systèmes donnent de bons résultats, ils ne sont pas parfaits. Les systèmes à l'état de l'art comme MISTRAL ont un taux d'erreur aux alentours de 5\% \cite{MeignierMistral}. Ils ne peuvent donc pas être considérés comme aussi sûr que les empreintes digitales et doivent être utilisés avec prudence à des fins judiciaires par exemple \cite{Boe99, Bonastre2003}. En revanche, avec l'arrivée des terminaux mobiles dans la vie de tous les jours, la RAL s'invite avec succès dans les téléphones portables de dernière génération \cite{Larcher_JEP10}.


L'utilisation de la RAL dans les applications que nous venons de citer peut se découper en quatre grandes catégories décrites ci-dessous. Ces catégories sont d'ailleurs sujet à évaluation dans les différentes campagnes comme NIST\cite{Alvin04nistspeaker} et ESTER \cite{Gravier04}.

%En plus de la difficulté à différencier deux locuteurs différents, la RAL est aussi confrontée à des problèmes intra-locuteurs. En effet, la voix d'un locuteur peut fortement varier d'un enregistrement à l'autre. Le niveau de fatigue, l'émotion, le contexte d'enregistrement ou encore l'âge d'un locuteur sont autant de variations qui peuvent venir perturber la RAL. Mais la biométrie n'est pas la seule application possible, de par ses caractéristiques, la RAL est utilisée dans bien d'autres domaines.

\subsection{Identification automatique du locuteur}

L'identification automatique du locuteur (IAL) a été une des premières utilisations de la RAL \cite{Atal76}. En IAL, la liste des locuteurs à identifier est connue du système. Le système doit pouvoir décider, à partir d'un échantillon de voix, à quelle identité connue du système correspond l'échantillon. La figure \ref{fig:principe_ial} décrit le principe général de l'IAL.

L'identification automatique du locuteur se découpe en deux étapes : une étape d'apprentissage et une étape de test. À partir d'un ensemble d'enregistrements de voix de chaque locuteur, le système apprend un modèle pour chaque locuteur lors de l'étape d'apprentissage. Lors de l'étape de test, le système confrontera l'échantillon de voix qu'il recevra aux différents modèles qu'il aura déjà appris afin de déterminer si l'identité du locuteur est déjà connue.

En fonction de l'application, deux types de décisions sont possibles. Prenons le cas d'un centre d'appel téléphonique qui enregistre les conversations de ses employés. Un système de IAL peut être utilisé pour classer chacun des enregistrements en fonction de l'employé concerné. Dans ce cas, la liste des locuteurs possibles est connue du système (tous les employés), et aucun autre locuteur non employé ne peut être concerné. Dans ce cas, l'application présuppose que l'ensemble des locuteurs possible est fermé, et connu du système. Le système de IAL choisira alors, parmi la liste, le modèle de locuteur le plus ressemblant à l'enregistrement de test.  

En revanche, dans une application utilisant un ensemble ouvert de locuteurs possibles, le système de IAL ne connait pas tous les locuteurs possibles. Dans ce cas, en plus de déterminer le locuteur le plus vraisemblable, le système a la possibilité de rejeter l'échantillon de test en ne renvoyant aucune identité connue pour cet échantillon. 

\begin{figure}[h]

\begin{center}
\includegraphics[width=1.0\columnwidth]{img/ial}

\caption{Principe de l'identification automatique du locuteur}
\label{fig:principe_ial}
\end{center}

\end{figure}

\subsection{Vérification automatique du locuteur}

La vérification automatique du locuteur (VAL) permet de décider si l'identité revendiquée par un locuteur est compatible avec sa voix. Il s'agit donc de trancher entre deux hypothèses : soit le locuteur est bien le locuteur autorisé (on l'appelle aussi locuteur client), c'est à dire que son identité correspond à celle qu'il revendique, soit le locuteur est un imposteur qui cherche à se faire passer pour la personne qu'il n'est pas. À partir d'un échantillon de voix de référence, et d'un échantillon de voix de test, le système va donc devoir dire si oui ou non les deux locuteurs correspondent \cite{Atal76}.

Les systèmes de VAL sont très dépendants des différences entre les échantillons de voix de référence et les échantillons de tests (cf. \ref{ssec:carac_variabilite}). Accepter un locuteur qui devrait être rejeté peut avoir de lourdes conséquences, en particulier dans les applications où un haut niveau de sécurité est demandé \cite{Bonastre2003} (contrôle aux frontières, système bancaire, identification judiciaire, etc.). 

\subsection{Suivi de locuteur}

\label{ssec:sl}

Le suivi de locuteur (SL) se base sur la tâche de vérification du locuteur. À partir d'un enregistrement de référence d'un locuteur, le système va devoir déterminer si le locuteur intervient dans le document. Si c'est le cas, il devra être capable de préciser où et quand ce locuteur intervient dans l'enregistrement. À noter qu'à part le locuteur à suivre, aucun autre locuteur du document n'est connu du système. Plusieurs méthodes différentes peuvent être employées pour réaliser cette tâche \cite{EURECOM269}, la figure \ref{fig:suivi_locu} propose un aperçu général du principe du SL.

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/suivi_locu}

\caption{Principe du suivi de locuteur}
\label{fig:suivi_locu}
\end{center}

\end{figure}

\subsection{Segmentation et classification en locuteur}

\label{ssub:decoupage_regroupement}

La tâche de segmentation et de classification en locuteur consiste à délimiter l'intervention de chaque locuteur dans le document audio. À la différence du suivi de locuteur, aucune information \emph{a priori} sur les locuteurs du document n'est disponible pour cette tâche. Plus précisément, aucune information sur le nombre de locuteurs n'est disponible, ni leur identité, ni des modèles de voix qui auraient pu être appris au préalable. 

Les travaux fondamentaux de la segmentation et de la classification en locuteur ont été réalisés au début des années 1990 par la société BBN sous la direction de H. Gish \cite{Siu1991,Siu1992}. Ces travaux consistent à indexer les différents échanges radio entre pilotes et contrôleurs aériens. Ces échanges sont enregistrés puis segmentés automatiquement. Ces enregistrements peuvent contenir plusieurs dialogues contrôleur/pilote. La segmentation est ensuite utilisée pour reconstruire ces dialogues.

Depuis, les techniques ont évoluées \cite{TranterReynolds2006} et le champ d'application de la segmentation en locuteurs s'est étendu, elle se retrouve intégrée dans le cadre plus vaste de l'indexation en locuteurs de bases de données de documents multimédia \cite{PhdMeignier}. Les documents traités sont divers et variés, on peut notamment citer les conversations téléphoniques, les enregistrements de journaux télévisés ou radiophoniques, les films ou encore les enregistrements de réunion.

La figure \ref{fig:seg_classif} décrit les principes du processus de segmentation et de classification qui s'effectue en plusieurs étapes :

\begin{itemize}
  \item La \textbf{paramétrisation} va extraire les caractéristiques acoustiques du document audio. Ces caractéristiques seront représentées via des vecteurs acoustiques qui pourront ensuite être exploités par les autres phases du processus.
  \item La \textbf{segmentation} va s'attacher à trouver des points de ruptures, des frontières, au sein de l'enregistrement. Ces frontières peuvent être de différentes natures : changement de locuteur, changement de canal de transmission (studio, téléphone), présence d'un long silence, musique, etc. Les portions de signal entre ces différentes frontières sont appelées segments. Ces segments contiennent des données homogènes : même locuteur, même canal de transmission.
  \item La \textbf{classification} groupe les segments locuteur par locuteur jusqu'à découvrir le nombre de classes correspondant au nombre de locuteurs intervenant dans le document. Chaque classe contient à la fin de la classification l'ensemble des segments prononcés par un locuteur. La classe définit alors l'intervention du locuteur dans le document.
  \item La \textbf{resegmentation} affine les frontières des segments. Dans cette étape, le document est de nouveau découpé en segments en fonction des données contenues dans les classes.
\end{itemize}

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.8\columnwidth]{img/segmentation-classification}

\caption{Principe de la segmentation et classification en locuteur}
\label{fig:seg_classif}
\end{center}

\end{figure}

À noter que les classes de locuteur produites sont anonymes. En effet, ce type de système étiquette chacune des classes avec un nom unique, mais dénué de tout sens. Le processus qui consiste à étiqueter ces classes avec le vrai couple prénom/nom du locuteur auquel elles se rapportent est appelé \textbf{identification nommée du locuteur}. C'est à cette tâche que s'intéressent les travaux de cette thèse. 


%Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.

%Dans un enregistrement audio, chaque prise de parole d'un locuteur est appelée \og tour de parole \fg. Ces tours de paroles commencent quand un locuteur prend la parole et s'arrêtent lorsqu'un autre locuteur prend la parole (ou lorsqu'un jingle ou une publicité coupe l'intervention). Ces locuteurs peuvent n'intervenir qu'une fois dans le fichier, auquel cas ils n'auront qu'un tour de parole. Mais, comme les journalistes ou les présentateurs, beaucoup de locuteurs sont présents tout au long du fichier et prennent la parole à plusieurs endroits dans l'enregistrement. Ces différents tours de paroles appartiennent au même locuteur, ils sont alors regroupés au sein de la même \og classe de locuteur \fg{}. La figure \ref{fig:decoupage_tours_classes} donne un exemple de découpage en tours de parole et de regroupement en classes de locuteurs.



%On peut noter sur la figure \ref{fig:decoupage_tours_classes}, qu'en plus de regrouper les tours de parole par classe de locuteur (classe du Locuteur 1, classe du Locuteur 2), le genre du locuteur est aussi mentionné. Cette information vient enrichir la transcription et pourra être utile par la suite. Les classes de locuteur sont pour l'instant anonymes. En effet, la transcription ne nous permet pas de dire que c'est \og Monsieur untel \fg{} qui parle, elle permet juste de dire que c'est le même locuteur qui parle à deux endroits différents. La solution idéale serait de pouvoir nommer les classes de locuteurs par le prénom et le patronyme du locuteur concerné (couple que nous appellerons par la suite nom complet), c'est ce que les techniques d'identification nommée du locuteur cherchent à réaliser.


%\chapter{Systèmes automatiques}
%\minitoc
%
%\label{sec:sys_auto}
%
%Depuis les débuts de l'informatique, l'Homme a toujours essayé de l'utiliser pour automatiser des tâches humaines. Que ce soit de la simple calculatrice à la reconnaissance de la parole, l'informatique est de nos jours utilisée dans tous les domaines. Mais pour pouvoir réaliser ces tâches, les programmes informatiques doivent avoir une certaine connaissance des actions à effectuer pour pouvoir les réaliser.
%
%Il existe deux principales façons d'insuffler ces connaissances à un programme. La première est de tout simplement lui donner les règles qui vont lui permettre de réaliser les tâches qu'il a à accomplir. Dans l'exemple de la calculatrice, il suffit de lui fournir les différentes formules arithmétiques pour qu'elle puisse fonctionner. Ce type de procédé marche bien lorsqu'il existe des règles formelles pour réaliser une tâche. Mais ces règles ne sont pas toujours disponibles ou peuvent varier fortement en fonction du contexte, c'est par exemple le cas en reconnaissance de la parole.
%
%La reconnaissance de la parole fait intervenir plusieurs domaines complexes à gérer pour un système automatique. Il faut tout d'abord transformer le signal audio en paramètres exploitables par un programme informatique. Il faut ensuite transformer ce signal audio en phonèmes, puis les phonèmes en mots et les mots en phrases. Ces étapes dépendent de beaucoup de paramètres comme les conditions d'enregistrement, la langue utilisée dans le document, l'accent des personnes qui parlent ou leur façon d'utiliser la langue (néologismes, mauvaises constructions grammaticales). En plus de variabilité de ces paramètres, il est impossible de définir des règles formelles pour encadrer chacun d'eux.
%
%Prenons l'exemple d'un enregistrement dans lequel les locuteurs parlent français. Même s'il existe une grammaire et un orthographe bien définis pour la langue française écrite, ce n'est pas le cas à l'oral. En effet les contractions (cheval sera prononcé ch'val), les faux départs et les répétitions sont légions. En plus de ces différences entre le français écrit et le français parlé, les locuteurs peuvent très bien faire des fautes de grammaire ou employer des mots qui ne font pas parti du dictionnaire courant. Toutes ces variations dans l'utilisation de la langue française rendent difficile l'utilisation de règles formelles pour reconnaître les mots prononcés. C'est pour pallier à ces problèmes que les systèmes de reconnaissance automatique de la parole utilisent des techniques d'apprentissage automatique qui vont permettre au système d'apprendre automatiquement de nouvelles connaissances en fonction des données qui lui seront fournies.
%
%\section{Apprentissage automatique}
%\subsection{Généralités}
%
%L'apprentissage automatique (qui est un sous-domaine de l'Intelligence Artificielle) a pour but de permettre à une machine, un programme, de s'adapter et d'évoluer grâce aux données qui lui sont fournies, de manière à remplir des tâches complexes qu'il serait difficile, voir impossible, de remplir avec des méthodes plus traditionnelles. Un des défis de l'apprentissage automatique est d'extraire les données pertinentes présentes dans les corpus d'apprentissage afin d'en tirer des généralités applicables à d'autres données. L'apprentissage automatique repose sur la notion de classification. La classification va viser à étiqueter chaque donnée en l'associant à une classe. Ces classes peuvent être connues à l'avance ou déterminées automatiquement par le système. C'est ce qui va différencier l'apprentissage supervisé de l'apprentissage non supervisé. 
%
%\subsection{Apprentissage supervisé}
%\label{sssec:apprentissage_supervise}
%L'apprentissage supervisé va permettre d'apprendre des règles à partir de données d'apprentissage servant d'exemple. Ces données sont généralement validées et étiquetées par des êtres humains. C'est à partir de ces exemples déjà classés que le système va apprendre des règles essayant de reproduire la classification à partir des données. Par exemple, les arbres de décision décrits dans \ref{sec:sct} sont des outils utilisant un apprentissage supervisé, puisqu'ils vont permettent de ranger des données de test dans des classes prédéfinies. Ils auront été appris grâce à des données d'apprentissage étiquetées au préalable avec les bonnes classes.
%
%\subsection{Apprentissage non supervisé}
%
%L'apprentissage non supervisé (aussi appelé classification automatique) consiste à trier un groupe hétérogène de données en regroupant les données homogènes au sein d'une même classe, et les données hétérogènes au sein d'autres classes. À la différence de l'apprentissage supervisé, il n'y a pas de \og sortie attendue \fg{}. C'est à dire que c'est au système d'établir ses propres classes, au lieu de devoir classer les données dans des classes prédéfinies comme pour l'apprentissage supervisé. En reconnaissance automatique de la parole, et plus précisément dans le domaine de la reconnaissance du locuteur, les modèles à mélange de gaussiennes (aussi appelés GMM) sont un bon exemple de système d'apprentissage non supervisé. Ils permettent de représenter une distribution multidimensionnelle quelconque par une somme pondérée de distributions gaussiennes. Chaque distribution gaussienne étant caractérisée par un vecteur moyen et une matrice de covariance.

%\subsection{Transcription enrichie}
%\label{sub:transcription_enrichie}

%Transcrire un signal audio consiste d'une part à retranscrire les mots qui ont été prononcés mais aussi à enrichir cette transcription avec différentes informations éventuellement disponibles comme le début et la fin de chaque intervention, le nom des différents locuteurs ou encore leur genre. Ces transcriptions peuvent être réalisées par un humain (elles seront appelées transcriptions manuelles) ou de manière entièrement automatique (elles seront appelés transcriptions automatiques) : en fonction de cela, les informations disponibles dans la transcription peuvent varier, c'est notamment le cas du nom des locuteurs. En effet, un humain pourra essayer, en fonction du contexte, de nommer les locuteurs de l'enregistrement à l'inverse d'un système automatique qui utilisera des étiquettes anonymes en guise de nom de locuteur (comme locu1, locu2, ...). C'est précisément à cet aspect de la transcription enrichie (à savoir le nommage des locuteurs) que les travaux sur l'identification nommée du locuteur s'intéressent. Il convient tout d'abord d'expliquer plus précisément les différents aspects d'une transcription enrichie.



% \subsubsection{Transcription et étiquetage des entités nommées}
% 
%   La transcription du signal audio consiste tout simplement à écrire les mots qui ont été prononcés par les locuteurs. Que cette transcription soit réalisée par un humain ou par un système automatique de reconnaissance de la parole (RAP), elle consiste à remplir les différents tours de parole avec les mots correspondants. QUand les transcriptions sont réalisées par des systèmes de RAP, elles sont beaucoup moins riches que celles produites par un humain. En effet, en plus des erreurs de transcription, elles ne contiennent généralement pas de ponctuation. Ce manque peut être comblé par des post-traitements qui essaient de remettre la ponctuation dans la transcription. 
% 
% Un autre type de post-traitement utile pour l'identification nommée du locuteur est la détection des entités nommées. En effet, pour pouvoir attribuer un nom complet à un locuteur à partir de la transcription, il va d'abord falloir être capable de détecter ce nom complet dans la transcription. Pour ce faire, il existe des systèmes de détection des entités nommées qui vont être capable de détecter, en plus des noms complets, différentes entités nommées comme les lieux, les organisations ou encore les radios. La figure \ref{fig:etapes_transcription_enrichie} montre toutes les étapes pour obtenir une transcription enrichie en entités nommées.


% \subsection{Les systèmes automatiques : reconnaissance de la parole et détection des entités nommées}
% 
% À l'heure où l'informatique et Internet sont partout, les quantités de données numériques ne cessent de croître. Ces grandes collections de données sont difficilement indexables manuellement, il faut donc, pour faciliter la recherche et l'accès à l'information, qu'elles soient traitées de manière automatique. Les enregistrements audio sont traités à l'aide de systèmes de reconnaissance automatique de la parole (SRAP), ces systèmes sont constitués de plusieurs composantes comment le montre la figure \ref{fig:principes_srap}.
% 
% \begin{figure}
% 
% \begin{center}
% \includegraphics[width=0.9\columnwidth]{img/sys_rap}
% 
% \caption{Principes généraux d'un système de Reconnaissance Automatique de la Parole (SRAP)}
% \label{fig:principes_srap}
% \end{center}
% 
% \end{figure}
% 
% \subsubsection{Principes généraux} % (fold)
% \label{ssub:principes_generaux}



% subsubsection principes_généraux (end)

% \subsubsection{Segmentation et classification} % (fold)
% \label{ssub:les_systemes_de_segmentation_et_de_classification_en_locuteurs}
% 
% Afin de traiter de manière efficace un signal complexe comme le signal audio, les SRAP ont besoin de segmenter le signal en parties homogènes appelées segments. Ces segments se doivent d'être cohérents, un même segment doit avoir les mêmes conditions d'enregistrement, doit être prononcé par le même locuteur, ... Ces segments sont caractérisés par des conditions acoustiques spécifiques comme la présence de parole, la nature de la parole (téléphonique ou enregistrement studio), la présence de musique, le genre du locuteur ou encore son identité ou une étiquette anonyme. Comme décrit dans \ref{ssub:decoupage_regroupement}, l'identification nommée utilise les tours de parole. Un tour de parole n'est en fait qu'une suite de segments dont l'identité ou l'étiquette sont identiques.
% 
% 
% Toutes ces informations sont utilisées par le système de reconnaissance de la parole de manière à choisir la manière la plus adéquat de traiter chaque type de segment. Le défi réside dans l'apprentissage de modèles acoustiques robustes vis-à-vis des différentes situations acoustiques auxquelles il sera confronté. Pour réaliser cette segmentation, le système va découper le signal en trames (environ 25 ms chacune) puis va étiqueter ces trames en fonction des différentes informations qu'il est capable d'extraire. Le principe général va ensuite consister à calculer des distances entre ces trames de manière à savoir si elles doivent être regroupées au sein du même segment ou pas. Ce principe de calcul de distance sera réutilisé pour regrouper tous les segments d'un enregistrement au sein de la même classe de locuteur.
% 
% 
% Les méthodes qui donnent les meilleurs résultats sur les journaux radiophoniques utilisent un regroupement BIC suivi d'un regroupement de type CLR [1, 2]. C'est ce type de système \ref{article sylvain} qui sera utilisé pour les travaux du présent papier.
% 
% Ces méthodes sont évaluées selon une métrique nommée DER (Diarization Error Rate). TODO : expliquer et donner la formule

% subsubsection les_systèmes_de_segmentation_et_de_classification_en_locuteurs (end)

% \subsubsection{Transcription automatique de la parole} % (fold)
% \label{ssub:transcription_automatique_de_la_parole}
% 
% Les systèmes de transcription automatique de la parole sont évalués selon une métrique nommée WER (Word Error Rate). TODO : expliquer et donner la formule

% subsubsection transcription_automatique_de_la_parole (end)


% \subsubsection{Détection des entités nommées} % (fold)
% \label{ssub:detection_des_entites_nommees}

% subsubsection détection_des_entités_nommées (end)




\chapter{L'identification nommée du locuteur}
\minitoc
\newpage

L'identification nommée du locuteur (INL) consiste à attribuer un prénom et un patronyme à chaque locuteur d'un document audio. Ces informations viennent enrichir la transcription et peuvent être utilisées dans plusieurs domaines d'applications. 

\section{Applications}

Disposer des noms complets des intervenants d'un document audio peut être utilisé à des fins de recherche d'informations. En effet, les bases de données audio et vidéo ne cessent de croître avec la numérisation massive des documents audiovisuels. Ainsi, les besoins en indexation et recherche automatiques croissent de la même manière. Par exemple en France, l'Institut National de l'Audiovisuel (INA) a lancé un plan de sauvegarde et de numérisation de ses différents documents en 1999. Ce plan a pour but de numériser plusieurs millions d'heures d'enregistrements de radio et de télévision. Il doit permettre de ne pas perdre les données qu'elles contiennent à cause de l'obsolescence des supports. Suite au lancement de ce plan de sauvegarde, de nombreux travaux ont été menés à l'INA sur l'indexation automatique des documents \cite{Veneau2001,Allauzen2003,Joly2004,Poli2007}. L'utilisation de l'INL pour ce type de bases de données pourrait apporter un réel plus pour l'indexation. Elle permettrait par exemple de faciliter la recherche des différents intervenants dans les documents indexés ou encore de remplir automatiquement les grilles de programmes avec le nom des intervenants.

Les récentes campagnes d'évaluation des systèmes de reconnaissance automatique de la parole (cf. \ref{ssec:trans_broadcast}) s'intéressent à l'enrichissement de la transcription (cf. \ref{sec:trans_enrichie}) par diverses informations comme les entités nommées ou encore les informations sur les locuteurs du document. Cette transcription contient notamment une segmentation et une classification en locuteurs (cf. \ref{ssub:decoupage_regroupement}) qui permettent de délimiter les interventions des différents locuteurs dans l'enregistrement. En revanche, les informations fournies par le processus de segmentation et de classification en locuteur ne permettent pas d'identifier les locuteurs du document par leur prénom et patronyme. Les locuteurs sont uniquement identifiés par des labels anonymes du type \emph{Locuteur1}, \emph{Locuteur2}. L'INL s'inscrit parfaitement dans l'enrichissement de la transcription en permettant de remplacer ces labels anonymes par la \og vraie identité \fg{} des locuteurs. On entend par \og vraie identité \fg{} du locuteur son prénom et son patronyme. Ce couple prénom/patronyme sera par la suite désigné comme étant le \textbf{nom complet} du locuteur.

Afin de pouvoir attribuer aux locuteurs d'un document leurs nom complets, il faut obtenir leurs prénoms et patronymes. Deux principales approches sont possibles : disposer d'informations a priori sur les locuteurs comme dans le cas du suivi de locuteur (cf. \ref{ssec:sl}) ou utiliser les informations fournies par le document lui même pour déterminer l'identité des locuteurs. Nous commençons par présenter la première approche avant de nous intéresser plus longuement à la deuxième approche qui constitue le coeur de cette thèse.

%Contrairement à la reconnaissance de la parole et aux tâches de reconnaissance du locuteur présentées dans le chapitre précédent, l'identification nommée de locuteurs est un domaine de recherche relativement récent. En effet, ce sont les travaux du LIMSI et plus particulièrement de Leonardo Canseco-Rodriguez \cite{CansecoRodriguez2005}, qui ont été les premiers à s'intéresser à l'identification nommée du locuteur utilisant la transcription enrichie (réalisée manuellement) d'enregistrements radiophoniques.

\section{Utilisation de connaissances a priori}
\label{sub:suivi_locuteur}

Obtenir l'identité d'un locuteur a d'abord été réalisé avec des méthodes purement acoustiques. Le suivi de locuteur (décrit dans le chapitre \ref{sec:ral}) peut être étendu à $n$ locuteurs afin d'identifier tous les locuteurs d'un document. Bien qu'utilisées depuis des années, ces méthodes souffrent de plusieurs inconvénients. 

Tout d'abord, il est absolument nécessaire de connaître les personnes que l'on cherche à identifier. En effet, les systèmes doivent commencer par apprendre des modèles acoustiques correspondant à chaque locuteur du document pour ensuite les associer aux différents intervenants de l'enregistrement traité. Ils ne peuvent donc identifier que des personnes dont ils possèdent déjà les modèles. De plus, lorsque l'on peut apprendre ce modèle acoustique, il faut posséder une quantité de données suffisante pour pouvoir l'apprendre : plusieurs minutes sont un minimum.

Pour finir, il faut que les conditions acoustiques des données d'apprentissage soient similaires à celles sur lesquelles le système va chercher à détecter les locuteurs : des données trop éloignées dans le temps (et donc avec une voix qui peut avoir changée) ou des conditions d'enregistrement différentes (studio ou téléphone par exemple) dégraderont les performances d'identification (cf \ref{ssec:carac_variabilite}).

Dans l'hypothèse d'une utilisation dans les récentes campagnes d'évaluation traitant de journaux radiophoniques (comme ESTER ou les campagnes NIST), il faudrait disposer des enregistrements de tous les locuteurs (présentateurs, invités, intervenants, etc.) en quantité suffisante. S'il semble plausible de pouvoir les obtenir pour les journalistes qui sont souvent présents dans les émissions de radios, il semble beaucoup plus compliquer de les récupérer pour certains invités peu connus ou pour les inconnus interrogés par téléphone. 

Ces différentes limitations ont motivé les récents travaux ne nécessitant pas de connaissances a priori sur les locuteurs pour réaliser l'identification. Ces travaux se basent sur la transcription du signal audio pour identifier les locuteurs d'un enregistrement. 

\section{Utilisation des informations de la transcription}

Puisque le but de ces méthodes est de n'utiliser aucune connaissance a priori sur le document traité, elles doivent extraire les noms complets à partir des informations disponibles dans le fichier audio. La transcription en mots du signal est une source d'informations de choix pour trouver les prénoms et les patronymes des locuteurs : dans beaucoup d'enregistrements (et notamment ceux provenant des journaux radiophoniques) les noms complets des intervenants font partie de la transcription. L'identification nommée consiste donc à détecter les noms complets présents dans la transcription, pour ensuite les attribuer aux différents locuteurs du document. 

La figure \ref{fig:principe_inl} donne un aperçu de ce que l'identification nommée à partir de la transcription cherche à réaliser. 

\begin{figure}[h]

\begin{center}
\includegraphics[width=0.9\columnwidth]{img/discussion_nommee}

\caption{Aperçu global de l'identification nommée du locuteur utilisant la transcription}
\label{fig:principe_inl}
\end{center}

\end{figure}


\subsection{Présence des noms complets}

À partir du moment où les noms complets des locuteurs sont cités dans le document lui même, il semble possible de les exploiter pour identifier les différents intervenants. C'est l'hypothèse de base de toutes les méthodes utilisant la transcription pour réaliser l'identification nommée : les noms complets des locuteurs du document sont présents dans le document lui même.

Même si cette hypothèse est très forte, elle permet d'exploiter un grand nombre de documents. Les journaux d'informations (qu'ils soient radiophoniques ou télévisuels) sont par exemple tout à fait adaptés à ce style de techniques. Dans ce type de document, la majorité des locuteurs s'annoncent ou sont annoncés. C'est systématiquement le cas des présentateurs et des invités par exemple. Dans les enregistrements de réunions, les locuteurs sont présentés et se passent la parole : il est donc possible de récupérer leurs noms complets. De manière plus générale, tout type de document où les locuteurs s'annoncent ou sont annoncés vont pouvoir être exploités avec les techniques d'identification nommée basées sur la transcription du signal audio.

Disposer de documents dans lesquels les noms complets des locuteurs sont cités n'est pas suffisant, il faut ensuite être capable d'attribuer ces noms complets aux différents intervenants du document. La partie suivante décrit le principe d'attribution commun à toutes les méthodes d'INL basées sur la transcription.


\subsection{Attribution des noms complets}

La deuxième hypothèse sur laquelle reposent les techniques utilisant la transcription est qu'il est possible d'exploiter les noms complets contenus dans la transcription pour identifier les locuteurs du document. La technique utilisée par tous les systèmes d'identification nommée à partir de la transcription est la même : attribuer un nom complet détecté dans le document au locuteur qui est en train de parler (appelé locuteur courant), au locuteur qui parle ensuite (appelé locuteur suivant) ou au locuteur qui vient de parler (appelé locuteur précédent). La figure \ref{fig:segment} résume le principe de cette attribution, que l'on doit au LIMSI et Leonardo-Canseco \cite{CansecoRodriguez2005}.


\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/segment.pdf}
\caption{Principe de base des systèmes d'identification nommée basés sur la transcription}
\label{fig:segment}
\end{center}
\end{figure}

Typiquement, un nom complet peut être attribué au locuteur courant lorsque celui-ci s'annonce ou se présente. Par exemple les journalistes ont pour habitude de finir leurs interventions en rappelant leur identité, comme par exemple \emph{\og c'était Paul Dupond en direct de \ldots \fg}. Ici, le nom complet \emph{Paul Dupond} devrait être attribué au locuteur qui a prononcé cette phrase (le locuteur courant). A l'inverse, un locuteur prononçant cette phrase \emph{\og Nous écoutons maintenant, Jean Durand.\fg{}} annonce une personne qui va parler après lui. Le nom complet \emph{Jean Durand} devra donc être attribué au locuteur suivant. Pour finir, une phrase du type \emph{\og Merci Maude Bayeu, nous passons maintenant à la météo \fg} rappelle le locuteur qui vient de s'exprimer juste avant. Le nom complet \emph{Maude Bayeu} sera donc attribué au locuteur précédent.


\subsection{Conflits d'attributions}

Ces techniques nécessitent de détecter les différents noms complets cités dans la transcription pour ensuite les attribuer aux locuteurs du document. Il faut donc disposer de la transcription du signal audio pour y détecter les différents noms complets. Cette transcription doit aussi être enrichie avec une segmentation et une classification en locuteurs, puisque ce sont à ces locuteurs qu'il faut attribuer les noms complets détectés dans la transcription pour les nommer. 
Comme le montre l'aperçu présenté sur la figure \ref{fig:principe_inl}, la première étape est de réaliser une transcription enrichie du signal audio. Mais, pour pouvoir être exploitée par les techniques d'identification nommée, cette transcription doit être préparée.

\subsubsection{Principes généraux} % (fold)
\label{ssub:principes_generaux}

Les méthodes d'identification nommée du locuteur à partir de la transcription sont multiples mais partagent toutes un principe commun. Du pré-traitement de la transcription, en passant par des décisions locales puis globales, les outils utilisés divergent mais le principe reste le même.

\section{Processus d'identification nommée}

\subsection{Préparation de la transcription} % (fold)
\label{ssub:enrichissement_de_la_transcription}

Bien que l'identification nommée se base sur des transcriptions, ces transcriptions doivent être enrichies (cf \ref{sub:transcription_enrichie}) et préparées avant de pouvoir les exploiter. En effet, dans le cadre de l'identification nommée, certains mots ont besoin d'être regroupés en catégories et remplacés dans la transcription. C'est le cas pour une bonne partie des entités nommées (lieux, radios, noms complets) qui ont été détectées dans la transcription. Ce pré-traitement a pour but de généraliser au maximum les règles qui seront utilisées par la suite. C'était  \og Maude Bayeu en direct de Paris\fg{} donnera par exemple \og C'était PERSONNE en direct de LIEU \fg{}. La figure \ref{fig:pre_traitement} illustre les pré-traitements nécessaires afin d'obtenir une transcription exploitable par un système d'identification nommée du locuteur.

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/pre_traitement.pdf}
\caption{Pré-traitement su signal audio}
\label{fig:pre_traitement}
\end{center}
\end{figure}

% subsubsection enrichissement_de_la_transcription (end)

\subsection{Étiquetage des nom complets et propagation} % (fold)
\label{ssub:etiquetage_des_nom_complets}

Lorsque la transcription est enrichie avec l'étiquetage des entités nommées qu'elle contient, il est possible de déterminer l'emplacement des différents nom complets cités dans l'enregistrement. La difficulté va être de réussir à déterminer si ces noms détectés sont des locuteurs de l'enregistrement ou non, et quand ils le sont à quelle partie de l'enregistrement ils se rapportent. Pour ce faire, les méthodes d'identification nommée attribuent des étiquettes à chaque nom complet détecté, ces étiquettes sont : \og précédent \fg{}, \og courant \fg{}, \og suivant \fg{} ou \og autre \fg{}. Ces étiquettes vont permettent de déterminer si le nom complet détecté dans la transcription désigne le locuteur s'exprimant dans le tour de parole précédent, dans le tour courant, dans le suivant ou dans aucun des tours contigus. Le dernier cas appelé \og autre \fg{} met en évidence une des limites de l'approche : que le nom complet détecté concerne un locuteur de l'enregistrement ou pas, ce locuteur doit parler dans les tours de parole contigus au tour de parole où son nom a été détecté. La figure \ref{fig:segment} décrit les différents cas de figures possibles.

À cette étape, les noms complets détectés sont propagés aux tours de parole contigus. On appellera cette propagation \og décision locale \fg{} par la suite.

% subsubsection Étiquetage_des_nom_complets (end)

\subsection{Attribution d'un nom complet à une classe anonyme} % (fold)
\label{ssub:attribution_d_un_nom_complet_a_une_classe_anonyme}

Attribuer un complet à une classe de locuteur anonyme revient à répercuter les décisions locales au sein du fichier tout entier, c'est à dire passer des tours de paroles aux classes de locuteur, avec tous les problèmes que cela implique. En effet, comme le montre la figure \ref{fig:decisions_locales_conflit}, il est tout à fait possible que plusieurs noms complets différents soient en conflit au sein d'un même tour de parole. Dans l'exemple, deux noms complets sont possibles pour le locuteur numéro 2 : Maude Bayeu et Pierre Moscovici.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/decision_multiple.pdf}
\caption{Exemple de décisions locales conflictuelles}
\label{fig:decisions_locales_conflit}
\end{center}
\end{figure}

Chaque tour de parole étant regroupé au sein d'une classe de locuteur (celle que l'on cherche à nommer), ces conflits vont se répercuter au sein de la classe de locuteur elle même. La figure \ref{fig:processus_entier} résume tous ces différents problèmes.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/entire_process.pdf}
\caption{Vue globale du processus attribution d'un complet à une classe de locuteur anonyme}
\label{fig:processus_entier}
\end{center}
\end{figure}

Même si les façons de réaliser les différentes étapes différent entre les méthodes d'identification nommée, les principes qui viennent d'être décrits sont communs à toutes. Le traitement d'un fichier audio avec un système d'identification nommée se résume donc aux étapes qui sont décrites dans la figure \ref{fig:fonctionnement_general}.

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{img/fonctionnement_general.pdf}
\caption{Aperçu global d'un processus d'identification nommée du locuteur}
\label{fig:fonctionnement_general}
\end{center}
\end{figure}


% subsubsection attribution_d_un_nom_complet_à_une_classe_anonyme (end)

\subsection{Métriques et évaluation}

\label{sec:metrique}

Un système d'identification nommée est évalué en comparant l'hypothèse générée par celui-ci à la référence distribuée avec le corpus. Cette comparaison met en évidence 5 cas d'erreur ou de succès possibles relatifs aux situations suivantes~:
\begin{itemize}
\item l'identité proposée est correcte ($C_1$)~: le système propose une identité correspondant à celle indiquée dans la référence ;
\item erreur de substitution ($S$)~: le système propose une identité différente de l'identité présente dans la référence ;
\item erreur de suppression ($D$) : le système ne propose pas d'identité alors que le locuteur est identifié dans la référence ;
\item erreur d'insertion ($I$)~: le système propose une identité alors que le locuteur n'est pas identifié dans la référence ;
\item il n'y a pas d'identité ($C_2$)~: le système ne propose pas d'identité et la référence ne contient pas d'identité.
\end{itemize}


Une mesure de Précision et de Rappel peut être définie à partir des 5 cas d'erreur~:
\begin{equation}
	\label{eq:PR}
	P = \frac{C_1}{C_1+S+I} \ \ ; \ \ R = \frac{C_1}{C_1+S+D}
\end{equation}

%Dans tous les précédents articles \cite{Tranter06,Esteve07,Chengyuan07} traitant de l'identification nommée du locuteur, les résultats sont présentés sous la forme de mesures de précision et de rappel.

Comme il a été proposé dans \cite{Tranter06}, ces valeurs peuvent être complétées par un taux d'erreur $Err$ global également calculé à partir de ces 5 erreurs. Ce taux s'inspire du calcul du WER utilisé pour l'évaluation de la transcription. Il a l'avantage de mesurer la qualité des résultats du système d'identification nommée en une seule valeur, facilitant les comparaisons entre les systèmes par rapport aux mesures de précision et de rappel.
\begin{equation}
	\label{eq:PR}
	Err = \frac{S+I+D}{S+I+D+C_2+C_1} \ \ ;
\end{equation}


Les erreurs peuvent être calculées en terme de durée ou en terme de nombre de locuteurs.
Pour une évaluation en durée, dans le cas où un locuteur parlant 90\% du temps est correctement nommé et que les six autres locuteurs parlant seulement 10\% du temps ne le sont pas, le système présentera un taux d'erreur de 10\%. 

Pour une évaluation en terme de nombre de locuteurs, dans le même cas de figure, le système aura un taux d'erreur de 87,5\%.

D'un point de vue applicatif, la métrique exprimée en durée est préférable si les locuteurs considérés comme importants correspondent aux locuteurs s'exprimant beaucoup. En revanche, si l'application cherche à nommer le plus possible de locuteurs, il est plus intéressant d'évaluer les performances en terme de nombre de locuteurs.

\section{Règles symboliques}
\label{sub:regles_manuelles}

 Les auteurs ont été les premiers à montrer que le couple prénom/patronyme d'un locuteur apparaissant dans un contexte lexical donné permettaient d'identifier de manière précise l'identité des locuteurs s'exprimant dans les tours de parole proches. Leur méthode repose sur l'utilisation de règles affectant les étiquettes \textit{\og tour courant \fg}, \textit{\og tour précédent \fg}, \textit{\og tour suivant \fg } aux noms complets détectés. Ces étiquettes ont été reprises dans l'ensemble des travaux réalisés sur l'identification nommée depuis. Les règles utilisées ont été définies manuellement après analyse d'un corpus de langue anglaise. Douze règles sont utilisées pour désigner le locuteur courant, 34 pour le suivant et 6 pour le précédent.


Dans le but de généraliser les règles, les mots pouvant être regroupés au sein d'une même classe sémantique, sont remplacés par cette classe dans la transcription. Les différentes classes sémantiques retenues sont :
\begin{itemize}
	\item les noms complets (prénom et patronyme) de locuteurs ([name]),
	\item le nom de l'émission ([show]),
	\item les toponymes comme les villes, pays et monuments [(location]),
	\item les professions ([title]),
	\item les remerciements ([greet]),
	\item les mots faisant office d'accord avec l'autre ([agree]),
	\item mots gérant la communication (bonjour, au revoir : [comm])
	\item et les questions ([quest]).
\end{itemize}

Ces dictionnaires de classes ont été réalisés en extrayant les informations des transcriptions et en les complétant avec des listes de noms et des journaux en ligne. Ils permettent de généraliser les règles utilisées.

À partir du corpus de développement de 150 heures d'enregistrements de radio et de télévision anglais, les règles les plus utiles pour déterminer l'identité des locuteurs ont été extraites. Elles sont décrites dans le tableau \ref{table:canseco_regles}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      3162 & [title] [name] \\
      848 & I am [name] \\
      673 & [show]'s [name] \\
      382 & [agree] [name] \\
      293 & [name] [show] [location] \\
      186 & [show]'s [name] reports \\
      176 & [thanks] [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Règles les plus utiles pour déterminer l'identité d'un locuteur sur le corpus de développement}
  \label{table:canseco_regles}    
\end{table}

En plus de ces règles, d'autres règles comprenant un caractère joker (*) ont été utilisées. Ce caractère va permettre de remplacer n'importe quel mot, de manière à généraliser encore un peu plus les règles. Des exemples de règles avec joker sont données dans le tableau \ref{table:canseco_joker}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      Nombre d'occurrences & Règle \\
      \hline
      458 & with [comm] us * [name] \\
      109 & joining * [name] \\
      108 & [name] * joins \\
      45 & with * [comm] me \\
      24 & [comm-agreement] * [name] reporting \\
      24 & we are joined * by [name] \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Exemple de règles utilisant des joker (*) sur le corpus de développement}
  \label{table:canseco_joker}    
\end{table}

L'évaluation a été réalisée sur un corpus de test en anglais, où a été étudié le comportement de chaque règle pour les noms de locuteurs présents dans l'enregistrement. Le taux d'erreur des règles est de 11\%. Les règles permettent donc d'attribuer correctement un nom de locuteur au tour de parole précédent, suivant ou courant.


Ces travaux ont prouvé la pertinence de l'utilisation du contexte lexical pour nommer les locuteurs d'un enregistrement. En revanche, ils nécessitent un traitement manuel du corpus~: les règles et les dictionnaires de concepts sont réalisés par un humain. Le temps de mise en place de telles règles peut être long suivant la quantité de corpus à analyser et demande une expertise du domaine pour pouvoir être réalisée. Dans les travaux décrits dans \cite{CansecoRodriguez2005,CansecoRodriguez2006}, un corpus en langue anglaise de 150 heures a été étudié.
De plus, le passage d'un corpus à un autre est fastidieux : il faut réécrire le jeu de règles pour l'utiliser sur des documents d'une autre langue ou provenant d'autres types d'émission.
Pour finir, seuls les tours de paroles contigus au tour de parole dans lequel le nom de locuteur a été détecté peuvent être nommés. Aucune propagation au sein du fichier entier n'a été étudiée.
Ces travaux permettent de valider l'idée d'utiliser des informations linguistiques contenues dans les transcriptions manuelles (comme les noms complets des locuteurs et les mots les entourant) pour nommer les locuteurs d'un document audio.

\section{Règles statistiques : N-grammes}

\label{sub:regles_statistiques}

\subsection{Principes}

\subsection{Modèles N-grammes pour l'identification nommée du locuteur}

Suite aux travaux de Canseco (2005), le laboratoire de l'Université de Cambridge s'est intéressé à l'identification nommée du locuteur \cite{Tranter06}. Les travaux de Cambridge consistent à automatiser l'apprentissage des règles linguistiques et à mesurer l'impact de leur système d'identification nommée sur des données provenant de systèmes automatiques (classification en locuteur et transcription automatiques).

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{img/n_gram_tranter.pdf}
\caption{Exemple de n-grammes}
\label{fig:tranter_n_gram}
\end{center}
\end{figure}

Comme le montre la figure \ref{fig:tranter_n_gram}, les règles linguistiques sont représentées par des N-grammes allant de 2 à 5 mots. Ces N-grammes sont obtenus via un corpus d'apprentissage dans lequel la liste des personnes (leurs noms complets) intervenant dans les enregistrements est connue à l'avance. Les occurrences de ces noms complets sont détectées grâce à cette liste et les N-grammes sont appris en utilisant le contexte du nom complet détecté (une fenêtre de cinq mots incluant le nom complet est utilisée). Chaque N-gramme apparaissant plus d'un certain nombre de fois (cinq dans le cas de ces travaux) est considéré comme une règle de prédiction et est donc retenu comme règle linguistique.
Toutes les règles précédemment apprises sont utilisées sur le corpus de test. Lorsqu'une règle avec une probabilité $p_1$ est déclenchée et suggère un nom $n_1$ pour une classe de locuteur $s_a$, le score pour l'hypothèse $s_a = n_1$ est incrémenté.
Lorsque deux règles sont déclenchées pour le même nom de locuteur, leurs probabilités sont combinées en utilisant la formule :
\begin{equation}\mathbf{}
  p_{1+2}=1 - (1-p_1)(1-p_2)
\label{eq:sl}
\end{equation}

Comme dans les précédents travaux de Canseco (2005), Tranter (2006) utilise des classes sémantiques pour généraliser les règles. Ces classes sont issues des données d'apprentissage et ont été complétées avec d'autres sources (journaux écrits notamment). Ces classes sont les suivantes : GOODBYE, HELLO, OKAY, THANKS, LOCATION, PERSON, PERSON'S, SHOW, SHOW'S and TITLE. Ces classes regroupent celles utilisées par Canseco, la classe [agree] devient OKAY, [greet] devient THANKS, ... En revanche là où Tranter différencie HELLO et GOODBYE, Canseco les regroupait au sein d'une même classe [comm]. De plus, Tranter n'utilise pas la classe [quest].

Les résultats montrent que l'utilisation de catégories pour généraliser les mots présents dans les règles apporte un gain significatif. Sur des transcriptions réalisées manuellement et sur le corpus de développement, à 95 \% de précision le rappel est de 60 \%. Sur les données d'évaluation, le rappel chute à 38 \% pour la même précision.
Le système a été testé sur des données issues de systèmes automatiques, les résultats sont présentés dans le tableau \ref{table:resultats_tranter} et la figure \ref{fig:resultats_tranter}. 

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Trans. & Seg. & Rap. maximum & Rap. à 95 \% Pré. \\
      \hline
      ref & ref & 64 \% & 38 \% \\
      auto & ref & 44 \% & 38 \% \\
      auto & auto & 38 \% & 26 \% \\
      \hline
    \end{tabular}
  \end{center}

  \caption{Résumé des résultats sur le corpus de test. Sont présentés le meilleur rappel obtenu et le rappel à 95 \% de précision}
  \label{table:resultats_tranter}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{img/resultats_tranter.pdf}
\caption{Résultats sur le corpus d'évaluation avec (cat) ou sans (word) classes sémantiques. Les résultats sont donnés pour les transcriptions de référence (ref), les transcriptions utilisant un SRAP (asr) pour obtenir les mots et les transcriptions utilisant un système de segmentation automatique pour les tours de parole et les locuteurs (aseg) }
\label{fig:resultats_tranter}
\end{center}
\end{figure}

Le taux d'erreur (DER : Diarization Error Rate) du système de segmentation/classification utilisé est de 6,9 \%. Le taux d'erreur du système de transcription (WER : Word Error Rate) est quant à lui de 12,6 \%. L'utilisation de transcriptions automatiques n'affecte pas le rappel à 95 \% de précision mais le rappel maximum possible (en faisant baisser la précision) passe de 64 \% à 44 \%. L'utilisation de transcriptions et de segmentations automatiques fait chuter le rappel à 26 \% (pour 95 \% de précision) et fait aussi chuter le rappel maximum possible à 34 \%.
À noter que tous les résultats sont exprimés en terme du durée : plus les locuteurs parlant beaucoup sont détectés, plus les résultats sont élevés : le nombre de locuteurs correctement nommés n'est pas pris en compte. Voir la section \ref{sec:metrique} pour plus de détails sur les différentes métriques d'évaluation.



La méthode à base de N-grammes permet d'automatiser l'apprentissage des règles que Canseco avait définies à la main. De plus, une propagation relativement simple des scores est effectuée au sein des classes de locuteurs : les différents scores obtenus pour un locuteur $n_1$ et une classe de locuteur $s_a$ sont cumulés. Les tests réalisés montrent de bonnes performances sur des données manuelles, mais de beaucoup moins bonnes sur les données automatiques alors que les systèmes automatiques de transcription et de segmentation sont performants.
De plus, l'étiquetage des noms complets est réalisé grâce à une liste de locuteurs et l'étiquetage des autres catégories est réalisé à partir des données de l'apprentissage étiquetées manuellement.
Afin de se rapprocher d'une utilisation "réelle" de l'identification nommée il serait donc bon d'utiliser un détecteur d'entités nommées pour réaliser cet étiquetage. De plus, la propagation des scores au niveau des classes de locuteur et les problèmes concernant l'attribution d'un seul nom complet à une classe de locuteur soulevés dans \ref{ssub:attribution_d_un_nom_complet_a_une_classe_anonyme} n'ont pas vraiment été étudiés, une simple somme des différents scores étant effectuée.


\section{Règles statistiques : Arbre de décision}

Le laboratoire d'informatique de l'université du Maine (LIUM) a présenté ses travaux sur l'identification nommée du locuteur dans \cite{MauclairOdyssey06} et \cite{Esteve07}.
Les travaux du LIUM \cite{MauclairOdyssey06} ont été publiés en même temps que ceux de l'Université de Cambridge. Ils font aussi suite aux premiers travaux de Canseco. Le LIUM utilise un arbre de classification sémantique (SCT - Semantic Classification Tree \cite{Kuhn1995}) pour déterminer les règles linguistiques à utiliser pour attribuer l'étiquette \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} à un nom complet détecté dans la transcription. Ces règles linguistiques sont basées sur des expressions régulières construites autour des noms complets détectés. Ce système ayant servi de base aux travaux présentés ici, il est plus longuement présenté dans la section qui lui est consacrée \label{sec:sys_id_lium}.

\section{Modèle d'entropie maximum}

\subsection{Principes}

\subsection{Modèle d'entropie maximum pour l'identification nommée du locuteur}

Suite aux travaux du LIUM et de Cambridge, Microsoft s'est appuyé sur le modèle N-gramme et a ajouté des informations supplémentaires pour améliorer l'identification nommée du locuteur \cite{Chengyuan07}.

Ces travaux ont principalement ajouté des informations acoustiques pour améliorer l'identification nommée à base de N-grammes. Ces informations sont de deux sortes : 
\begin{itemize}
	\item l'utilisation de modèles acoustiques des voix des locuteurs cibles (lorsqu'il sont disponibles). Des modèles sont appris sur les échantillons de voix des locuteurs cibles. Ils seront ensuite utilisés pour essayer d'identifier ce locuteur acoustiquement pendant le test.
	\item l'utilisation du genre du locuteur. Lors de l'attribution d'un nom complet à un locuteur, une vérification est effectuée : le genre du prénom devrait correspondre au genre détecté par l'acoustique.
\end{itemize}
La position du nom complet dans l'enregistrement est aussi prise en compte. Posons $S_f$ le premier segment de la classe de locuteur anonyme $C$, $S_l$ le dernier segment et $n_C$ un nom complet étiqueté comme devant appartenir à la classe $C$. Plusieurs cas de figure sont étudiés, les plus pertinents sont : 
\begin{itemize}
	\item $n_l$ apparait avant $S_f$,
	\item $n_l$ apparait dans $S_f$,
	\item $n_l$ apparait avant $S_l$,
	\item $n_l$ apparait dans $S_l$.
\end{itemize}

Pour combiner toutes ces informations, Microsoft a choisi d'utiliser un modèle d'entropie maximum conditionnelle. Les règles linguistiques et les catégories sont obtenues de la même manière que dans \cite{Tranter06} : un modèle N-gramme pour apprendre les règles et un dictionnaire pour les catégories.

Les résultats montrent qu'en utilisation les mêmes informations que dans \cite{Tranter06} le modèle d'entropie maximum est plus performant. À l'inverse de l'information sur les genres et la position des noms complets, l'ajout des modèles de locuteur n'apporte en revanche aucun gain significatif. Les expériences ont été menées sur des transcriptions entièrement manuelles.

Ces travaux sont une extension des travaux menés par Cambridge dans \cite{Tranter06}. Ils améliorent les performances obtenues par Cambridge grâce au modèle d'entropie. Ils sont les premiers à introduire l'utilisation du genre du locuteur. En revanche, aucune expérience sur des données automatiques n'a été conduite. De plus,   les entités nommées et les catégories ont été étiquetées manuellement.

\section{Conclusion}

Les travaux sur l'identification nommée du locuteur utilisant la transcription enrichie du signal sont relativement récents puisque les premiers travaux menés par Canseco datent de 2004. Ils partagent tous la même hypothèse de base, à savoir qu'il est possible de déterminer, pour un nom de locuteur présent dans la transcription, s'il se rapporte au locuteur qui parle actuellement, à celui qui va parler juste après, ou à celui qui a parlé juste avant.

Deux principaux systèmes statistiques ont suivis les travaux de Canseco, à savoir celui à base de N-grammes de l'université de Cambridge et celui à base d'arbre de classification sémantique (SCT) de l'université du Mans. Les travaux de \cite{Esteve07} ont démontré la supériorité du système à base de SCT par rapport au système à base de N-grammes sur des transcriptions automatiques. Ces systèmes ne seront utilisables que sur des transcriptions automatiques, en effet l'intérêt de nommer automatiquement des locuteurs sur des transcriptions manuelles est nul. Il faudra donc s'intéresser plus particulièrement au système à base de SCT par la suite. Par rapport à ces systèmes précurseurs, Microsoft a ajouté la prise en compte du genre du locuteur et la prise en compte de la position du nom complet détecté dans l'enregistrement. Ces informations améliorant les performances, il faudra les prendre en compte.

Les travaux du LIUM ont été les seuls à utiliser un système automatique de détection des entités nommées. Dans le but d'automatiser le plus possible le processus, l'utilisation d'un tel système est indispensable. De plus, l'affectation des étiquettes \og précédent \fg{}, \og courant \fg{} et \og suivant \fg{} a beaucoup été étudiée dans la littérature. En revanche, le processus de propagation de ces étiquettes au niveau des classes de locuteur et la résolution des conflits n'a pas été étudiée jusqu'ici. Pour finir, beaucoup de travaux ont été conduits sur des transcriptions partiellement ou totalement manuel ; un travail sur des transcriptions entièrement automatiques est à réaliser.


\chapter{Le système d'identification nommée du Laboratoire d'Informatique de l'Université du Maine (LIUM\_NI)}
\minitoc
\newpage

\label{sec:sys_id_lium}

\section{Corpus et données}
\section{Segmentation et classification automatique}

Comme décrit dans \ref{ssec:ral}, la tâche de segmentation et de classification en locuteur consiste à annoter des régions du signal audio. Ces annotations peuvent être de plusieurs types : des étiquettes représentant les différents locuteurs, le genre des locuteurs, le type de canal (radio, studio) ou encore le type d'environnement sonore comme le bruit, la musique, \dots Dans la tâche de segmentation, les étiquettes représentant les locuteurs sont anonymes. Le système du LIUM est entièrement décrit dans \cite{liumspkdiarization}, ce qui suit en donne un court aperçu.

\subsection{Segmentation BIC}
Le système utilisé au LIUM (LIUM\_SpkDiarization) utilise une segmentation basée sur GLR (Generalized Likelihood Ratio) et BIC \cite{Siegler97automaticsegmentation} (Bayesian Information Criterion). Une première passe permet de détecter les points de rupture du signal constituant les frontières des segments. Ces points de rupture sont détectés en utilisant la mesure de vraisemblance GLR calculée (grâce à des GMM à matrice de covariance pleine) sur une fenêtre de 5 secondes tout le long du signal. Lorsque la mesure atteint un maximum dans cette fenêtre de 5 secondes, le système considère que c'est un point de rupture. Une seconde passe est ensuite effectuée afin de fusionner les segments correspondant au même locuteur, du début à la fin de l'enregistrement. La mesure utilisée pour le regroupement est la distance BIC utilisant des matrices de covariance pleines. 

\subsection{Classification BIC}

L'algorithme utilisé pour regrouper les segments au sein de classes se base sur une classification hiérarchique. Au départ, chaque classe est constituée d'un unique segment, chaque classe étant modélisée avec une GMM à matrice de covariance pleine. La mesure utilisée pour regrouper les classes entre elles, et pour stopper le regroupement est la mesure BIC. Les classes les plus proches sont regroupées tant que la distance BIC entre ces classes est inférieure à 0.

\subsection{Détection du genre et du canal}

La détection du genre est réalisée grâce à des GMM (avec une diagonale à 128 composantes) pour chacun des quatre combinaisons possibles entre le genre (homme/femme) et le canal (studio/téléphone). Chaque classe est étiquetée en fonction de la GMM qui maximise la vraisemblance avec la classe en question. Chaque modèle a été préalablement appris en utilisant le corpus d'apprentissage de la campagne ESTER.

\section{Reconnaissance automatique de la parole}
\section{Entités nommées}

Dans \cite{Esteve07} ils utilisent pour la première fois un détecteur d'entités nommées pour détecter les noms complets mais aussi les catégories servant à généraliser l'apprentissage. Le système est celui qui a été utilisé pour la campagne d'évaluation ESTER 1, les entités nommées utilisées ont été tirées de celles demandées pour ESTER, à savoir : personne, lieu, organisation, groupe/organisation politique et temps. 

\section{Arbres de classification sémantique (SCT) et décision locales}
\label{sec:sct}

Comme l'Université de Cambridge, le LIUM a généralisé l'approche de Canseco en automatisant l'apprentissage des règles linguistiques (grâce à l'arbre de classification sémantique) et en automatisant le processus de décision consistant à attribuer un nom complet à une classe de locuteur anonyme.
L'arbre de classification utilisé est un arbre de décision binaire construit à partir du corpus d'apprentissage. Chacun des noeuds de l'arbre représente une expression régulière. Lorsque le contexte lexical (vingt mots à gauche et vingt à droite) d'un nom complet correspond à une des expressions régulières de l'arbre, une probabilité que ce soit le locuteur \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} lui est attribué, en fonctions des données apprises sur le corpus d'apprentissage. La décision \og locale \fg{} ( \og suivant \fg{}, \og précédent \fg{} ou \og courant \fg{} ) avec le plus grand score est reportée au sein de la classe de locuteur anonyme correspondant. Lorsque le même nom complet est attribué plusieurs fois à une classe de locuteur, les probabilités de l'arbre de classification sont cumulées pour obtenir son score. Finalement, pour chaque classe de locuteur, le nom complet avec le plus grand score est attribué. À noter que l'arbre de classification permet de prendre en compte des informations plus \og globales \fg{} que le contexte lexical seul. Le LIUM utilise ainsi la position du nom complet dans le tour de parole (au début de celui-ci, à la fin celui-ci ou dans un tour de parole très court) comme information supplémentaire lors de l'apprentissage et du test de l'arbre.

Ils ont réimplémenté le système à base de N-grammes de \cite{Tranter06} et l'ont comparé à l'approche basée sur les SCT. Les évaluations ont été conduites sur le corpus d'évaluation de la campagne ESTER 1 avec une liste fermée de locuteurs provenant des corpus d'apprentissage, de développement et de test. Cette liste contient 1007 locuteurs. Les résultats ont montrés que le système à base de N-gramme et celui à base de SCT se comportait de manière similaire sur des transcriptions réalisées entièrement manuellement. En revanche, sur des transcriptions automatiques (en gardant la segmentation et la classification de référence), le système à base de SCT s'est avéré beaucoup plus robuste.

\section{Système de décision global}

Ces travaux sont les premiers à utiliser un système automatique de détection des entités nommées. Ils automatisent complètement le processus d'identification nommée, de l'apprentissage au test : toutes les étapes sont automatiques.
En revanche, le système de décision est relativement sommaire : seule l'étiquette avec le score maximal est pris en compte, les probabilités des autres étiquettes pour un nom complet sont tout simplement ignorées. De plus la prise de décision au niveau de la classe de locuteur se réduit à attribuer le nom de locuteur ayant un score maximum : l'incertitude au sein d'une même classe (plusieurs locuteurs possibles avec des scores élevés) n'est pas prise en compte.

\section{Expériences et résultats}

\subsection{Légende}

\textit{\textbf{Trans.}: Transcription \textbf{M}anuelle ou \textbf{A}utomatique.\newline
\textbf{Seg/Class.}: segmentation/classification manuelles ou automatiques.\newline
\textbf{R, P}: rappel et précision calculés en en durée.\newline
\textbf{ErrDur}: Taux d'erreur en durée.\newline
\textbf{ErrLoc} : Taux d'erreur en nombre de locuteurs.\newline}

\subsection{Résultats avec liste de locuteurs globale}

Le tableau \ref{tab:results_liumni} présente les résultats du système sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles ou totalement automatiques. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs d'ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste des locuteurs (le genre des locuteurs étant extrait des transcriptions de référence).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 87,36\% & 95,69\% & 11,67\% & 14,11\% \\ \hline
\textbf{A} & \textbf{A} & 29,31\% & 59,98\% & 64,49\% & -\% \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 85,99\% & 95,72\% & 12,88\% & 18,26\% \\ \hline
\textbf{A} & \textbf{A} & 29,48\% & 69,42\% & 64,46\% & -\% \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs}
\label{tab:results_liumni}
\end{table}

\subsection{Résultats avec liste de locuteurs par show}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 88,67\% & 97,31\% & 10,45\% & 12,45\% \\ \hline
\textbf{A} & \textbf{A} & 28,80\% & 69,65\% & 64,91\% & -\% \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 86,85\% & 96,56\% & 12,10\% & 17,01\% \\ \hline
\textbf{A} & \textbf{A} & 28,83\% & 73,77\% & 64,84\% & -\% \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs du show}
\label{tab:results_liumni_liste_show}
\end{table}


\subsection{Impact de la prise en compte du genre}

Le tableau \ref{tab:results_liumni_sansgenre} présente les mêmes résultats que le tableau \ref{tab:results_liumni} sans utiliser l'information sur le genre des locuteurs. La liste des locuteurs d'ESTER 1 (tous corpus confondus) est utilisée, mais sans prendre en compte le genre du locuteur.

Lorsque le genre n'est pas utilisé pour affiner les décisions, les performances du système se dégradent, que ce soit sur les transcriptions automatiques ou manuelles. Sur les transcriptions manuelles, le taux d'erreur augmente d'environ 3\% en absolu. Sur les transcriptions automatiques, le taux d'erreur augmente très peu sur les corpus de développement, mais augmente de 4\% sur le corpus de test. Le genre des locuteurs étant détecté automatiquement, il est donc soumis à des erreurs \todo{donner le taux d'erreur}. Mais même si cette détection comporte des erreurs, l'utiliser améliore quand même les performances.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 84,30\% & 90,60\% & 14,55\% & 17,84\% \\ \hline
\textbf{A} & \textbf{A} & 28,56\% & 54,08\% & 65,65\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 84,12\% & 93.40\% & 14,68\% & 20,33\% \\ \hline
\textbf{A} & \textbf{A} & 24,61\% & 58,82\% & 68,78\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs, sans prise en compte du genre}
\label{tab:results_liumni_sansgenre}
\end{table}


\subsection{Système ouvert sans liste de locuteurs et de leur genre}

\ref{tab:results_liumni_sansliste} \ref{tab:results_liumni_genre_prenom}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 67,23\% & 93,12\% & 30,04\% & 28,63\% \\ \hline
\textbf{A} & \textbf{A} & 20,26\% & 76,84\% & 72,73\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 71,46\% & 95,03\% & 26,11\% & 27,80\% \\ \hline
\textbf{A} & \textbf{A} & 20,60\% & 64,03\% & 72,32\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système de base avec une transcription enrichie manuelle ou automatique sans liste de locuteurs et sans genre}
\label{tab:results_liumni_sansliste}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 70,97\% & 97,89\% & 26,55\% & 22,82\% \\ \hline
\textbf{A} & \textbf{A} & 20,91\% & 80,64\% & 72,10\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 74,58\% & 97,85\% & 23,23\% & 24,90\% \\ \hline
\textbf{A} & \textbf{A} & 23,77\% & 74,53\% & 69,40\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système proposé avec une transcription enrichie manuelle ou automatique sans liste de locuteurs, et avec prise en compte du genre par le prénom}
\label{tab:results_liumni_genre_prenom}
\end{table}

\section{Positionnement et hypothèses}
      Les travaux ont beaucoup porté sur la manière d'étiqueter les suivant/précédent/courant/autre => Un papier montre que les SCT > N-grammes. Aller plus loin : améliorer le processus de décision qui vient après ces décisions locales
      Utiliser une analyse conjointe du signal sonore et du texte pour améliorer les décisions "aveugles" (renforcer les décisions avec le genre des locuteurs)

Les noms complets des locuteurs sont présents dans la transcription, et ils est possible de les affecter à tour de parole contigu

\chapter{LIUM\_NI : Un nouveau système de décision (titre à trouver ?)}
\minitoc
\newpage

\section{Analyse des erreurs}


Toutes les mauvaises décisions prises par le système ont été étiquetées manuellement et comparées avec les différents scores générés par l'arbre. Sont comptées comme erreurs toutes les décisions non prises (erreur de suppression), ou toutes les décisions prises à tort (erreur de substitution). Cette étude a permis de mettre en évidence deux grandes catégories d'erreurs~: les erreurs de détection d'entités nommées et les erreurs de décision de l'arbre. D'autres types d'erreurs ont aussi été constatés mais elles sont moins significatives. 
	
%Dans les 18 fichiers analysés correspondant au test ESTER (10 heures d'enregistrement de journaux d'informations), 86 erreurs ont été détectées. Ces erreurs peuvent être de deux types, soit un mauvais nom de locuteur attribué, soit aucun nom attribué alors que c'était possible.

\subsection{Formalisme}

Dans l'étude qui suit, un même formalisme sera utilisé dans chacun des exemples. Les entités nommées détectées comme telles par Nemesis seront entourées de crochets, les anthroponymes seront en plus mis en italique (ex : \textit{[Jacques Chirac]}). ``(...)'' signifiera que la phrase a été coupée pour plus de lisibilité. Les exemples seront de ce type :\\\\
\texttt{
LOCU1 (Jacques Chirac) : (...) fin d'un tour de parole\\
LOCU2 (inconnu) : [Entité nommée] fin de phrase\\
LOCU2 (inconnu) : \textit{[Anthroponyme]}\\
LOCU3 (Bernard Thomasson): début du tour de parole suivant (...)\\\\
}

Chaque tour de parole sera identifié par une étiquette anonyme \texttt{LOCUX} suivie entre parenthèse de la vraie identité du locuteur tirée de la transcription de référence. Il pourra y avoir plusieurs segments à l'intérieur du même tour de parole, c'est le cas du \texttt{LOCU2} ici.

\subsection{Entités nommées}

Ce qui est ici appelé ``erreur'' est relatif à ce qu'attend l'arbre. \'Etiqueter \textit{[président Jacques Chirac]} comme anthroponyme est correct du point de vue linguistique et n'est pas une erreur commise par Nemesis, mais n'est pas conforme à ce qui est attendu par le système d'identification nommée. En effet, le système d'identification nommée ne marche qu'avec les couples noms~/prénoms et n'acceptera pas \textit{[président Jacques Chirac]}. Il faut donc bien garder à l'esprit que ce qui est appelé ici erreur, est ce qui met le système d'identification nommée en échec.\\

%\subsection{Anaphores et autres erreurs}

%Dans quelques cas, même un humain n'aurait pas été capable d'identifier la personne qui parle à partir du texte. En effet, il arrive que l'information ne soit pas du tout présente dans le texte. C'est le cas pour Bernard Thomasson dans 20041008\_1800\_1830\_INFO\_DGA, il n'est jamais cité dans la transcription. Dans les enregistrements de la Radio RTM, la journaliste Samira lamrani n'est jamais appelée par son nom/prénom. Seul son prénom est utilisé. Même si le système arrive à attribuer Samira au locuteur, le système attend uniquement des nom/prénoms, le locuteur est donc mis en inconnu.

%\subsection{Nemesis}


\textbf{\'Etiquetage erroné}\\

Nemesis rencontre des problèmes pour l'étiquetage des radios type France Inter et France Info. En effet, France étant un prénom, il étiquette France Inter et France Info comme étant des anthroponymes. L'ajout de France Info et de France Inter dans ses lexiques de médias ne change rien. La règle qui est déclenchée prioritairement par Nemesis est donc celle qui considère France comme un prénom. Par exemple :\\\\
\texttt{
LOCU1 (Frank Noblesse) : (...) sans pouvoir baisser sa vitesse\\
LOCU2 (inconnu) : \textit{[France Info]} actualités\\
LOCU2 (inconnu) : \textit{[Franck Noblesse]}\\
LOCU1 (\textbf{Frank Noblesse}) : dix huit heures quatre l' otage (...)\\\\
}

Dans cet exemple, \texttt{France Info} et \texttt{Franck Noblesse} sont étiquetés tous les deux comme anthroponymes.\texttt{ France Info} est évalué comme étant \texttt{LOCU1} (le locuteur précédent le tour de parole où il a été détecté) à 42\%. \texttt{Franck Noblesse} comme étant aussi \texttt{LOCU1} (cette fois ci le suivant) à 46\%. Un peu plus loin dans le fichier \texttt{France Info} est encore attribué à \texttt{LOCU1} :\\\\
\texttt{
LOCU3 (Nicolas Hénin) : (...) très hostile au sud de [Bagdad]\\
LOCU3 (Nicolas Hénin) : [Amman] \textit{[Nicolas Hénin]} \textit{[France Info]}\\
LOCU1 (Frank Noblesse) : \textit{[Christian Chesnot]} et \textit{[Georges Malbrunot]} otages en (...)\\\\
}

Ici \texttt{France Info} est encore détecté comme étant \texttt{LOCU1} (le locuteur suivant) à 92\%. Lorsque le système prend une décision, il se rend compte que \texttt{LOCU1} a été attribué deux fois à \texttt{France Info} avec un total de 1,34 points (0,42 + 0,92) alors qu'il n'a été attribué qu'une fois à \texttt{Franck Noblesse} avec un total de 0,46 points. Il choisit ainsi d'attribuer \texttt{France Info} comme identité pour LOCU1 au lieu de Franck Noblesse.


Lors de l'évaluation, le système ne prend pas en compte l'identité attribuée au locuteur puisque \texttt{France Info} n'est pas connu comme locuteur correct dans la transcription de référence, ceci fait donc chuter le rappel. C'est le seul cas d'étiquetage erroné posant vraiment problème dans le corpus étudié.\\

\textbf{\'Etiquetage partiel}\\

Dans quelques rares cas (3 sur les 86 erreurs), Nemesis n'étiquette que partiellement les identités de locuteur. Ceci peu être du à un prénom étranger comme \texttt{Anissa} (\texttt{Anissa \textit{[El Jabri]}} partiellement étiqueté) ou à prénom peu commun comme \texttt{Eymard} (\texttt{Eymard du \textit{[Chatenet]}} partiellement étiqueté). Un autre cas a été relevé, \texttt{\textit{[Frédéric]} Rivière} est en effet mal étiqueté puisque seul son prénom l'est.\\

%En revanche, il est aussi assez fréquent (15 occurrences sur les 18 fichiers) que Némésis étiquette mal le nom de locuteur, soit parce que c'est un nom étranger, soit parce qu'il lui arrive d'étiqueter la fonction de la personne. Quelques exemples ci dessous.
%CJ pourquoi ci-dessus fais tu une transition vers la partie suivante ? En lisant, on est un peu déstabilisé, on se demande où on est.
%CJ avant toute chose, il y a ambiguité sur le mot nom. On parle de nom du locuteur qui est en fait son prénom et son nom (de famille). Pourquoi ne pas appeler ce nom de famille le patronyme. ce serait plus clair.
%CJ Je ne comprends pas => Nemesis étiquette mal le nom du locuteur parce que c'est un patronyme étranger. Pourquoi ce n'est pas dans la catégorie "etiquetage erroné" ?
%CJ Ce n'est pas parce que Nemesis inclus le rôle de la personne dans l'entité nommée, qu'il commet une erreur d'étiquetage (voir explication thèse Fourour chap 4 p 84 de pourquoi il prend "large")
%VJ Oui je sais, c'est ce que j'essaie d'expliquer en dessous, je vais faire en sorte d'être plus clair.
\textbf{\'Etiquetage inadéquat}\\

Certaines étiquettes sont correctes du point de vue linguistique mais ne correspondent pas à ce qui est attendu par le système d'identification.
 Par exemple, \texttt{\textit{[président du Fetia Api]}} est bien un anthroponyme et est détecté comme tel par Nemesis. En revanche, lors de l'évaluation, \texttt{\textit{[président du Fetia Api]}} ne sera pas connu de la référence (dans laquelle il n'y a que des couples prénoms~/patronymes) et sera donc considéré comme inconnu. Ce genre d'erreur provient donc d'une inéquation entre les sorties de Nemesis et les entrées du système d'identification nommée.


Nemesis essaye tant que possible d'étendre l'entité nommée au maximum, ce qui donne des entités nommées comme celle-là : \texttt{\textit{[ministre de l'intérieur Jean Pierre Chevènement]}}. C'est en effet un anthroponyme et une entité nommée valide, mais qui ne correspond pas aux attentes du système d'identification nommée.

Un dernier cas d'étiquetage inadéquat relève quant à lui d'un problème lié à Nemesis. Ainsi, il est difficile pour Nemesis de délimiter le contexte droit d'une entité nommée, c'est à dire savoir où elle s'arrête, quelles autres entités nommées inclure, etc. Dans le cas d'entités nommées mixtes ou le problème de sur-composition se pose (entité nommée contenant une autre entité nommée), Nemesis choisi de ne ``retenir que la forme la plus complète constituant une entité nommée'' (cf \cite{Fourour04}, 4.1.2). Deux problèmes de ce type ont été relevées dans notre corpus : \texttt{\textit{[Joël Collado de Météo France]}} et \texttt{\textit{[Emmanuel Butstraten de BASF Agro]}}. En effet, le système d'identification nommée n'est évalué qu'avec des couples prénoms/patronymes, la sur-composition lui pose donc problème.\\
%CJ tu peux être plus explicite sur le problème (thèse Fourour chap 2  paragraphe 2.1.2.1 p 28 et chap 4 paragraphe 4.1.2 p 85 (l'identification). Dans tes 2 exemples, c'est un problème de sur-composition pour le contexte droit.

%Un ensemble de mots ne contenant pas de nom et prénom est détecté comme une personne. Cette détection fait chuter le rappel en attribuant ce \textit{faux locuteur} à un tour de parole. Par exemple, Nemesis étiquette l'entité nommée ``[président du Fetia Api]'' comme personne qui est ensuite attribuée à un tour de parole. ``[président du Fetia Api]'' est bien une personne, mais cette entité nommée n'est pas un locuteur au sens de l'identification. Il est identifié par sa fonction au lieu de son nom et prénom.

\textbf{Utilisation de Némésis dans un mauvais contexte}\\

Les entrées qui sont données à Nemesis sont des transcriptions utilisées pour la reconnaissance automatique de la parole. Ces transcriptions sont allégées de toute ponctuation, ce pourquoi Nemesis n'a pas été prévu. Nemesis étant un outil tiré du TALN, il se base sur la ponctuation pour détecter les entités nommées. Utiliser Nemesis avec de telles transcriptions l'amène à accoler des entités nommées qui sont normalement séparées par une ponctuation. Voici quelques exemples d'entités nommées agglutinées qui auraient du être étiquetées séparément (la ponctuation manquante a été rajoutée entre parenthèses): \texttt{\textit{[Bernard Thomasson (.) Claude Thélot]}}, \texttt{\textit{[Friponil (.) Frank Aletru]}}, \texttt{\textit{[France Info (,) Tendance Junior (,) Aurélie Kieffer (.)]}}.


\subsection{Erreurs de l'arbre de décision}

\textbf{Décision incertaine}\\

Dans beaucoup de cas, l'arbre de classification n'est pas capable de décider si le locuteur est le suivant, le précédent ou le courant. Ces locuteurs se retrouvent donc étiquetés comme \textit{autre} alors que dans la majorité des cas ils parlent avant ou après le tour de parole où ils ont été étiquetés.

Ceci est du à un locuteur qui n'est pas clairement annoncé, ce qui ne permet pas à l'arbre de savoir si l'entité nommée concerne le locuteur suivant. C'est le cas dans l'exemple ci-dessous.\\\\
\texttt{
LOCU1 (Valérie Crova) : (...) qui a toujours entretenu selon \textit{[Jean Christophe Bouisson]} des rapports tendus avec le gouvernement.\\
LOCU2 (Jean Christophe Bouisson) : On ne peut pas penser (...)\\
}
\\Dans cet exemple ``[Jean Christophe Bouisson]'' est étiqueté comme désignant un \textit{autre} tour de parole alors qu'il se rapporte au \textit{suivant} (LOCU2).\\\\

Un des cas relevé plusieurs fois est celui ou la personne annoncée est suivie de la fonction quelle occupe.\\\\
\texttt{
LOCU1 (Yves Izard) : (...) \textit{[Marie George Buffet]} la secrétaire nationale du parti communiste français\\
LOCU2 (Marie George Buffet) : le Premier ministre pense d'après (...)\\
}\\
On se retrouve ici dans un cas presque identique au premier, à cette différence prêt que la fonction ``la secrétaire nationale du parti communiste français'' pourrait être catégorisée et donc réduite à un unique mot FONCTION par exemple. Un ajout dans ce sens lors de la détection d'entités nommées serait intéressant pour généraliser au plus possible les mots lors de l'apprentissage de l'arbre.\\

\textbf{Décision erronnée}\\

Les erreurs de substitution sont toujours dues à des locuteurs qui ont été étiquetés à tort comme suivant, précédent ou courant. Comme décrit dans \ref{metrique}, ces erreurs n'affectent donc que les performances en terme de rappel.

Un cas qui revient plusieurs fois est celui où un locuteur annonce le prochain interviewé et le prochain intervieweur dans la même phrase. Dans ce cas, le système accorde toujours plus d'importance au locuteur cité en dernier, qui n'est généralement pas le locuteur suivant, mais celui juste après (le journaliste généralement).\\\\

\texttt{
LOCU1 (Fabrice Drouelle) : écoutez (...) \textit{[pers Jean Michel Hibon]} au micro de \textit{[pers Jérôme Susini]}\\
LOCU2 (Jean-Michel Hibon) : mon logement de fonction (...)
}\\\\
Dans ce cas LOCU2 est étiqueté comme étant Jérôme Susini et non Jean-Michel Hibon. En effet, selon l'arbre la probabilité que Jérôme Susini désigne le tour de parole suivant est plus grande que celle concernant Jean Michel Hibon.\\\\

%\noindent \textbf{20041013\_1700\_1800\_INFO\_DGA} :\\
%Bernard\_Thomasson 1144.075 1149.122 \textbf{[pers Jean Jacques Vanier]} sera à partir de demain sur la scène de L' Européen à [gsp Paris] pour trois mois \\
%Bernard\_Thomasson 1149.122 1153.543 il présente son nouveau spectacle à part ça la vie est belle à \textbf{[pers Bernard Stéphane]}\\
%\textbf{Jean-Jacques\_Vanier} 1153.543 1159.549 c' est l' histoire de quatre amis qui ont décidé de garder leur amitié au delà de la vie et au delà de la mort quoi \\\\

%Ici c'est Bernard Stéphane qui est attribué à Jean-Jacques Vanier.\\
%Sur les 12 erreurs affectant la précision que j'ai pu détecter, 10 étaient de ce type. Généralement, lorsque c'est la personne interrogée et non le journaliste qui parle en premier, le système commet une erreur.

% \subsection{Mauvaise décision de l'arbre}
% 
% La performance en terme de rappel du système est principalement affectée par les personnes qui sont signalées comme n'étant ni avant ni après (elles sont étiquetées en autre). Sur les 58 non attributions, 16 sont dues à ce type de problème. On observe généralement le problème lorsque la personne n'est pas mentionnée juste avant de parler, mais une ou deux phrases avant. Voici quelques cas représentatifs :\\\\
% 
% \noindent \textbf{20041011\_1300\_1400\_INTER\_DGA} :\\
% 
% \noindent delphine simon 1127.884 1134.114 Vanessa Descoureaux vient de joindre \textbf{Marie Sophie Desaulle} la présidente de l' association des paralysés de de France\\
% delphine simon 1134.114 1135.522 on écoute sa réaction\\
% \textbf{marie sophie desaulle} 1135.522 1138.677 ben Christopher Reeve permettait effectivement que\\\\
% 
% \noindent \textbf{20041013\_1700\_1800\_INFO\_DGA} :\\
% 
% \noindent yves izard 725.272 729.093 \textbf{Marie George Buffet} la secrétaire nationale du parti communiste français
% \textbf{marie george buffet} 729.093 734.142 le Premier ministre pense d' après les informations ils sont en vie mais tout cela bien sûr reste fragile\\\\
% 
% Il arrive aussi à l'arbre d'étiqueter un locuteur comme étant le prochain, et non l'actuel (7 occurrences). Ce type d'erreur génère aussi de mauvaises attributions de nom, 2 cas sur les 7. Les deux erreurs d'attribution de nom :\\\\
% 
% 
% \noindent \textbf{20041027\_1230\_1300\_RFI\_ELDA} :\\
% 
% \noindent eduardo febro 481.682 484.570 \textbf{Eduardo Febro} Miami radio France internationale\\
% \textbf{françois bernard} 484.570 491.834 République démocratique du Congo les ministres des affaires étrangères du Rwanda\\
% 
% François Bernard est étiqueté comme étant Eduardo Febro. A noter qu'Eduardo febro ayant déjà été étiqueté correctement plus haut, on ne devrait pas pouvoir attribuer deux fois le même nom à deux locuteurs différents.\\
% 
% \noindent \textbf{20041222\_1300\_1320\_RTM\_ELDA} :\\
% 
% \noindent samir bour 777.168 780.378 \textbf{Samir Bour} depuis Tanger pour RTM Chaîne Internationale\\
% \textbf{aziza ziani} 783.356 785.518 et puis cette inf activité royale\\\\
% Aziza ziani est ici étiquetée comme étant samir bour.

\subsection{Autres erreurs}
D'autres erreurs plus indépendantes du système ont été relevées~: certaines personnes ne sont citées que partiellement (seul leur prénom est cité dans la transcription), d'autres ont été mal orthographiées par le transcripteur ou encore certaines ne sont pas du tout citées ou annoncées dans la transcription. Sur les 11 heures du corpus, ce dernier cas ne concerne que 3 personnes. Cette constatation permet de valider l'hypothèse de départ~: les noms des locuteurs sont présents dans la transcription.

\subsection{Répartition des erreurs}

Le tableau \ref{tab:repart_erreurs} montre la répartition et l'importance des différents types d'erreurs sur le corpus de test. Les erreurs provenant de l'arbre de classification (A1 et A2) sont clairement dominantes avec plus de 72\% des erreurs totales alors que les erreurs dues à Nemesis (N1, N2 et N3) ne représentent qu'un peu moins de 19\% des erreurs. Pour l'arbre de classification, les erreurs A1 et A2 ont le même ordre de grandeur (environ 36\% en moyenne). En revanche, en ce qui concerne Nemesis les erreurs de type N1 sont plus fréquentes que celles de type N2 et N3 (environ 2 fois plus). Les étiquetages de France Info et de France Inter comme anthroponymes ne sont pas comptabilisés dans ces résultats. Leur proportion (79 étiquettes) relativement importante aurait faussé la répartition des autres erreurs. Nous considérons en effet que ce souci lié à Nemesis est marginal et sera facilement corrigé.

{\footnotesize{
\begin{figure}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Erreurs} & \textbf{Fréquence} & \textbf{Pourcentage Total} \\ \hline
\hline
\textbf{(N1) \'Etiquetage partiel ou inadéquat} & 7 & 8,2\% \\ \hline
\textbf{(N2) \'Etiquetage erroné} & 4 & 4,6\% \\ \hline \hline
\textbf{(N3) \'Mauvais contexte} & 5 & 5,8\% \\ \hline \hline
%\textit{total} & \textit{16} & \textit{18,6\%} \\ \hline \hline
\textbf{(A1) Décision incertaine} & 32 & 37,2\% \\ \hline
\textbf{(A2) Décision erronée} & 30 & 34,9\% \\ \hline \hline 
%\textit{total} & \textit{62} & \textit{72,1\%} \\ \hline \hline
\textbf{(E) Autre} & 8 & 9,3\%\\ \hline \hline
\textbf{Total} & 86 & 100\% \\ \hline
\end{tabular}
\end{center}

\caption{Répartition des erreurs sur le corpus de test.\newline
%------------\newline
{\textit{N1, N2, N3~: erreurs issues de la détection d'entités nommées.\newline
A1, A2~: erreurs issues de l'arbre de classification.\newline
E~: autres erreurs.\newline}}
%------------
}
\label{tab:repart_erreurs}

\end{figure}
}}

% \subsection{Autres erreurs}
% 
% Dans deux des cas relevés, la transcription du nom est fausse. La personne ayant transcrit manuellement le nom s'est trompé dans l'orthographe, le système ne trouve donc pas de concordance entre les noms trouvés.\\\\
% 
% \noindent \textbf{20041027\_1230\_1300\_RFI\_ELDA} :\\
% 
% \noindent  françois bernard 340.634 341.727 \textbf{Patrick Gouy}\\
% \textbf{patrice gouy} 341.727 345.982 les deux candidats ont ouvert au Mexique où vit un million d' expatriés\\\\
% 
% Il arrive aussi (3 occurrences) que le locuteur ne soit pas annoncé mais soit juste mentionné quelque part dans le texte, souvent plus loin, en faisant référence à ce qu'il a dit. C'est par exemple le cas de Jean Pierre Raffarin dans 20041012\_1800\_1830\_INFO\_DGA.\\
% Il peut aussi arriver que les erreurs soient reportée globalement alors que l'on hésitait entre deux personnes. Dans 20041007\_0800\_0900\_INTER\_DGA, pour un locuteur nous avons un score de 0.47 pour dominique de villepin et de 0.46 pour alain ray, nous choisissons dominique de villepin (ce qui est faux) et nous reportons l'erreur sur tout le fichier.

\newpage
\subsection{Conclusion}

Nous avons présenté notre système d'identification de locuteur ayant comme principales caractéristiques de s'appuyer sur les données de transcription, d'effectuer la reconnaissance des entités nommées grâce à un outil issu du traitement automatique des langues et de réaliser l'apprentissage et la prise de décision à l'aide d'un arbre de classification sémantique. Nous avons d'abord analysé les résultats obtenus en terme de précision et de rappel et nous avons obtenu des résultats prometteurs. Nous avons ensuite mené une étude exhaustive concernant les erreurs commises par le système, erreurs qui se situent principalement au niveau du système de reconnaissance d'entités nommées Nemesis et de l'arbre de classification. 

Au niveau de Nemesis, il faudrait premièrement calibrer ses sorties pour les adapter aux entrées attendues de notre système d'identification. En effet, Nemesis a été conçu pour détecter des entités nommées simples et complexes (par exemple ``président de la république Jacques Chirac'' est une entité nommée étiquetée comme anthroponyme par Nemesis) et le système d'identification exploite des entités nommées sous une forme simple de type ``prénom/patronyme'', ce qui génère des erreurs.  Deuxièmement, l'utilisation de Nemesis peut aussi être optimisée par un ajout de ponctuation au niveau de la transcription qui lui permettrait de mieux détecter les entités nommées (notamment quand une entité se trouve à la fin d'un segment et l'autre au début du suivant, il étiquette les deux entités nommées ensemble).

%Au niveau de Nemesis, la prise en compte des fonctions des personnes et la modification de certaines règles de décision permettrait d'améliorer l'étiquetage. Mais aussi, un ajout de ponctuation au niveau de la transcription
% (virgule après chaque fin de segment) 
%lui permettrait de mieux détecter les entités nommées.
% (notamment quand une entité se trouve à la fin d'un segment et l'autre au début du suivant, il étiquette les deux entités nommées ensemble).
%SM c'est pas déjà fait !

Au niveau de l'arbre de classification, il serait intéressant de prendre en compte le numéro d'ordre du nom de locuteur détecté dans le tour de parole lors de l'apprentissage de l'arbre et du test. Ceci permettrait dans certains cas de faire abstraction des mots pouvant suivre le dernier nom de locuteur du tour de parole. Dans un souci d'augmenter les données d'apprentissage fournies à l'arbre, catégoriser ou lemmatiser le plus de mots possibles semblerait être une piste intéressante. La détection des fonctions des personnes par Nemesis irait dans ce sens.

Pour finir, comme c'est déjà le cas pour certains modèles de langage en reconnaissance de la parole, utiliser les informations fournies par un étiqueteur grammatical pour apprendre et tester l'arbre de classification sera à étudier.

\section{Meilleure détection des entités nommées}

\subsection{Némésis, un outil prévu pour le TAL}

\subsection{Némésis et LIA\_NE :  expériences et résultats}

\section{Système de décision}

\subsection{Problématique}

\subsection{Décisions locales : le classifieur}

\subsection{Décisions globales : Théorie des croyances}

\section{Applications et connaissances a priori}
liste de locuteurs, genre des locuteurs


\section{Expériences et résultats}

\subsection{Légende}

\textit{\textbf{Trans.}: Transcription \textbf{M}anuelle ou \textbf{A}utomatique.\newline
\textbf{Seg/Class.}: segmentation/classification manuelles ou automatiques.\newline
\textbf{R, P}: rappel et précision calculés en en durée.\newline
\textbf{ErrDur}: Taux d'erreur en durée.\newline
\textbf{ErrLoc} : Taux d'erreur en nombre de locuteurs.\newline}

\subsection{Corpus et données}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Corpus & Courant & Suivant & Précédent & Autre \\
      \hline
      Train & 3,9 & 16,6 & 19,2 & 32,5 \\
      Dev & 8,3 & 17,6 & 9,3 & 45,2 \\
      Test & 26,7 & 17,1 & 12.3 & 39,3 \\      
      \hline
    \end{tabular}
  \end{center}

  \caption{Taux d'erreur des étiquettes courantes / suivantes / précédentes des locuteurs faisant partie de la liste de locuteur fermée ESTER1}
  \label{table:etiquettes_sct}
\end{table}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Corpus & Courant & Suivant & Précédent & Autre \\
      \hline
      Train & 17,3 & 46,3 & 51,9 & 6,8 \\
      Dev & 42,1 & 46 & 40 & 13,2 \\
      Test & 52,1 & 42,4 & 42 & 10,7 \\      
      \hline
    \end{tabular}
  \end{center}

  \caption{Taux d'erreur des étiquettes courantes / suivantes / précédentes des locuteurs en liste ouverte}
  \label{table:etiquettes_sct}
\end{table}

% Résultats sans filtrage de liste de locuteurs sur du manuel (même pas sur le genre) & LIA_NE
%
% Train ESTER 1 (Train spkid)
% Next : 934/2019 = 46.2605250123824 %
% Previous : 188/362 = 51.9337016574586 %
% Current : 36/208 = 17.3076923076923 %
% Other : 666/9766 = 6.81957812819988 %

% Test ESTER 1 (Dev spkid)
%
% Next : 139/302 = 46.0264900662252 %
% Previous : 26/65 = 40 %
% Current : 8/19 = 42.1052631578947 %
% Other : 168/1274 = 13.1868131868132 %

% Test ESTER 2 sans africa (Test spkid)
%
% Next : 132/311 = 42.443729903537 %
% Previous : 37/88 = 42.0454545454545 %
% Current : 12/23 = 52.1739130434783 %
% Other : 132/1238 = 10.6623586429725 %



% Résultats avec filtrage via liste de locuteurs sur du manuel (mais pas avec la prise en compte du genre) & LIA_NE
%
% Train ESTER 1 (Train spkid)
%
% Next : 211/1273 = 16.5750196386489 %
% Previous : 41/213 = 19.2488262910798 %
% Current : 7/179 = 3.91061452513966 %
% Other : 647/1988 = 32.5452716297787 %


% TEST ESTER 1 (Dev spkid)
%
% Next : 34/193 = 17.6165803108808 %
% Previous : 4/43 = 9.30232558139535 %
% Current : 1/12 = 8.33333333333333 %
% Other : 161/356 = 45.2247191011236 % 

% Test ESTER 2 sans africa (Test spkid)
%
% Next : 36/211 = 17.0616113744076 %
% Previous : 7/57 = 12.280701754386 %
% Current : 4/15 = 26.6666666666667 %
% Other : 126/321 = 39.2523364485981 %

\subsection{Utilisation d'une liste de locuteurs globale et de leur genre}

Le tableau \ref{tab:results_fctcroyance} présente les résultats du système utilisant les fonctions de croyances. Les expériences sont menées sur les corpus de développement et de test, en utilisant des transcriptions totalement manuelles ou totalement automatiques. Le système de détection d'entités nommées utilisé est LIA\_NE, la liste de locuteurs d'ESTER 1 est utilisée pour filtrer les décisions. Le genre des locuteurs est pris en compte via la liste des locuteurs (le genre des locuteurs étant extrait des transcriptions de référence).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 86,16\% & 94,38\% & 12,75\% & 15,35\% \\ \hline
\textbf{A} & \textbf{A} & 21,45\% & 49,96\% & 71,59\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 85,49\% & 95,01\% & 13,34\% & 18,67\% \\ \hline
\textbf{A} & \textbf{A} & 25,63\% & 61,20\% & 67,95\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs et la détection du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Utilisation d'une liste de locuteurs par show et de leur genre}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 87,01\% & 95,75\% & 11,95\% & 14,11\% \\ \hline
\textbf{A} & \textbf{A} & 20,95\% & 61,53\% & 72,01\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 86,35\% & 96,10\% & 12,56\% & 17,43\% \\ \hline
\textbf{A} & \textbf{A} & 24,97\% & 64,90\% & 68,33\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs spécifique au show et la détection du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Influence de la prise en compte du genre}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 80,19\% & 89,33\% & 18,32\% & 19,09\% \\ \hline
\textbf{A} & \textbf{A} & 20,70\% & 45,15\% & 72,81\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 82,24\% & 91,16\% & 16,45\% & 21,58\% \\ \hline
\textbf{A} & \textbf{A} & 24,61\% & 58,82\% & 68,78\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique en utilisant une liste de locuteurs et sans prise en compte du genre}
\label{tab:results_fctcroyance}
\end{table}

\subsection{Système ouvert sans liste de locuteurs et sans genre}

\ref{tab:results_fctcroyance_sansliste}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{3}{|c|}{En durée}  & En nb de Locuteur \\ \hline
\textbf{Trans.} & \textbf{Seg/Class.} & \textbf{R} & \textbf{P} & \textbf{ErrDur} & \textbf{ErrLoc} \\ \hline
\hline
\multicolumn{6}{|c|}{Corpus de Développement} \\
\hline
\textbf{M} & \textbf{M} & 65,74\% & 91,96\% & 31,63\% & 29,46\% \\ \hline
\textbf{A} & \textbf{A} & 19,53\% & 70,52\% & 73,39\% & - \\
\hline
\hline
\multicolumn{6}{|c|}{Corpus de Test} \\
\hline
\textbf{M} & \textbf{M} & 69,85\% & 94,10\% & 27,79\% & 28,22\% \\ \hline
\textbf{A} & \textbf{A} & 20,59\% & 61,42\% & 72,31\% & - \\
\hline
\end{tabular}
\end{center}
\caption{Système utilisant les fonctions de croyance avec une transcription enrichie manuelle ou automatique sans liste de locuteurs et sans genre}
\label{tab:results_fctcroyance_sansliste}
\end{table}

\section{Limites de l'approche}
très dépendant du corpus pour l'automatique (car on mesure en durée), un ou deux locuteurs détectés en plus ou en moins peuvent beaucoup faire varier les résultats alors que ça ne représente qu'une infime partie du nombre des locuteurs à détecter



% \section{Commandes}
% 
% Un mot entre guillemets : \quotes{mot}.
% 
% \todo{écrire cette thèse}
% 
% \tocite{une référence}
% 
% \textit{et caetera} : a, b, c\etc
% 
% \section{Références natbib}
% 
% \citet{exemple1}
% 
% \citep{exemple1}
% 
% \citep[avant][après]{exemple1}
% 
% \citet{exemple2}
% 
% \citep{exemple2}
% 
% \citep[avant][après]{exemple2}
% 
% \section{Les maths}
% 
% \begin{align}
% x^2-y^2&=(x+y)\times(x-y)\label{equation:trivia}\\
% \max_{i<5}f(i)&=\argmin_{j<18} g(j)\nonumber
% \end{align}
% 
% Voilà l'équation~\ref{equation:trivia}.
% 
% \section{Figures et tables}
% 
% Les figures sont centrées, comme on peut le voir dans la figure~\ref{figure:logo_lia}.
% 
% \cfigurex{logos/logo_uapv}{Titre court pour la table des illustrations.}{\label{figure:logo_lia}Caption de la figure\etc c'est le logo du LIA.}{.5}
% 
% Les tables ont un trait en haut et en bas, comme le démontre la table~\ref{table:stats}.
% 
% \ctablexx{lr}{Titre court pour la liste des tables.}{Caption de la table\etc quelques statistiques.}{table:stats}{
% \textbf{Aliment} & \textbf{Quantité} \\
% \hline
% Chips & 198 \\
% Poulet & 1 \\
% }

\postdocument
\end{document}
